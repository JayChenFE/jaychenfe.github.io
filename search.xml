<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SpringBoot启动引导-自动装备实例SpringWebMvc]]></title>
    <url>%2F2020%2F01%2F03%2FSpringBoot%E5%90%AF%E5%8A%A8%E5%BC%95%E5%AF%BC-%E8%87%AA%E5%8A%A8%E8%A3%85%E5%A4%87%E5%AE%9E%E4%BE%8BSpringWebMvc%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot启动引导-自动动装配]]></title>
    <url>%2F2020%2F01%2F02%2FSpringBoot%E5%90%AF%E5%8A%A8%E5%BC%95%E5%AF%BC-%E8%87%AA%E5%8A%A8%E5%8A%A8%E8%A3%85%E9%85%8D%2F</url>
    <content type="text"><![CDATA[[toc] 6. SpringBoot的自动装配SpringBoot的自动配置完全由 @EnableAutoConfiguration 开启。 @EnableAutoConfiguration 的内容： 1234567@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(AutoConfigurationImportSelector.class)public @interface EnableAutoConfiguration 文档注释原文翻译： Enable auto-configuration of the Spring Application Context, attempting to guess and configure beans that you are likely to need. Auto-configuration classes are usually applied based on your classpath and what beans you have defined. For example, if you have tomcat-embedded.jar on your classpath you are likely to want a TomcatServletWebServerFactory (unless you have defined your own ServletWebServerFactory bean). 启用Spring-ApplicationContext的自动配置，并且会尝试猜测和配置您可能需要的Bean。通常根据您的类路径和定义的Bean来应用自动配置类。例如，如果您的类路径上有 tomcat-embedded.jar，则可能需要 TomcatServletWebServerFactory （除非自己已经定义了 ServletWebServerFactory 的Bean）。 When using SpringBootApplication, the auto-configuration of the context is automatically enabled and adding this annotation has therefore no additional effect. 使用 @SpringBootApplication 时，将自动启用上下文的自动配置，因此再添加该注解不会产生任何其他影响。 Auto-configuration tries to be as intelligent as possible and will back-away as you define more of your own configuration. You can always manually exclude() any configuration that you never want to apply (use excludeName() if you don’t have access to them). You can also exclude them via the spring.autoconfigure.exclude property. Auto-configuration is always applied after user-defined beans have been registered. 自动配置会尝试尽可能地智能化，并且在您定义更多自定义配置时会自动退出（被覆盖）。您始终可以手动排除掉任何您不想应用的配置（如果您无法访问它们，请使用 excludeName() 方法），您也可以通过 spring.autoconfigure.exclude 属性排除它们。自动配置始终在注册用户自定义的Bean之后应用。 The package of the class that is annotated with @EnableAutoConfiguration, usually via @SpringBootApplication, has specific significance and is often used as a ‘default’. For example, it will be used when scanning for @Entity classes. It is generally recommended that you place @EnableAutoConfiguration (if you’re not using @SpringBootApplication) in a root package so that all sub-packages and classes can be searched. 通常被 @EnableAutoConfiguration 标注的类（如 @SpringBootApplication）的包具有特定的意义，通常被用作“默认值”。例如，在扫描@Entity类时将使用它。通常建议您将 @EnableAutoConfiguration（如果您未使用 @SpringBootApplication）放在根包中，以便可以搜索所有包及子包下的类。 Auto-configuration classes are regular Spring Configuration beans. They are located using the SpringFactoriesLoader mechanism (keyed against this class). Generally auto-configuration beans are @Conditional beans (most often using @ConditionalOnClass and @ConditionalOnMissingBean annotations). 自动配置类也是常规的Spring配置类。它们使用 SpringFactoriesLoader 机制定位（针对此类）。通常自动配置类也是 @Conditional Bean（最经常的情况下是使用 @ConditionalOnClass 和 @ConditionalOnMissingBean 标注）。 @EnableAutoConfiguration 也是一个组合注解,分开来看 6.1 @AutoConfigurationPackage123456789101112131415/** * Indicates that the package containing the annotated class should be registered with * &#123;@link AutoConfigurationPackages&#125;. * 表示包含该注解的类所在的包应该在 AutoConfigurationPackages 中注册。 * @author Phillip Webb * @since 1.3.0 * @see AutoConfigurationPackages */@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Import(AutoConfigurationPackages.Registrar.class)public @interface AutoConfigurationPackage 它的实现原理是在注解上标注了 @Import，导入了一个 AutoConfigurationPackages.Registrar 。 6.1.1 AutoConfigurationPackages.Registrar123456789101112131415/** * &#123;@link ImportBeanDefinitionRegistrar&#125; to store the base package from the importing ImportBeanDefinitionRegistrar 用于保存导入的配置类所在的根包 * configuration. */static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports &#123; @Override public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) &#123; register(registry, new PackageImport(metadata).getPackageName()); &#125; @Override public Set&lt;Object&gt; determineImports(AnnotationMetadata metadata) &#123; return Collections.singleton(new PackageImport(metadata)); &#125;&#125; 它就是实现把主配置所在根包保存起来以便后期扫描用的。分析源码： Registrar 实现了 ImportBeanDefinitionRegistrar 接口，它向IOC容器中要手动注册组件。 在重写的 registerBeanDefinitions 方法中，它要调用外部类 AutoConfigurationPackages 的register方法。 看传入的参数：new PackageImport(metadata).getPackageName(),它实例化的 PackageImport 对象的构造方法： 123PackageImport(AnnotationMetadata metadata) &#123; this.packageName = ClassUtils.getPackageName(metadata.getClassName());&#125; 它取了一个 metadata 的所在包名。那 metadata 又是什么呢？ 翻看 ImportBeanDefinitionRegistrar的文档注释： 12345678public interface ImportBeanDefinitionRegistrar &#123; /** * ...... * @param importingClassMetadata annotation metadata of the importing class * @param registry current bean definition registry */ void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry);&#125; 注意 importingClassMetadata 的参数说明：导入类的注解元数据。 它实际代表的是被 @Import 标记的类的信息。 那在 SpringBoot 的主启动类中，被标记的肯定就是最开始案例里的 DemoApplication。 也就是说它是 DemoApplication 的类信息，那获取它的包名就是获取主启动类的所在包。 拿到这个包有什么意义呢？不清楚，那就回到那个 Registrar 中，看它调用的 register 方法都干了什么： 6.1.2 register方法123456789101112131415161718private static final String BEAN = AutoConfigurationPackages.class.getName();public static void register(BeanDefinitionRegistry registry, String... packageNames) &#123; // 判断 BeanFactory 中是否包含 AutoConfigurationPackages if (registry.containsBeanDefinition(BEAN)) &#123; BeanDefinition beanDefinition = registry.getBeanDefinition(BEAN); ConstructorArgumentValues constructorArguments = beanDefinition.getConstructorArgumentValues(); // addBasePackages：添加根包扫描包 constructorArguments.addIndexedArgumentValue(0, addBasePackages(constructorArguments, packageNames)); &#125; else &#123; GenericBeanDefinition beanDefinition = new GenericBeanDefinition(); beanDefinition.setBeanClass(BasePackages.class); beanDefinition.getConstructorArgumentValues().addIndexedArgumentValue(0, packageNames); beanDefinition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); registry.registerBeanDefinition(BEAN, beanDefinition); &#125;&#125; 划重点：它要判断当前IOC容器中是否包含 AutoConfigurationPackages 。如果有，就会拿到刚才传入的包名，设置到一个 basePackage 里面！basePackage 的意义很明显是根包。 换句话说，它要取主启动类所在包及子包下的组件。 到这里，就呼应了文档注释中的描述，也解释了为什么 SpringBoot 的启动器一定要在所有类的最外层。 6.2 @Import(AutoConfigurationImportSelector.class)根据上一章节的基础，看到这个也不难理解，它导入了一个 ImportSelector，来向容器中导入组件。 导入的组件是：AutoConfigurationImportSelector 6.2.1 AutoConfigurationImportSelector12public class AutoConfigurationImportSelector implements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered 文档注释原文翻译： DeferredImportSelector to handle auto-configuration. This class can also be subclassed if a custom variant of @EnableAutoConfiguration is needed. DeferredImportSelector 处理自动配置。如果需要自定义扩展 @EnableAutoConfiguration，则也可以编写该类的子类。 咱能看出来它是 ImportSelector , 可它又特别提到了 DeferredImportSelector，它又是什么呢？ 6.2.2 DeferredImportSelector1public interface DeferredImportSelector extends ImportSelector 它是 ImportSelector 的子接口，它的文档注释原文和翻译： A variation of ImportSelector that runs after all @Configuration beans have been processed. This type of selector can be particularly useful when the selected imports are @Conditional . Implementations can also extend the org.springframework.core.Ordered interface or use the org.springframework.core.annotation.Order annotation to indicate a precedence against other DeferredImportSelectors . Implementations may also provide an import group which can provide additional sorting and filtering logic across different selectors. ImportSelector 的一种扩展，在处理完所有 @Configuration 类型的Bean之后运行。当所选导入为 @Conditional 时，这种类型的选择器特别有用。 实现类还可以扩展 Ordered 接口，或使用 @Order 注解来指示相对于其他 DeferredImportSelector 的优先级。 实现类也可以提供导入组，该导入组可以提供跨不同选择器的其他排序和筛选逻辑。 由此我们可以知道，DeferredImportSelector 的执行时机，是在 @Configuration 注解中的其他逻辑被处理完毕之后（包括对 @ImportResource、@Bean 这些注解的处理）再执行，换句话说，DeferredImportSelector 的执行时机比 ImportSelector 更晚。 回到 AutoConfigurationImportSelector，它的核心部分，就是 ImportSelector 的 selectImport 方法： 12345678910111213@Overridepublic String[] selectImports(AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return NO_IMPORTS; &#125; AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader .loadMetadata(this.beanClassLoader); // 加载自动配置类 AutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(autoConfigurationMetadata, annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());&#125; 关键的源码在 getAutoConfigurationEntry(autoConfigurationMetadata, annotationMetadata) ： 6.2.3 getAutoConfigurationEntry(autoConfigurationMetadata, annotationMetadata)12345678910111213141516171819202122/** * Return the &#123;@link AutoConfigurationEntry&#125; based on the &#123;@link AnnotationMetadata&#125; * of the importing &#123;@link Configuration @Configuration&#125; class. * * 根据导入的@Configuration类的AnnotationMetadata返回AutoConfigurationImportSelector.AutoConfigurationEntry。 */protected AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata, AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return EMPTY_ENTRY; &#125; AnnotationAttributes attributes = getAttributes(annotationMetadata); // 【核心】加载候选的自动配置类 List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); Set&lt;String&gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = filter(configurations, autoConfigurationMetadata); fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationEntry(configurations, exclusions);&#125; 这个方法里有一个非常关键的集合：configurations（最后直接拿他来返回出去了，给 selectImports 方法转成 String[]）。 既然最后拿它返回出去，必然它是导入其他组件的核心。 这个 configurations 集合的数据，都是通过 getCandidateConfigurations 方法来获取： 123456789101112protected Class&lt;?&gt; getSpringFactoriesLoaderFactoryClass() &#123; return EnableAutoConfiguration.class;&#125;protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; // SPI机制加载自动配置类 List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, &quot;No auto configuration classes found in META-INF/spring.factories. If you &quot; + &quot;are using a custom packaging, make sure that file is correct.&quot;); return configurations;&#125; 这个方法又调用了 SpringFactoriesLoader.loadFactoryNames 方法，传入的Class就是 @EnableAutoConfiguration： 6.2.4 SpringFactoriesLoader.loadFactoryNames123456789101112131415161718192021222324252627282930313233343536373839public static final String FACTORIES_RESOURCE_LOCATION = &quot;META-INF/spring.factories&quot;;public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, @Nullable ClassLoader classLoader) &#123; String factoryClassName = factoryClass.getName(); // ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ return loadSpringFactories(classLoader).getOrDefault(factoryClassName, Collections.emptyList());&#125;private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) &#123; MultiValueMap&lt;String, String&gt; result = cache.get(classLoader); if (result != null) &#123; return result; &#125; try &#123; // ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ Enumeration&lt;URL&gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); result = new LinkedMultiValueMap&lt;&gt;(); while (urls.hasMoreElements()) &#123; URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) &#123; String factoryClassName = ((String) entry.getKey()).trim(); for (String factoryName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) &#123; result.add(factoryClassName, factoryName.trim()); &#125; &#125; &#125; cache.put(classLoader, result); return result; &#125; catch (IOException ex) &#123; throw new IllegalArgumentException(&quot;Unable to load factories from location [&quot; + FACTORIES_RESOURCE_LOCATION + &quot;]&quot;, ex); &#125;&#125; 源码中使用 classLoader 去加载了指定常量路径下的资源： FACTORIES_RESOURCE_LOCATION ，而这个常量指定的路径实际是：META-INF/spring.factories 。 这个文件在 spring-boot-autoconfiguration 包下可以找到。 spring-boot-autoconfiguration 包下 META-INF/spring.factories 节选： 123456789101112131415161718192021222324252627# Initializersorg.springframework.context.ApplicationContextInitializer=\org.springframework.boot.autoconfigure.SharedMetadataReaderFactoryContextInitializer,\org.springframework.boot.autoconfigure.logging.ConditionEvaluationReportLoggingListener# Application Listenersorg.springframework.context.ApplicationListener=\org.springframework.boot.autoconfigure.BackgroundPreinitializer# Auto Configuration Import Listenersorg.springframework.boot.autoconfigure.AutoConfigurationImportListener=\org.springframework.boot.autoconfigure.condition.ConditionEvaluationReportAutoConfigurationImportListener# Auto Configuration Import Filtersorg.springframework.boot.autoconfigure.AutoConfigurationImportFilter=\org.springframework.boot.autoconfigure.condition.OnBeanCondition,\org.springframework.boot.autoconfigure.condition.OnClassCondition,\org.springframework.boot.autoconfigure.condition.OnWebApplicationCondition# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\...... 之后拿到这个资源文件，以 Properties 的形式加载，并取出 org.springframework.boot.autoconfigure.EnableAutoConfiguration 指定的所有自动配置类（是一个很大的字符串，里面都是自动配置类的全限定类名），装配到IOC容器中，之后自动配置类就会通过 ImportSelector 和 @Import 的机制被创建出来，之后就生效了。 这也就解释了为什么 即便没有任何配置文件，SpringBoot的Web应用都能正常运行。 6.2.5 【总结规律】从上面的 Properties 中发现，所有配置的 EnableAutoConfiguration 的自动配置类，都以 AutoConfiguration 结尾！由此规律，以后我们要了解一个 SpringBoot 的模块或者第三方集成的模块时，就可以大胆猜测基本上一定会有 XXXAutoConfiguration 类出现！ 6.3 【扩展】SpringBoot使用的工厂机制SpringBoot 在非常多的位置都利用类似于上面 “通过读取 spring.factories 加载一组预先配置的类” 的机制，而这个机制的核心源码来自 SpringFactoriesLoader 。这一章节我们来详细了解一下这个类，对于后续 SpringBoot 的应用启动过程的源码阅读和原理的理解都有所帮助。 12345678package org.springframework.core.io.support;/** * ...... * * @since 3.2 */public final class SpringFactoriesLoader 我们发现它不是来自 SpringBoot，而是在 SpringFramework3.2 就已经有了的类。它的文档注释原文翻译： General purpose factory loading mechanism for internal use within the framework. SpringFactoriesLoader loads and instantiates factories of a given type from &quot;META-INF/spring.factories&quot; files which may be present in multiple JAR files in the classpath. The spring.factories file must be in Properties format, where the key is the fully qualified name of the interface or abstract class, and the value is a comma-separated list of implementation class names. For example: example.MyService=example.MyServiceImpl1,example.MyServiceImpl2 where example.MyService is the name of the interface, and MyServiceImpl1 and MyServiceImpl2 are two implementations. 它是一个框架内部内部使用的通用工厂加载机制。 SpringFactoriesLoader 从 META-INF/spring.factories 文件中加载并实例化给定类型的工厂，这些文件可能存在于类路径中的多个jar包中。spring.factories 文件必须采用 properties 格式，其中key是接口或抽象类的全限定名，而value是用逗号分隔的实现类的全限定类名列表。 例如：example.MyService=example.MyServiceImpl1,example.MyServiceImpl2 其中 example.MyService 是接口的名称，而 MyServiceImpl1 和 MyServiceImpl2 是两个该接口的实现类。 到这里已经能够发现，这个思路跟Java原生的SPI非常类似。 6.3.1 【扩展】Java的SPISPI全称为 Service Provider Interface，是jdk内置的一种服务提供发现机制。简单来说，它就是一种动态替换发现的机制。 SPI规定，所有要预先声明的类都应该放在 META-INF/services 中。配置的文件名是接口/抽象类的全限定名，文件内容是抽象类的子类或接口的实现类的全限定类名，如果有多个，借助换行符，一行一个。 具体使用时，使用jdk内置的 ServiceLoader 类来加载预先配置好的实现类。 举个例子： 在 META-INF/services 中声明一个文件名为 com.linkedbear.boot.demo.SpiDemoInterface 的文件，文件内容为： 1com.linkedbear.boot.demo.SpiDemoInterfaceImpl 在 com.linkedbear.boot.demo 包下新建一个接口，类名必须跟上面配置的文件名一样：SpiDemoInterface。 在接口中声明一个 test() 方法： 123public interface SpiDemoInterface &#123; void test();&#125; 接下来再新建一个类 SpiDemoInterfaceImpl，并实现 SpiDemoInterface： 123456public class SpiDemoInterfaceImpl implements SpiDemoInterface &#123; @Override public void test() &#123; System.out.println(&quot;SpiDemoInterfaceImpl#test() run...&quot;); &#125;&#125; 编写主运行类，测试效果： 123456public class App &#123; public static void main(String[] args) &#123; ServiceLoader&lt;SpiDemoInterface&gt; loaders = ServiceLoader.load(SpiDemoInterface.class); loaders.foreach(SpiDemoInterface::test); &#125;&#125; 运行结果： 1SpiDemoInterfaceImpl#test() run... 6.3.2 SpringFramework的SpringFactoriesLoaderSpringFramework 利用 SpringFactoriesLoader 都是调用 loadFactoryNames 方法： 1234567891011121314/** * Load the fully qualified class names of factory implementations of the * given type from &#123;@value #FACTORIES_RESOURCE_LOCATION&#125;, using the given * class loader. * @param factoryClass the interface or abstract class representing the factory * @param classLoader the ClassLoader to use for loading resources; can be * &#123;@code null&#125; to use the default * @throws IllegalArgumentException if an error occurs while loading factory names * @see #loadFactories */public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, @Nullable ClassLoader classLoader) &#123; String factoryClassName = factoryClass.getName(); return loadSpringFactories(classLoader).getOrDefault(factoryClassName, Collections.emptyList());&#125; 文档注释原文翻译： Load the fully qualified class names of factory implementations of the given type from “META-INF/spring.factories”, using the given class loader. 使用给定的类加载器从 META-INF/spring.factories 中加载给定类型的工厂实现的全限定类名。 文档注释中没有提到接口、抽象类、实现类的概念，结合之前看到过的 spring.factories 文件，应该能意识到它只是key-value的关系！ 这么设计的好处：不再局限于接口-实现类的模式，key可以随意定义！ （如上面的 org.springframework.boot.autoconfigure.EnableAutoConfiguration 是一个注解） 来看方法实现，第一行代码获取的是要被加载的接口/抽象类的全限定名，下面的 return 分为两部分：loadSpringFactories 和 getOrDefault。getOrDefault 方法很明显是Map中的方法，不再解释，主要来详细看 loadSpringFactories 方法。 6.3.3 loadSpringFactories1234567891011121314151617181920212223242526272829303132333435public static final String FACTORIES_RESOURCE_LOCATION = &quot;META-INF/spring.factories&quot;;private static final Map&lt;ClassLoader, MultiValueMap&lt;String, String&gt;&gt; cache = new ConcurrentReferenceHashMap&lt;&gt;();// 这个方法仅接收了一个类加载器private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) &#123; MultiValueMap&lt;String, String&gt; result = cache.get(classLoader); if (result != null) &#123; return result; &#125; try &#123; Enumeration&lt;URL&gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); result = new LinkedMultiValueMap&lt;&gt;(); while (urls.hasMoreElements()) &#123; URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) &#123; String factoryClassName = ((String) entry.getKey()).trim(); for (String factoryName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) &#123; result.add(factoryClassName, factoryName.trim()); &#125; &#125; &#125; cache.put(classLoader, result); return result; &#125; catch (IOException ex) &#123; throw new IllegalArgumentException(&quot;Unable to load factories from location [&quot; + FACTORIES_RESOURCE_LOCATION + &quot;]&quot;, ex); &#125;&#125; 我们分段来看。 6.3.3.1 获取本地缓存1234MultiValueMap&lt;String, String&gt; result = cache.get(classLoader);if (result != null) &#123; return result;&#125; 进入方法后先从本地缓存中根据当前的类加载器获取是否有一个类型为 MultiValueMap 的值，这个类型有些陌生，我们先看看这是个什么东西： 12345678910package org.springframework.util;/** * Extension of the &#123;@code Map&#125; interface that stores multiple values. * * @since 3.0 * @param &lt;K&gt; the key type * @param &lt;V&gt; the value element type */public interface MultiValueMap&lt;K, V&gt; extends Map&lt;K, List&lt;V&gt;&gt; 发现它实际上就是一个 Map&gt;。 那第一次从cache中肯定获取不到值，故下面的if结构肯定不进入，进入下面的try块。 6.3.3.2 加载spring.factories1234Enumeration&lt;URL&gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION));result = new LinkedMultiValueMap&lt;&gt;(); 这部分动作就是获取当前 classpath 下所有jar包中有的 spring.factories 文件，并将它们加载到内存中。 6.3.3.3 缓存到本地123456789101112while (urls.hasMoreElements()) &#123; URL url = urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) &#123; String factoryClassName = ((String) entry.getKey()).trim(); for (String factoryName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) &#123; result.add(factoryClassName, factoryName.trim()); &#125; &#125;&#125;cache.put(classLoader, result); 它拿到每一个文件，并用 Properties 方式加载文件，之后把这个文件中每一组键值对都加载出来，放入 MultiValueMap 中。 如果一个接口/抽象类有多个对应的目标类，则使用英文逗号隔开。StringUtils.commaDelimitedListToStringArray会将大字符串拆成一个一个的全限定类名。 整理完后，整个result放入cache中。下一次再加载时就无需再次加载 spring.factories 文件了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot启动引导-SpringFramework的手动装配]]></title>
    <url>%2F2019%2F12%2F31%2FSpringBoot%E5%90%AF%E5%8A%A8%E5%BC%95%E5%AF%BC-SpringFramework%E7%9A%84%E6%89%8B%E5%8A%A8%E8%A3%85%E9%85%8D%2F</url>
    <content type="text"><![CDATA[[toc] 在了解 @EnableAutoConfiguration 之前，先了解 SpringFramework 的原生手动装配机制，这对后续阅读 @EnableAutoConfiguration 有很大帮助。 5.SpringFramework的手动装配在原生的 SpringFramework 中，装配组件有三种方式： 使用模式注解 @Component 等（Spring2.5+） 使用配置类 @Configuration 与 @Bean （Spring3.0+） 使用模块装配 @EnableXXX 与 @Import （Spring3.1+） 其中使用 @Component 及衍生注解很常见 但@Component注解只能在自己编写的代码中标注，无法装配jar包中的组件。为此可以使用 @Configuration 与 @Bean，手动装配组件（如上面的 @Configuration 示例）。 但这种方式一旦注册过多，会导致编码成本高，维护不灵活等问题。 SpringFramework 提供了模块装配功能，通过给配置类标注 @EnableXXX 注解，再在注解上标注 @Import 注解，即可完成组件装配的效果。 下面介绍模块装配的使用方式。 5.1 @EnableXXX与@Import的使用创建几个颜色的实体类，如Red，Yellow，Blue，Green，Black等。 新建 @EnableColor 注解，并声明 @Import。（注意注解上有三个必须声明的元注解） 123456@Documented@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface EnableColor &#123; &#125; @Import 可以传入四种类型：普通类、配置类、ImportSelector 的实现类，ImportBeanDefinitionRegistrar 的实现类。具体如文档注释中描述： 123456789public @interface Import &#123; /** * &#123;@link Configuration @Configuration&#125;, &#123;@link ImportSelector&#125;, * &#123;@link ImportBeanDefinitionRegistrar&#125;, or regular component classes to import. */ Class&lt;?&gt;[] value();&#125; value中写的很明白了，可以导入配置类、ImportSelector 的实现类，ImportBeanDefinitionRegistrar 的实现类，或者普通类。 下面介绍 @Import 的用法。 5.1.1 导入普通类直接在 @Import 注解中标注Red类： 1234@Import(&#123;Red.class&#125;)public @interface EnableColor &#123; &#125; 之后启动类标注 @EnableColor，引导启动IOC容器： 12345678910111213@EnableColor@Configurationpublic class ColorConfiguration &#123; &#125;public class App &#123; public static void main(String[] args) throws Exception &#123; AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(ColorConfiguration.class); String[] beanDefinitionNames = ctx.getBeanDefinitionNames(); Stream.of(beanDefinitionNames).forEach(System.out::println); &#125;&#125; 控制台打印： 1234567org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactorycolorConfigurationcom.example.demo.enablexxx.Red 可见Red类已经被注册。 5.1.2 导入配置类新建 ColorRegistrarConfiguration，并标注 @Configuration ： 123456789@Configurationpublic class ColorRegistrarConfiguration &#123; @Bean public Yellow yellow() &#123; return new Yellow(); &#125; &#125; 之后在 @EnableColor 的 @Import 注解中加入 ColorRegistrarConfiguration： 1234@Import(&#123;Red.class, ColorRegistrarConfiguration.class&#125;)public @interface EnableColor &#123; &#125; 重新启动IOC容器，打印结果： 123456789org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactorycolorConfigurationcom.example.demo.enablexxx.Redcom.example.demo.enablexxx.ColorRegistrarConfigurationyellow 可见配置类 ColorRegistrarConfiguration 和 Yellow 都已注册到IOC容器中。 5.1.3 导入ImportSelector新建 ColorImportSelector，实现 ImportSelector 接口： 12345678public class ColorImportSelector implements ImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; return new String[] &#123;Blue.class.getName(), Green.class.getName()&#125;; &#125; &#125; 之后在 @EnableColor 的 @Import 注解中加入 ColorImportSelector： 1234@Import(&#123;Red.class, ColorRegistrarConfiguration.class, ColorImportSelector.class&#125;)public @interface EnableColor &#123; &#125; 重新启动IOC容器，打印结果： 1234567891011org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactorycolorConfigurationcom.example.demo.enablexxx.Redcom.example.demo.enablexxx.ColorRegistrarConfigurationyellowcom.example.demo.enablexxx.Bluecom.example.demo.enablexxx.Green ColorImportSelector 没有注册到IOC容器中，两个新的颜色类被注册。 5.1.4 导入ImportBeanDefinitionRegistrar新建 ColorImportBeanDefinitionRegistrar，实现 ImportBeanDefinitionRegistrar 接口： 12345678public class ColorImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; registry.registerBeanDefinition("black", new RootBeanDefinition(Black.class)); &#125; &#125; 之后在 @EnableColor 的 @Import 注解中加入 ColorImportBeanDefinitionRegistrar： 1234@Import(&#123;Red.class, ColorRegistrarConfiguration.class, ColorImportSelector.class, ColorImportBeanDefinitionRegistrar.class&#125;)public @interface EnableColor &#123; &#125; 重新启动IOC容器，打印结果： 123456789101112org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactorycolorConfigurationcom.example.demo.enablexxx.Redcom.example.demo.enablexxx.ColorRegistrarConfigurationyellowcom.example.demo.enablexxx.Bluecom.example.demo.enablexxx.Greenblack 由于在注册Black的时候要指定Bean的id，而上面已经标明了使用 “black” 作为id，故打印的 beanDefinitionName 就是black。]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot启动引导-原理概述及包扫描]]></title>
    <url>%2F2019%2F12%2F31%2FSpringBoot%E5%90%AF%E5%8A%A8%E5%BC%95%E5%AF%BC-%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0%E5%8F%8A%E5%8C%85%E6%89%AB%E6%8F%8F%2F</url>
    <content type="text"><![CDATA[[toc] 1.新建入门程序使用IDEA的SpringInitializer 创建一个 SpringBoot 应用 pom文件我只引入了 spring-boot-starter-web： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 启动类如下12345678@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 尝试注释掉@SpringBootApplication注解并启动123org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.context.ApplicationContextException: Unable to start ServletWebServerApplicationContext due to missing ServletWebServerFactory bean. at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:156) ~[spring-boot-2.2.2.RELEASE.jar:2.2.2.RELEASE] ... 因为没有 ServletWebServerFactory，而导致无法启动IOC容器。那@SpringBootApplication是做什么的呢? 2.@SpringBootApplication还原@SpringBootApplication注解 123456789101112131415161718192021222324/** * Indicates a &#123;@link Configuration configuration&#125; class that declares one or more * &#123;@link Bean @Bean&#125; methods and also triggers &#123;@link EnableAutoConfiguration * auto-configuration&#125; and &#123;@link ComponentScan component scanning&#125;. This is a convenience * annotation that is equivalent to declaring &#123;@code @Configuration&#125;, * &#123;@code @EnableAutoConfiguration&#125; and &#123;@code @ComponentScan&#125;. * * @author Phillip Webb * @author Stephane Nicoll * @author Andy Wilkinson * @since 1.2.0 标识了一个配置类，这个配置类上声明了一个或多个 @Bean 的方法，并且它会触发自动配置和组件扫描。 这是一个很方便的注解，它等价于同时标注 @Configuration + @EnableAutoConfiguration + @ComponentScan 。 */@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;)public @interface SpringBootApplication 从SpringBoot1.2.0开始出现的，在 SpringBoot1.1及以前的版本，在启动类上标注的注解应该是三个：@Configuration +@EnableAutoConfiguration + @ComponentScan，只不过从1.2以后 SpringBoot 帮我们整合起来了,即 @SpringBootApplication = (默认属性)@Configuration + @EnableAutoConfiguration + @ComponentScan 注意@SpringBootConfiguration点开查看发现里面还是应用了@Configuration 文档注释已经描述的很详细：它是一个组合注解，包括3个注解。标注它之后就会触发自动配置（@EnableAutoConfiguration）和组件扫描（@ComponentScan）。 那这三个注解有什么作用呢? 3. @SpringBootConfiguration1234567891011121314151617181920212223/** * Indicates that a class provides Spring Boot application * &#123;@link Configuration @Configuration&#125;. Can be used as an alternative to the Spring's * standard &#123;@code @Configuration&#125; annotation so that configuration can be found * automatically (for example in tests). 标识一个类作为 SpringBoot 的配置类， 它可以是Spring原生的 @Configuration 的一种替换方案，目的是这个配置可以被自动发现。 * &lt;p&gt; * Application should only ever include &lt;em&gt;one&lt;/em&gt; &#123;@code @SpringBootConfiguration&#125; and * most idiomatic Spring Boot applications will inherit it from * &#123;@code @SpringBootApplication&#125;. 应用应当只在主启动类上标注 @SpringBootConfiguration， 大多数情况下都是直接使用 @SpringBootApplication。 * * @author Phillip Webb * @author Andy Wilkinson * @since 1.4.0 */@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Configurationpublic @interface SpringBootConfiguration 从文档注释以及它的声明上可以看出，它被 @Configuration 标注，说明它实际上是标注配置类的，而且是标注主启动类的。 3.1 @Configuration的作用@Configuration是JavaConfig形式的Spring Ioc容器的配置类使用的那个@Configuration，SpringBoot社区推荐使用基于JavaConfig的配置形式，所以，这里的启动类标注了@Configuration之后，本身其实也是一个IoC容器的配置类。 例如: 3.1.1表达形式层面基于XML配置的方式是这样： 1234567&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd" default-lazy-init="true"&gt; &lt;!--bean定义--&gt;&lt;/beans&gt; 而基于JavaConfig的配置方式是这样： 1234@Configurationpublic class MockConfiguration&#123; //bean定义&#125; 任何一个标注了@Configuration的Java类定义都是一个JavaConfig配置类。 3.1.2注册bean定义层面基于XML的配置形式是这样： 123&lt;bean id="mockService" class="..MockServiceImpl"&gt; ...&lt;/bean&gt; 而基于JavaConfig的配置形式是这样的： 1234567@Configurationpublic class MockConfiguration&#123; @Bean public MockService mockService()&#123; return new MockServiceImpl(); &#125;&#125; 任何一个标注了@Bean的方法，其返回值将作为一个bean定义注册到Spring的IoC容器，方法名将默认成该bean定义的id。 3.1.3表达依赖注入关系层面为了表达bean与bean之间的依赖关系，在XML形式中一般是这样： 1234&lt;bean id="mockService" class="..MockServiceImpl"&gt; &lt;propery name ="dependencyService" ref="dependencyService" /&gt;&lt;/bean&gt;&lt;bean id="dependencyService" class="DependencyServiceImpl"&gt;&lt;/bean&gt; 而基于JavaConfig的配置形式是这样的： 123456789101112@Configurationpublic class MockConfiguration&#123; @Bean public MockService mockService()&#123; return new MockServiceImpl(dependencyService()); &#125; @Bean public DependencyService dependencyService()&#123; return new DependencyServiceImpl(); &#125;&#125; 如果一个bean的定义依赖其他bean，则直接调用对应的JavaConfig类中依赖bean的创建方法就可以了。 @Configuration：提到@Configuration就要提到他的搭档@Bean。使用这两个注解就可以创建一个简单的spring配置类，可以用来替代相应的xml配置文件。 123456&lt;beans&gt; &lt;bean id = "car" class="com.test.Car"&gt; &lt;property name="wheel" ref = "wheel"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id = "wheel" class="com.test.Wheel"&gt;&lt;/bean&gt;&lt;/beans&gt; 相当于： 1234567891011121314@Configurationpublic class Conf &#123; @Bean public Car car() &#123; Car car = new Car(); car.setWheel(wheel()); return car; &#125; @Bean public Wheel wheel() &#123; return new Wheel(); &#125;&#125; 3.1.4验证初始化一个IOC容器，并打印IOC容器中的所有bean的name： 1234567public class MainApp &#123; public static void main(String[] args) throws Exception &#123; AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(ConfigurationDemo.class); String[] beanDefinitionNames = ctx.getBeanDefinitionNames(); Stream.of(beanDefinitionNames).forEach(System.out::println); &#125;&#125; 12345678org.springframework.context.annotation.internalConfigurationAnnotationProcessororg.springframework.context.annotation.internalAutowiredAnnotationProcessororg.springframework.context.annotation.internalCommonAnnotationProcessororg.springframework.context.event.internalEventListenerProcessororg.springframework.context.event.internalEventListenerFactoryConfwheelcar 可以发现组件，以及配置类本身被成功加载。 @Configuration的注解类标识这个类可以使用Spring IoC容器作为bean定义的来源。 @Bean注解告诉Spring，一个带有@Bean的注解方法将返回一个对象，该对象应该被注册为在Spring应用程序上下文中的bean。 3.2 @SpringBootConfiguration的附加作用借助IDEA搜索 @SpringBootConfiguration 的出现位置，发现除了 @SpringBootApplication 外，只有一个位置使用了它： 发现是一个测试包中的usage（默认的 SpringInitializer 会把 spring-boot-starter-test 一起带进来，故可以搜到这个usage。如果小伙伴手动使用Maven创建 SpringBoot 应用且没有导入 spring-boot-start-test 依赖，则这个usage都不会搜到）。 翻看 SpringBoot 的官方文档，发现通篇只有两个位置提到了 @SpringBootConfiguration，还真有一个跟测试相关： https://docs.spring.io/spring-boot/docs/2.2.2.RELEASE/reference/htmlsingle/#boot-features-testing-spring-boot-applications-detecting-config 第三段中有对 @SpringBootConfiguration 的描述： The search algorithm works up from the package that contains the test until it finds a class annotated with @SpringBootApplication or @SpringBootConfiguration. As long as you structured your code in a sensible way, your main configuration is usually found. 搜索算法从包含测试的程序包开始工作，直到找到带有 @SpringBootApplication 或 @SpringBootConfiguration 标注的类。只要您以合理的方式对代码进行结构化，通常就可以找到您的主要配置。 这很明显是解释了 SpringBoot 主启动类与测试的关系，标注 @SpringBootApplication 或 @SpringBootConfiguration 的主启动类会被 Spring测试框架 的搜索算法找到。回过头看上面的截图，引用 @SpringBootConfiguration 的方法恰好叫 getOrFindConfigurationClasses，与文档一致。 4. @ComponentScan@ComponentScan这个注解在Spring中很重要，它对应XML配置中的元素，@ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。 我们可以通过basePackages等属性来细粒度的定制@ComponentScan自动扫描的范围，如果不指定，则默认Spring框架实现会从声明@ComponentScan所在类的package进行扫描。 注：所以SpringBoot的启动类最好是放在root package下，因为默认不指定basePackages。 不过在上面的声明中有显式的指定了两个过滤条件： 12@ComponentScan(excludeFilters = &#123; @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) &#125;) 这两个过滤器又是做什么的呢 4.1 TypeExcludeFilter文档注释原文翻译： Provides exclusion TypeFilters that are loaded from the BeanFactory and automatically applied to SpringBootApplication scanning. Can also be used directly with @ComponentScan as follows: 提供从 BeanFactory 加载并自动应用于 @SpringBootApplication 扫描的排除 TypeFilter 。 12&gt; @ComponentScan(excludeFilters = @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class))&gt; Implementations should provide a subclass registered with BeanFactory and override the match(MetadataReader, MetadataReaderFactory) method. They should also implement a valid hashCode and equals methods so that they can be used as part of Spring test’s application context caches. Note that TypeExcludeFilters are initialized very early in the application lifecycle, they should generally not have dependencies on any other beans. They are primarily used internally to support spring-boot-test. 实现应提供一个向 BeanFactory 注册的子类，并重写 match(MetadataReader, MetadataReaderFactory) 方法。它们还应该实现一个有效的 hashCode 和 equals 方法，以便可以将它们用作Spring测试的应用程序上下文缓存的一部分。 注意，TypeExcludeFilters 在应用程序生命周期的很早就初始化了，它们通常不应该依赖于任何其他bean。它们主要在内部用于支持 spring-boot-test 。 从文档注释中大概能看出来，它是给了一种扩展机制，能让我们向IOC容器中注册一些自定义的组件过滤器，以在包扫描的过程中过滤它们。 这种Filter的核心方法是 match 方法，它实现了过滤的判断逻辑： 1234567891011121314151617181920@Overridepublic boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException &#123; if (this.beanFactory instanceof ListableBeanFactory &amp;&amp; getClass() == TypeExcludeFilter.class) &#123; for (TypeExcludeFilter delegate : getDelegates()) &#123; if (delegate.match(metadataReader, metadataReaderFactory)) &#123; return true; &#125; &#125; &#125; return false;&#125;private Collection&lt;TypeExcludeFilter&gt; getDelegates() &#123; Collection&lt;TypeExcludeFilter&gt; delegates = this.delegates; if (delegates == null) &#123; delegates = ((ListableBeanFactory) this.beanFactory).getBeansOfType(TypeExcludeFilter.class).values(); this.delegates = delegates; &#125; return delegates;&#125; 注意看if结构体中的第一句，它会从 BeanFactory （可以暂时理解成IOC容器）中获取所有类型为 TypeExcludeFilter 的组件，去执行自定义的过滤方法。 由此可见，TypeExcludeFilter 的作用是做扩展的组件过滤。 4.2 AutoConfigurationExcludeFilter看这个类名，总感觉跟自动配置相关，还是看一眼它的源码： 1234567891011121314151617181920public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException &#123; return isConfiguration(metadataReader) &amp;&amp; isAutoConfiguration(metadataReader);&#125;private boolean isConfiguration(MetadataReader metadataReader) &#123; return metadataReader.getAnnotationMetadata().isAnnotated(Configuration.class.getName());&#125;private boolean isAutoConfiguration(MetadataReader metadataReader) &#123; return getAutoConfigurations().contains(metadataReader.getClassMetadata().getClassName());&#125;protected List&lt;String&gt; getAutoConfigurations() &#123; if (this.autoConfigurations == null) &#123; this.autoConfigurations = SpringFactoriesLoader.loadFactoryNames(EnableAutoConfiguration.class, this.beanClassLoader); &#125; return this.autoConfigurations;&#125; 它的 match 方法要判断两个部分：是否是一个配置类，是否是一个自动配置类。其实光从方法名上也就看出来了，下面的方法是其调用实现，里面有一个很关键的机制：SpringFactoriesLoader.loadFactoryNames，我们留到下面解释。 小结 @SpringBootApplication 是组合注解。 @ComponentScan 中的 exclude 属性会将主启动类、自动配置类屏蔽掉。 @Configuration 可标注配置类，@SpringBootConfiguration 并没有对其做实质性扩展。 @EnableAutoConfiguration 的作用篇幅较长，单独成篇。]]></content>
  </entry>
  <entry>
    <title><![CDATA[win10安装wsl2和docker]]></title>
    <url>%2F2019%2F08%2F28%2Fwin10%E5%AE%89%E8%A3%85wsl2%E5%92%8Cdocker%2F</url>
    <content type="text"><![CDATA[[TOC] 参考https://zhuanlan.zhihu.com/p/69121280 踩过的坑 WSL2需要Windows build 18917 更新,WSL不需要.但是最好首先将操作系统更新到需要的版本,而不是先安装WSL再更新 不要安装不在windows商店的Linux发行版(包括github上的各种centos以及centos官方docker镜像,centos目前不是windows商店的Linux发行版之一) windows升级过程可能会有多个更新,每次更新都检查C盘,预留10G以上的硬盘空间 安装过程以下默认使用管理员权限打开powershell 1.确认操作系统版本开始菜单(右键)&gt;设置&gt;系统&gt;关于&gt;windows规格 如果版本号(小数点之前的数字)小于18917需要更新,否则进入步骤3安装wsl 2.升级window版本2.1开启快速通道(Insider Preview)开始菜单(右键)&gt;设置&gt;更新和安全&gt;windows预览体验计划 2.2更新开始菜单(右键)&gt;设置&gt;更新和安全&gt;windows更新 开始更新,可能需要重启,时间比较长 更新完成后如下: 操作系统版本大于18917即可 3.安装wsl3.1 开启wsl在powershell命令行中输入如下命令开启wsl 1Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux 可能需要重启 3.2 在windows商店中选择合适的linux发行版这里我选择了Ubuntu,安装 3.3测试wsl命令行中输入wsl -l -v 查看,这里版本应该是1 4.升级到wsl24.1开启VirtualMachinePlatform1Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform 4.2将发行版的wsl版本设置为2命令为wsl --set-version &lt;Distro&gt; 2,例如wsl --set-version Ubuntu 2 如果想使wsl2 成为默认架构，可以使用以下命令执行此操作: 1wsl --set-default-version 2 这将使你安装的任何新发行版初始化为 wsl2 发行版。 4.3测试wsl2 5.安装docker5.1进入wsl2输入wsl -u root进入wsl2,并切换到根目录 5.2快速安装docker123$ curl -fsSL https://get.docker.com -o get-docker.sh$ sudo sh get-docker.sh$ sudo service docker start 测试一下,运行docker run hello-world 5.3在docker中安装mysql在 https://hub.docker.com/_/mysql 找到合适的mysql镜像版本,这里使用mysql5.7.27 运行如下命令 1docker run --name mysql -p3306:3306 -e MYSQL_ROOT_PASSWORD=123qweasd -d mysql:5.7.27 这里启动了一个mysql5.7.27的docker镜像,密码为123qweasd,并将docker的3306端口(mysql默认端口)映射到了wsl2的3306端口 在wsl2中使用ifconfig获取局域网ip记住 6.在windows中测试在Win10中打开IDEA旗舰版,用自带的数据库连接工具连接成功 host和mysql密码和上一步相同 至此,在windows中连接wsl2中的docker中的mysql成功]]></content>
      <categories>
        <category>工具建设</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用BaGet搭建私有nuget服务器]]></title>
    <url>%2F2019%2F08%2F15%2F%E4%BD%BF%E7%94%A8BaGet%E6%90%AD%E5%BB%BA%E7%A7%81%E6%9C%89nuget%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[[TOC] 参考文章https://medium.com/@onurvatan/net-core-custom-nuget-server-baget-on-docker-b763a3c7a276 缘起在微服务中,有公用的业务类库,如果用共享代码的方式引入,每个使用的人会带来版本维护上的诸多麻烦.如果发到官方nuget上,有两个问题 代码太菜T_T 公司内部业务私密问题 为此我们需要搭建一个私有的nuget服务器. 虽然微软官方有nuget-server这种实现方式,但是还是有些麻烦.在查看了官方推荐的第三方实现列表后,决定使用BaGet 步骤公司服务器的环境是CentOS 7,已安装了docker 配置环境在任务目录中新建baget_nuget文件夹,用来存放相关内容,并在其中建立子文件夹baget-data和baget.env文件,文件内容为 1234567ApiKey=你的ApiKeyStorage__Type=FileSystemStorage__Path=/var/baget/packagesDatabase__Type=SqliteDatabase__ConnectionString=Data Source=/var/baget/baget.dbSearch__Type=DatabaseAllowPackageOverwrites=True BaGet的后端使用.net core,前端使用react.baget.env文件存储了BaGet配置所需的环境变量,其中ApiKey是发布类库时需要用到的key 在docker中运行切换到上一步中新建的baget_nuget文件夹下,运行 1docker run --rm --name nuget-server -p 5555:80 --env-file baget.env -v &quot;$(pwd)/baget-data:/var/baget&quot; loicsharma/baget:latest 此时docker的80端口就运行起了Baget的nuget-server并映射到了服务器本机的5555端口 如果有图形界面,应该就可以看见如下的内容了 如果没有图形界面在命令行中用curl http://localhost:5555测试一下,出现html内容文本即可 测试ok后进入下一步 配置nginx映射如果服务器的5555端口可以暴露出去,就可以省略这一步直接到下一步. 因为公司只有一个入口所以需要配置nginx映射 公司访问的域名是www.xxxx.com,我们需要事先在云的CDN在配置好二级域名,这里我们使用nuget.xxx.com nginx配置为 1234567891011121314151617server &#123; listen 80; server_name nuget.xxx.com; location / &#123; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_pass http://localhost:5555/; &#125;&#125;server &#123; listen 80 default_server; // ....其他配置&#125; 这样我们就把对nuge.xxx.com的请求转发到了5555端口 发布库并测试 在本地新建一个TestLib随意输入一些方法 切换到TestLib目录使用dotnet pack命令打包 生成了nupkg打包文件 切换到bin\Debug目录并使用发布 执行dotnet nuget push -s http://nuget.xxx.com/v3/index.json -k 你的ApiKey TestLib.1.0.0.nupkg 发布成功 使用类库 新建一个控制台程序CallTest 在 工具/选项/Nuget包管理器/程序包源 中增加私有nuget服务地址 名称随意 这里我使用了nuget.xxx.com,源填写http://nuget.xxx.com/v3/index.json 添加成功 引入TestLib 选择私有地址,可以找到我们刚刚上传的库,安装 这样就可以在程序中使用了 更新类库 待更新…..]]></content>
      <categories>
        <category>工具建设</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[转载-IL汇编语言介绍（译）]]></title>
    <url>%2F2019%2F08%2F06%2F%E8%BD%AC%E8%BD%BD-IL%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80%E4%BB%8B%E7%BB%8D%EF%BC%88%E8%AF%91%EF%BC%89%2F</url>
    <content type="text"><![CDATA[[TOC] 转载出处原文地址 前言最近在学习IL,在CodeProject上看到一篇老外的文章，介绍IL的，写的比较好，就翻译了一下，供大家参考。水平有限，请大家包涵，如果你想认真学习，推荐你最好去看原文，原文是Introduction to IL Assembly Language。 介绍这篇文章介绍了基本的IL汇编语言知识，你可以用它从底层来分析你的.NET代码（任何.NET平台下的高级语言写的）。从底层，我说的底层是你的高级语言在编译器中完成它工作的地方，用这些基本知识，你可以为.NET语言重新开发一个你自己的编译器。 目录 IL汇编语言介绍 评估堆栈 IL数据类型 变量声明 判断和条件语句 循环 定义方法 通过引用传递参数 创建命名空间和类 对象的作用域 创建和使用类对象 创建构造函数和属性 创建WinForms窗体 错误和调试 总结 结论 任何时候你在.NET中编译你的代码，不管你使用什么语言。它都会被转换成一种中间语言IL（Intermediate Language ），通常也被叫做微软中间语言MSIL（Microsoft Intermediate Language ）或通用中间语言CIL（Common Intermediate Language）。你可以把IL当作是JAVA中的“字节码”（译者注：也是一种中间语言，由JAVA虚拟机编译成的）。如果你对.NET是怎样处理不同数据类型以及怎样把我们写的代码转换成IL代码等等问题感兴趣，那么知道一些IL知识是非常有用的，包括你会知道．NET编译器产生的代码是什么。所以如果你知道IL，那么你可以检查编译器产生的代码是否正确，或者根据需要做一些修改（可能在大多数时候是不需要的），你可以更改IL代码去对程序做一些改动（一些在高级语言中不允许的）增加你代码的性能。这也可以帮助你从底层去分析你的代码。另外，如果你计划为．NET写一个编译器，那么你就必须了解IL。 IL本身是以二进制格式存储的，所以我们不可能阅读。但是和其它二进制代码有汇编语言一样，IL也有一种汇编语言叫作IL汇编语言（ＩＬＡｓｍ），IL汇编语言和其它汇编语言一样有许多指令。例如，两个数相加，有相加的指令，两个数相减，有相减的指令，等等。很显然．NET的运行时中的编译器（JIT）不能直接执行IL汇编语言，如果你用IL汇编语言编写了一些代码，那么首先你要把这些代码编译成IL代码，然后JIT才可以运行这些代码。 注意：请注意IL和IL汇编语言是两个不同的概念，当我们说到IL时，是指．ＮＥＴ编译器产生的二进制代码，而IL汇编语言不是二进制格式的。 注意：请注意我期望你们非常熟悉．ＮＥＴ平台（包括任何高级语言），在这篇文章里面，我不会去详细解释所有的东西，只是那些我认为需要解释的。如果你对一些东西比较迷惑，你可以联系我来进行更深入的讨论。 IL汇编语言简介现在我们开始我写这篇文章的主要目的，介绍IL汇编语言。IL汇编语言和其它汇编语言一样有一系列指令集。你可以在任何文本编辑器里面写IL汇编语言代码，像记事本等等然后用.NET平台自带的命令行编译器(ILAsm.exe)去编译它。对于使用高级语言的程序员来说，IL汇编语言是一种非常难以学习的语言，而对于使用C 或C++的程序员来说，可以很容易的接受它。学习IL汇编语言是很困难的工作，所以我们不要浪费时间，直入正题。在IL汇编语言中，我们要人工的做一切事情，包括数据进栈，内存管理等等。 你可以把IL汇编语言和汇编语言认为是一样的，但是汇编语言是在windos平台下执行的，而IL汇编语言是在.NET平台下执行的，另外还有一点就是IL汇编语言比汇编语言要简单一些，并且还有一些面向对象的特性。 那么我们用一个简单的例子来开始我们对这种语言的学习，在屏幕（控制台）上打印一个简单的字符串。在学习一种新语言的时候都有一种传统，就是创建一个hello world的程序，那么我们今天也这样做，只不过我们把打印的字符串改变了。 123456789101112131415161718192021//Test.IL//A simple programme which prints a string on the console .assembly extern mscorlib &#123;&#125; .assembly Test&#123; .ver 1:0:1:0&#125;.module test.exe .method static void main() cil managed&#123; .maxstack 1 .entrypoint ldstr "I am from the IL Assembly Language..." call void [mscorlib]System.Console::WriteLine (string) ret&#125; 图1.1 用IL汇编语言写的一个简单的测试程序 把上面的代码（图1.1 ）写到一个简单的文本编辑器中，如记事本中，然后把它保存为Test.il。我们先来编译运行这段代码，待会我们会详细的来看这段代码。要编译些段代码，输入以下的命令提示符 1ILAsm Test.il (See the screen shot below) ​ 图1.2 测试程序的输出，你可以看到我用来编译代码的命令 ILAsm.exe 是.NET框架下自带的一个命令行工具，你可以在\Microsoft.NET\Framework\ 文件夹中找到它。当你编译完你的IL文件后，它会输出一个和你IL文件名字相同的exe文件，你可以用指令/OutPut= 更改输出的exe文件的名字，例如ILAsm Test.il /output=MyFile.exe.要运行这个exe文件，你只需要输入这个exe文件的名字，然后输入回车。输出马上会在屏幕上出现。现在让我们用一点时间去理解一下我们所写的代码。记住我在下面描述的代码是图1.1中的代码。 最开始的两行（以//开始的）是注释，在IL汇编语言中，你可用在C#或C++中相同的方式去写注释，写多行注释或在行内写注释，你也可以用/ … / 接下来我们告诉编译器去引用一个叫mscorlib的类库(.assembly extern mscorlib {})。在IL汇编语言中，任何语句都是以一个点号（.）开始的，以此告诉编译器这是一种特殊的指令。所以这里的.assembly 指令告诉编译器，我们准备去用一个外部的类库（不是我们自己写的，而是提前编译好的） 接下来的一个.assembly 指令(.assembly Test ….)定义了这个文件的程序集的信息，在上面的一个例子中，我们假设“Test”是这个程序集的名字，在括号内部是我们想给外部看的一些信息，就是版本信息。我们可以在这里面写上更多有关这个程序集的信息，如公钥等等。 下一条指令告诉了我们程序集中模块的名称(.module Test.exe). 我们知道，每一个程序集中至少应该有一个模块。 接下来的一条指令(.method static void main () cil managed), 这里的.method 标记告诉编译器我们准备定义一个方法，而且还是一个静态（Static）的方法（和C#中一样的关键字）并且返回空（Void）。另外这个方法的名字叫做main，并且它没有参数（因为它后面的圆括号中没有任何东西），最后面的cil managed 指令告诉编译器，把这段代码当作托管代码进行编译。 到方法里面去，第一条指令是最大栈(.maxstack 1). 这是非常重要的一点，它告诉编译器我们要加载到内存（实际是评估堆栈）中去的项的最大数目，我们将会在后面详细的进行讨论，如果你没有注意到，你暂时可以跳过。 .entrypoint 指令告诉编译器去把这个函数标记为整个应用程序的入口点（Entry Point ），也就是执行这个应用程序时最先执行的函数。 接下来的一个语句是(ldstr “I am from the IL Assembly Language…”), ldstr指令是用来把一个字符串加载到内存或评估堆栈中。在我们使用这些变量之前，是需要把这些变量加载到评估堆栈（evaluation stack ）中去的。我们在下面马上就会详细的讨论评估堆栈的。 下一条指令(call void [mscorlib]System.Console::WriteLine (string)) ，是调用一个在mscorlib 类库中的方法。注意我们调用时告诉了所有关于这个方法的信息，包括返回的类型，参数类型以及所属的类库。我们把后面的(string))当作参数，string并不是一个变量，而是一种数据类型。前面的语句(ldstr “I am from the IL…..”)已经把这个字符串加载到栈里面去了，这个方法将会打印已加载进去了的字符串。 最后一句ret，尽管不需要去解释，它的意思是表示从方法中返回。 通过上面的一些讲解，你可能对怎样去写IL汇编代码有一个大致的想法了，你也会认为IL汇编语言和高级语言是不同的像.NET下的语言(VB, C#)，然而，无论你写的代码是什么，你必须去遵守类似的规则（尽管操作类的时候可能会有一些改变），现在还有很多事情需要更详细的讨论，最主要的就是评估堆栈，那么我们先从它开始吧。 Evaluation Stack评估堆栈评估堆栈可以认为是普通机器中的栈，栈是用来存储语句在执行之前的信息的。我们知道当我们对信息进行一些操作时，这些信息是要存入在内存中的。就像我们在汇编语言中执行指令之前都要把值移到寄存器中去，同样的我们要在进行操作（在上面的例子中就是输出）之前把信息（在上面的例子中也就是那个字符串）移到栈中，在我们的main方法（图1.1）之前，我们注意到，我们在执行我们的函数期间需要在.NET运行时中（CLR）存储一些信息。我们用maxstack指令说明了，我们将会把一个值移到栈中，只移动一次。因此如果我们把指令写成.maxstack 3，那么运行时（CLR）就会在栈中开辟可以放三个变量的空间，任何时候都可以使用。注意，这并不是说明在我们的函数执行期间我们只能加载三个参数到栈中，而是说我们一次最多只能加载三个变量到栈中去。当执行完毕后，变量将会从栈中移除，所以我们还需要注意，不管函数什么时候调用，这个函数中被用到的参数在函数调用完毕后都会被从栈中移出，栈中的空间将会空出来。这也就是.NET中垃圾回收器所做的事。可以移到栈中去的数据类型是没有限制的，我们可以把任何数据（比如字符串，整形，对象等等）在任何时候加载到栈中去。 我们来看另外一个例子，它可以让我们对评估堆栈的概念有一个更清晰的认识。 12345678910111213141516171819202122232425//Add.il//Add Two Numbers .assembly extern mscorlib &#123;&#125; .assembly Add&#123; .ver 1:0:1:0&#125;.module add.exe .method static void main() cil managed&#123; .maxstack 2 .entrypoint ldstr "The sum of 50 and 30 is = " call void [mscorlib]System.Console::Write (string) ldc.i4.s 50 ldc.i4 30 add call void [mscorlib]System.Console::Write (int32) ret&#125; 图1.3两数相加 main函数中的一部分与例1是一样的，只是模块的名称改变了。我们要讨论的就是main函数中的.maxstack 2，它告诉运行时去分配足够的内存空间存储两个值。然后我们加载一个字符串到栈里面去，然后把它打印出来。然后我们同时加载两个整型数到内存（译者注：在篇文章中，内存就是指栈）中去（用ldc.i4.s和ldc.i4指令），然后执行相加指令最后输出一个整型的数字。相加的指令会在栈里面找两个数字，如果找到了，那么它就会把这两个数相加并把结果放在栈的顶部。相加后，调用另外一个函数Write，在控制台输出。调用这个方法之前要确保栈顶一定要有值。在这个例子里面，它会找一个整型，如果它找到了一个整型的数字，它将会把它打印出来，否则它会报错。 1不要对ldc.i4.s和ldc.i4感到迷惑，两者都是加载一个整型的数字，但是前者是单字节类型，后者是一个占四字节的数字。 我希望你明白使用评估堆栈的方式以及它是如何工作的，现在我们去讨论更多关于IL汇编语言的知识 IL数据类型学习一门新语言，首先我们应该去了解这门语言的数据类型。所以我们首先先看一下下面的这张表（图１.４），去了解一下IL汇编语言中的数据类型。但是在看之前，我要指出，在.NET平台下的各种不同语言中，数据类型没有一致性，例如一个整型数（32位），在VB.NET中定义为Integer，但是在C#和C++中被定义成int，尽管如此，在这两种情况下，它们统统是指System.Int32.另外我们记住它是否符合CLS（Common Language Specification ）规范。就像UInt32 ，在VB.NET中就没有，同时也不被CLS承认。 图1.4 IL汇编语言中的数据类型 我们也记得一些在IL汇编语言中的数据类型比如.i4, .i4.s, .u4 等等我们在上面的例子中用到过的。上面图表列出的数据类型都是被IL汇编语言所识别的，而且在表中也提到了哪些是符合ＣＬＳ规范，哪些是不符合的。所以把这些类型都记在脑海里。我们可以以下列形式调用任何函数： 1call int32 SomeFunction (string, int32, float64&lt;code lang=msil&gt;) 它的意思是函数SomeFunction 返回一个int32 (System.Int32)的类型, 传入其它三种类型string (System.String), int32 (System.Int32) and float64(System.Double) 的参数. 注意这些都是CLR和IL汇编语言中的基本数据类型. 如果我们对非基本类型(自定义类型)是怎样处理的感兴趣，我们可以如下写： 123456//In C#ColorTranslator.FromHtml(htmlColor) //In ILAsmcall instance string [System.Drawing]System.Drawing.ColorTranslator::ToHtml(valuetype [System.Drawing]System.Drawing.Color) 你可能注意到，我们显示的声明了参数的类型。我们也定义了这个类型所在的命令空间，而且用一个关键字标识了我们将要引用的是一个非基本的数据类型。 在接下来的部分，我将用一个示例程序来演示使用这些类型，你对这些类型的认识将变得更清晰。但是首先，我们还是先来学习一下语言的基础，比如变量声明，循环，判断条件等等。 变量声明变量是每个程序语言中最主要的一部分，因此IL汇编语言也提供了一种我们声明和使用变量的方法。尽管没有高级语言(VB .NET, C#) 中的那样简单。在IL汇编语言中.locals 指令是用来定义变量的，这条指令一般是写在函数的最开始部分的，尽管你可以把变量声明放在任何地方，当然肯定要在使用前。下面是一个例子来演示怎样定义变量，给变量赋值，以及使用变量把它打印出来。 123456789.locals init (int32, string)ldc.i4 34stloc.0ldstr "Some Text for Local Variable"stloc.1ldloc.0call void [mscorlib]System.Console::WriteLine(int32)ldloc.1call void [mscorlib]System.Console::WriteLine(string) 图1.5 局部变量 我们用.locals 定义了两个变量，一个是int32类型的，另外一个是string类型的。然后我们把一个整型数34加载到内存中去并且把这个值赋给了局部变量0，也就是第一个变量。在IL汇编语言中我们可以通过索引（定义的序号）来访问这些变量，这些索引是从0开始的。然后我们加载一个字符串到内存中然后把它赋给第二个变量。最后我们把这两个变量都打印出来了。ldloc.? 指令可以用来加载任何类型的变量到内存中去（整型，双精度，浮点型或者对象）。 我没有用到变量的名字，因为这些变量都是局部变量，我们不准备在方法外面去使用它。但是这并不代表你不能通过名称来定义变量。当然，肯定可以。定义局部变量的时候，你可以用它们的类型来来给这些变量取名。就像C#中。例如.locals init (int32 x, int32 y) 。 然后，你可以用同样的方法来加载或给这些变量赋值，例如用变量的名字来加载变量可以写成如下：stloc x 或ldloc y。尽管你是用名称来定义这些变量的，但是你照样可以通过索引来访问这些变量，例如ldloc.0, stloc.0等等。 1注意：这篇文章的所有代码中，我用的都是没有名字的变量。 现在你知道了怎样去操作变量以及栈了，如果你有什么问题，就复习一下上面的代码，因为下面我们将会处理一些和栈有关的比较难的问题。我们将会频繁的把数据加载到内存中去，然后取出。所以在学习下面的内容之前，熟悉怎样初始化变量，怎样对变量赋值以及怎样把变量加载到内存中去是非常必要的。 判断/条件判断和条件也是程序语言中不可缺少的部分，在低级语言中，例如本地汇编语言，判断是用jumps (or branch)，在IL汇编语言中，也是这样的，我们来看一下下面的代码片断。 12345br JumpOver //Or use the br.s instead of br//Other code here which will be skipped after getting br statement.//JumpOver://The statements here will be executed 把上面的语句与任何高级语言中的goto语句进行比较，goto语句是把控制流程转到写在goto语句后面的标签处。但是在这里，br代替了goto。如果你确定你要跳转的目标与跳转语句在-128到+127字节之间，那么你可以使用br.s，因为br.s会用int8来代替int32来代表跳转偏移量。上面方法中的跳转是无条件的跳转，因为在跳转语句之前没有判断条件，所以每次执行时程序都会直接跳转到JumpOver标签处。下面我们来看一个代码片段，使用条件跳转的，只有满足条件的才能进行跳转。 12345678910111213141516171819202122232425262728293031//Branching.ilmethod static void main() cil managed .maxstack 2 .entrypoint //Takes First values from the User ldstr "Enter First Number" call void [mscorlib]System.Console::WriteLine (string) call string [mscorlib]System.Console::ReadLine () call int32 [mscorlib]System.Int32::Parse(string) //Takes Second values from the User ldstr "Enter Second Number" call void [mscorlib]System.Console::WriteLine (string) call string [mscorlib]System.Console::ReadLine () call int32 [mscorlib]System.Int32::Parse(string ) ble Smaller ldstr "Second Number is smaller than first." call void [mscorlib]System.Console::WriteLine (string) br Exit smaller: ldstr "First number is smaller than second." call void [mscorlib]System.Console::WriteLine (string)exit: ret 图1.6只有主函数 上面的程序接收了两个用户输入的数字,然后找出较小的一个.在这些语句里面需要注意的是“ble Smaller”语句,它告诉编译器去检查栈里面的第一数是否小于或等于第二个数,如果是小于,那么它将会跳转到”Smaller”这个标签处.如果大于第二个数,那么就不会执行跳转,接着执行下面的语句.也就是加载一个字符串然后输出.在这之后,有一个无条件的分支,这是非常必要的,因为如果没有的话,那么按照程序的流程,在”Smaller”标签后面的语句将会被执行.所以我们加了一个“br Exit”,就是让它跳转到”Exit”标签处然后执行这条语句,退出程序. 还有其它的一些判断式包括beq (==), bne(!= ),bge (&gt;= ),bgt(&gt;), ble (&lt;= ), blt(&lt;) ,还有brfalse (如果栈顶的元素是0),brtrue(如果栈顶的元素非0).你可以用其中的任何一个去执行你程序中的一部分代码然后跳过其它的.就如我在前面提到的,在IL汇编语言中没有高级语言中的那些便利措施,如果你计划用IL汇编语言写代码,那么所有的事情你必做亲自做. 循环在程序语言中比较基础的另外一部分就是循环.循环就是一遍遍执行重复的一段代码.它包括一些跳转分支,由循环里面的索引变量(判断是否满足条件)决定是否跳转.同上面一样,你需要看一下代码,然后花一点时间去理解循环是怎样工作的. 1234567891011121314151617181920212223242526272829.method static void main() cil managed //Define two local variables .locals init (int32, int32) .maxstack 2 .entrypoint ldc.i4 4 stloc.0 //Upper limit of the Loop, total 5 ldc.i4 0 stloc.1 //Initialize the Starting of loop Start: //Check if the Counter exceeds ldloc.1 ldloc.0 bgt Exit //If Second variable exceeds the first variable, then exit ldloc.1 call void [mscorlib]System.Console::WriteLine(int32) //Increase the Counter ldc.i4 1 ldloc.1 add stloc.1 br StartExit: ret 图1.7 只有主函数 在高级语言中,例如C#,上面的代码会写成下面的形式: 12for (temp=0; temp &lt;5; temp++) System.Console.WriteLine (temp) 让我们检查一下上面的代码，首先我们声明了两个变量，第一个变量初始化为4第二个变量初始化为0.循环是从“Start”标签处真正开始的，首先我们检查循环变量（变量2, ldloc 1）是否超过了循环变量的上界（变量1, ldloc 0），如果超过了循环变量的上限，那么程序就会跳转到“Exit”指令处，结束整个程序。如果没有超过，那么这个变量将会被打印到屏幕上，然后循环变量加1，然后又到“start”指令处，再来判断循环变量是否超过上限。这主是IL汇编语言中循环的工作机理。 定义方法上面我们学习了，判断（条件和分支），循环，变量声明。现在我们来讨论在IL汇编语言中怎么去创建方法。IL汇编语言中创建方法与Ｃ＃和Ｃ＋＋中创建函数基本一样，只是有一点点改变，我希望到现在你们也能够猜到。所以下面我们先来看一段代码片断，然后我们来讨论写的这些代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344//Methods.il//Creating Methods .assembly extern mscorlib &#123;&#125; .assembly Methods&#123; .ver 1:0:1:0&#125;.module Methods.exe .method static void main() cil managed&#123; .maxstack 2 .entrypoint ldc.i4 10 ldc.i4 20 call int32 DoSum(int32, int32) call void PrintSum(int32) ret&#125; .method public static int32 DoSum (int32 , int32 ) cil managed&#123; .maxstack 2 ldarg.0 ldarg.1 add ret&#125;.method public static void PrintSum(int32) cil managed&#123; .maxstack 2 ldstr "The Result is : " call void [mscorlib]System.Console::Write(string) ldarg.0 call void [mscorlib]System.Console::Write(int32) ret&#125; 图１.７定义及调用方法 一个简单的把两数相加然后打印出结果的程序，为了简化代码，这两个数是提前定义好的。我们在这里定义了两个方法。注意两个方法都是静态的，所以我们可以不用创建实例直接调用。首先我们加载这两个数到栈中，然后调用第一个方法Dosum，它需要两个参数。在这个方法中，方法里面的声明与主函数中的差不多，我们在上面的例子中已经见过很多次了。我们又定义了评估堆栈的大小maxstack，但是注意我们没有定义入口点.entrypoint ，因为每个程序只能有一个入口点，在上面的的这个例子中，我们把主函数定义成了入口点。ldarg.0和ldarg.1指令告诉运行时加载两个数到评估堆栈，也就是传进来的两个参数。然后我们用add语句把这两个数简单的相加，返回结果。注意这个方法返回一个Int32类型的整数。那么哪一个值将会被返回呢？当然是“相加”指令执行完毕后，在栈上面的数。同时我们调用一个也需要传入一个int32类型参数的方法PrintSum。因此DoSum方法返回的值将会传入到PrintSum方法中去，在PrintSum方法中，首先打印一个简单的字符串，然后加载传进来的一个参数，也把它打印出来。 1从上面我们可以看出在IL汇编语言中，创建一个方法并不是很难。是的，确实不难。在方法中也有引用传值，那么下面我们来看一下用引用传递参数。 用引用传递参数IL也支持引用传递参数，这是理所当然，因为．ＮＥＴ下的高级语言中支持引用传递参数，而这些高级语言的代码又会转换成IL代码。而我们讨论的IL汇编语言就是产生IL代码的。当我们用引用传递参数的时候，会把参数在内存中的存储地址传递给相应的方法而不是像用值传递参数时，把值的副本传递给方法。我们用一个例子来看看IL汇编语言中是怎样用引用传递参数的。 123456789101112131415161718192021222324252627282930313233343536373839.method static void main() cil managed&#123; .maxstack 2 .entrypoint .locals init (int32, int32) ldc.i4 10 stloc.0 ldc.i4 20 stloc.1 ldloca 0 //Loads the address of first local variable ldloc.1 //Loads the value of Second Local Variable call void DoSum(int32 &amp;, int32 ) ldloc.0 //Load First variable again, but value this time, not address call void [mscorlib]System.Console::WriteLine(int32) ret &#125; .method public static void DoSum (int32 &amp;, int32 ) cil managed &#123; .maxstack 2 .locals init (int32) //Resolve the address, and copy value to local variable ldarg.0 ldind.i4 stloc.0 ldloc.0 //Perform the Sum ldarg.1 add stloc.0 ldarg.0 ldloc.0 stind.i4 ret&#125; 图１.８用引用传递参数 在上面的例子中比较有趣的就是我们使用了一些新的指令比如ldloca，它的作用是加载某个变量的地址到栈中去，而不是它的值。在主函数中我们声明了两个局部变量，然后分别对它们赋初值１０和２０。然后我把第一个变量的地址和第二个变量的值加载到栈里面去，然后调用DoSum函数。如果你看到了这个函数调用的签名，那么你就会发现我们在第一个参数前面加了一个&amp;，说明栈里面加载的将会是变量的地址而不是变量的值，它告诉编译器我们将会用引用来传递参数。同样的在函数定义的地方，你也会看到第一个参数前面有同样的一个符号&amp;，当然不用说，也是告诉编译器参数需要以引用的方式传递进来。所以第一个参数是通过引用传递，第二个参数是传递值。现在问题就是怎样通过这个引用地址来得到这个值，以便我们后面对这个值进行一些操作，或者对这个变量重新赋值，如果有需要的话。为了达到这个目的，我们先把第一个参数（实际上是一个值的地址而不是值本身）加载到栈里面去，然后用ldind.i4 指令通过这个地址得到它的值（到这个地址所指的地方去，读出所指向的值，然后加载到栈里面去）。我们把那个存到一个局部变量里面去，以便于我们后面可以重复的使用（如果不这样的话，我们后面要使用这个值的时候，就要重复这些步骤）。然后，很简单，我们得到那个地址所指向的值和第二个参数（通过值传递的），把它们加载到栈里面去，然后相加，最后把结果存储到同样的局部变量里面去。更有趣的一件事是我们在内存中改变了第一个参数（通过引用传递的参数）所指向的值。我们首先是加载第一个参数（通过引用传递的那个参数）到栈里面去，实际上是传进来的那个值的地址。然后加载我们想要被前面那个地址所指向的值（译者注：相当是改变前面的地址所指向的值）。然后用和我们上面用到的ldind.i4 指令相反的指令stind.i4。这条指令把已存在栈里面的值存到已经加载到栈里面的一个内存地址里面去 。（译者注：stind.i4的作用是在所提供的地址存储 int32 类型的值。在调用此指令之前，要确保地址和值已经加载到栈里面）。我们在主函数中打印出来这个值来看它是否改变了，注意DoSum方法并没有返回任何东西。在主函数中，我们仅仅需要重新加载第一个变量（现在是值，而不是内存地址），然后调用WriteLine方法把它打印出来。 这就是IL汇编语言中的引用变量，到现在为止，我们学习了声明变量，条件判断，循环，方法（值传递和引用传递）的使用方法，现在是该学习怎样在IL汇编语言中定义命名空间和类的时候了。 创建命名空间和类当然，在IL汇编语言中肯定可以创建你自己的类和命名空间。实际上，就像其它任何高级语言一样，在IL汇编语言中创建自己的类以及命名空间是非常容易的。不信吗？那我们接下来看一看。 123456789101112131415161718192021222324252627//Classes.il//Creating Classes.assembly extern mscorlib &#123;&#125; .assembly Classes &#123; .ver 1:0:1:0 &#125; .module Classes.exe.namespace HangamaHouse &#123; .class public ansi auto Myclass extends [mscorlib]System.Object &#123; .method public static void main() cil managed &#123; .maxstack 1 .entrypoint ldstr "Hello World From HangamaHouse.MyClass::main()" call void [mscorlib]System.Console::WriteLine(string) ret &#125; &#125; &#125; 图1.9创建自己的命名空间和类 我想，现在上面的代码不需要过多的解释了。非常简单。.namespace 指令，后面跟着一个名字HangamaHouse，它告诉编译器我想要创建一个叫HangamaHouse的命名空间。在这个命名空间里，我们用.class 指令定义了一个类，并且告诉编译器，我这个类是公有的，它继承于System.Object 这个父类。我们定义的这个类里面只有一个公有的静态方法。其余的代码相信你肯定很熟悉。 12这里我还想提一下，那就是你创建的所有类如果没有说明，那么默认都是继承于Object基类。像我们的这个例子，我们显示说明了这个类是继承于命名空间System里面的一个Object基类。如果我们没有显示的说明，它还是默认继承于Object这个基类。当然你定义的类也可以继承于别的类，那么你的类就不是继承于Object类（但是你的类所继承的类很有可能继承于Object，译者注：所有的类最终都是继承于Object这个类）。在上面的创建类的过程中还有两个关键字ansi和auto。Ansi告诉类中所有的字符串必须转换成ANSI（American National Standards Institute）字符。还有其它的一些选项是unicode和autochar（根据不同的平台，会自动转换成相对应的字符集）。另外一个关键字auto告诉运行时（CLR）自动为非托管内存中的对象的成员选择适当的布局。对应这个关键字的其它的选项还有sequential（对象的成员按照它们在被导出到非托管内存时出现的顺序依次布局）explicit（对象的各个成员在非托管内存中的精确位置被显式控制）。想获得更多的信息请参考MSDN上的StructLayout或LayoutKind枚举变量。auto和ansi是类中默认的关键字，如果你没有定义任何东西，那么它们将会被自动的附上。 对象的作用域（成员访问修饰符）下面的表格总结了一下IL汇编语言中类的作用域 图1.10IL汇编语言中的成员访问修饰符 还有其它的一些可以用在方法和字段（类中的变量）前面的修饰符。你可以在MSDN上找到一个完整的列表。 创建和使用类的对象在这一部分，我将向你们演示怎样在IL汇编语言中创建一个类的对象并使用它。在这之前，你必须知道怎样在IL汇编语言中创建你自己的命名空间和类。但是如果不使用它，那么创建的这些东西就没有用，所以我们来开始创建一个简单的类并使用它。 我们来在IL汇编语言中创建我们自己的一个类库。这个简单的类库只包含一个公有的方法，这个方法接受一个变量并且返回这个变量的平方。简单的比较容易理解，我们来看代码： 12345678910111213141516171819202122232425.assembly extern mscorlib &#123;&#125;.assembly MathLib&#123; .ver 1:0:1:0&#125; .module MathLib.dll .namespace HangamaHouse&#123; .class public ansi auto MathClass extends [mscorlib]System.Object &#123; .method public int32 GetSquare(int32) cil managed &#123; .maxstack 3 ldarg.0 //Load the Object's 'this' pointer on the stack ldarg.1 ldarg.1 mul ret &#125; &#125;&#125; 图1.11求数学平方的类库 注意：把上面的代码编译成DLL文件，用ILAsm MathLib.il /dll 指令 上面的代码看起来很简单，它定义了一个命名空间HangamaHouse，然后在命名空间里面定义了一个MathClass类，这个类和上面的代码（图1.10）中定义的类一样继承于System命名空间里面的Object类。在这个类里面我们定义了一个需要传入一个int32类型参数的方法GetSquare。 我们把maxstack的大小定义为3，然后加载第0个参数。接下来我们重复的加载两次第二个参数。等等，我们在这里只接受了一个参数，但是我们却加载了参数0和参数1（总共两个参数）。这怎么可能？确实可以，实际上第一个参数(ldarg.0)是这个对象的this指针的引用，因为每个对象的实例都会把自己在内存中的地址也传进来。所以实际上我们的参数是从索引1处开始的。我们加载第一个参数两次去为了后面执行mul指令把这两个数相乘。最后的结果将会放在栈的顶部，然后在我们调用ret指令的时候，返回给调用这个方法的地方去。 这个类库编译没有问题，我们来看看一个使用这个类库的例子 1234567891011121314151617181920.assembly extern mscorlib &#123;&#125;.assembly extern MathLib &#123;.ver 1:0:1:0&#125;////rest code here//.method static void Main() cil managed&#123; .maxstack 2 .entrypoint .locals init (valuetype [MathLib]HangamaHouse.MathClass mclass) ldloca mclass ldc.i4 5 call instance int32 [MathLib]HangamaHouse.MathClass::GetSquare(int32) ldstr "The Square of 5 Returned : " call void [mscorlib]System.Console::Write(string) call void [mscorlib]System.Console::WriteLine(int32) ret&#125; ​ 图1.12使用类库中的MathClass类 这个方法里面最前面的两行很简单。我们看到第三行定义了一个MathClass类型的局部变量（HangamaHouse命名空间中的，注意我们已经在导入mscorlib 类库的时候导入了这个类库）。我们也提供了这个类库的版本信息，尽管可以不需要，因为我们在前面引用外部类库mscorlib类库的时候没有提供版本信息。另外在我们要创建的对象类型前面，我们加了关键字valuetype，我们也提供了这个类的完整签名包括类库的名字。接下来我们加载了局部变量mclass的地址到栈里面。然后加载了一个整型数5到栈里面，接着调用了MathClass类中的GetSquare方法。之所以能够这样做，是因为我们在之前已经加载了mclass类的对象，这个对象的内存引用已经加载到栈里面去了。当我们调用MathClass类的GetSquare方法时，它会在栈上面找这个对象的引用以及所要传入的参数。如果找到了，它就使用这个引用变量去调用那个方法。另外一个我们还注意的就是在调用方法的时候我们用到了一个我们以前从来没有用到的关键字instance，instance关键字告诉编译器我们将会以对象的实例来调用方法，它不是静态的方法。执行完这些指令后，GetSquare方法返回一个int32类型的数并把它存入栈里面，我们后面就以字符串的形式在控制台把这个数打印出来。 所以在这里最重要的事就是用.locals 指令和valuetype以及完整的签名包括类库名来声明一个类的对象，第二就是调用这个类里面的方法，它首先会加载这个类对象的引用，然后加载要传递给这个方法的任何变量，最后调用这个方法的时候加上关键字instance。 同样的，我们可以在类里面用属性和构造函数，并且在外面的代码里面使用它们。我这篇文章的下一部分将会介绍怎样在IL汇编语言中创建私有字段，构造函数和属性，然后用IL汇编代码去调用它们。 创建构造函数和属性构造函数在高级语言中是在创建对象时会调用的一个方法，但是在低级语言中，像IL汇编语言，你需要人工的去调用它，它是不返回任何东西的一个方法。下面的示例代码演示了怎样创建构造函数。我只是把需要的代码拿出来了，这篇文章的源代码里面包括了这一部分的所有代码。在阅读这一部分的时候，一定要集中注意力，因为这一部分将会教给你很多东西。 123456789101112131415.class public MathClass extends [mscorlib]System.ValueType&#123; .field private int32 mValue //Other code goes here .method public specialname rtspecialname instance void .ctor() cil managed &#123; ldarg.0 ldc.i4.s 15 stfld int32 HangamaHouse.MathClass::mValue ret &#125; //Other code goes here. 图1.13 MathLib类的构造函数 第一个你会注意到的就是我创建的类是继承于System.ValyeType而不是继承于Object。在.NET里面，一个类实际上也是一种类型所以它继承于ValueType。在这之前我的类是继承于Object类，但是现在不是时候去讨论这两者的区别了，因为我们要创造一个完整的类（有构造函数，属性等等），如果你不想利用类的这些特性，你可以让你的类继承于任何类。 在声明了类之后，我定义了一个私有字段mValue（高级语言中的私有变量）。然后我用.method 指令声明了一个构造函数。记住，构造函数也是一个方法。现在你会对用.ctor代替类名（在高级语言像c++中也是如此）觉得吃惊。是的，在IL汇编语言中.ctor就代表构造函数。这是一个默认的构造函数，因为它没有任何参数。我们在这里做的就是用ldarg.0语句来加载对象的引用，然后我们加载一个常量15，赋值给类中的私有变量mValue。stfld语句可以用来对任何字段赋值。我们提供了这个字段的完整签名。我想你现在应该不会吃惊我们为什么要这样做。最后我们从这个方法（构造函数）中返回。 你可能也注意到了我们在声明构造函数的时候用了一系列关键字，包括specialname和rtspecilaname.实际上，这些关键字告诉运行时把这些方法的名字当作一种特殊的名字。你可以在声明构造函数或属性的使用它，但是这些不是必要的。 不像高级语言那样，在IL汇编语言中，构造函数不是自动调用的，而是需要你显示的调用它，下面的一个代码片断演示了怎样调用一个构造函数去初始化类中的变量。 12345678.method public static void Main() cil managed&#123; .maxstack 2 .entrypoint .locals init (valuetype [MathLib]HangamaHouse.MathClass mclass) ldloca mclass call instance void [MathLib]HangamaHouse.MathClass::.ctor() 上面的代码创建一个MathClass类型的局部变量mclass，这个类型是在HangamaHouse命名空间里面的。然后我们加载这个对象变量的地址到材里面去，然后调用构造函数（.ctro方法），如果你仔细观察，你就会发现它和我们在IL汇编语言中调用其它普通方法一样，没有区别，同样的，你可以定义重载的构造函数当你在定义.ctor方法的时候，让它接受几个参数。然后像我们调用默认构造函数一样去调用它。 现在来讨论属性，其实属性实质上也是方法，看一下它的特性，我们就会完全明白。 1234567891011121314151617181920.method specialname public instance int32 get_Value() cil managed&#123; ldarg.0 ldfld int32 HangamaHouse.MathClass::mValue ret&#125;.method specialname public instance void set_Value(int32 ) cil managed&#123; ldarg.0 ldarg.1 stfld int32 HangamaHouse.MathClass::mValue ret&#125; //Define the property, Value.property int32 Value()&#123; .get instance int32 get_Value() .set instance void set_Value(int32 )&#125; 图1.15属性 你可以看一下上面的代码，你会肯定的说，这和方法的代码一样。但是你在这可以看到另外一个东西，那就是.property 指令，在里面它定义了两个东西，一个是属性get，一个是属性set。说明这两个方法属于属性的一部分。我们可以说这两个方法在上面都定义了，一个是get_Value 方法一个是set_Value方法。这些方法就像在文章中出现的一些普通方法，调用属性很简单，因为它们就像方法一样。 1234567891011.maxstack 2 .localsinit (valuetype [MathLib]HangamaHouse.MathClass tclass) ldloca tclassldc.i4 25 call instance void [MathLib]HangamaHouse.MathClass::set_Value(int32) ldloca tclass call instance int32 [MathLib]HangamaHouse.MathClass::get_Value()ldstr "Propert Value Set to : " call void [mscorlib]System.Console::Write(string) call void [mscorlib]System.Console::WriteLine(int32) 图1.16 使用属性，GetSquare方法也被调用了 不要吃惊，我们创建了一个类的实例，然后调用了set_Value方法（实际上是一个属性，我们准备去修改这个属性的值）。然后为了证实一下值是否被修改了，我们重新读取了一下这个属性，然后把它打印出来。 到现在为止，我们已经讨论了IL汇编语言中的大部分内容。但是还有很重要的一部分，那就是错误和调试。（译者注：老外这好像写错了，因为调试是在最后一部分，下面实际上是讲如何创建窗体） 创建窗体这部分内容告诉我们怎样联系上面所讲的内容创建一个简单的GUI，Windows窗体。在这个应用程序里面，我从System.Windows.Forms.Form 类中继承创建了一个简单的窗体，它没有包含任何控件，但是我修改了一些它的属性，例如BackColor,Text和WindowState。代码一步一步的比较简单。大家一起来看一下我结束这篇文章之前的最后一段代码。 123456789101112131415161718.namespace MyForm&#123; .class public TestForm extends [System.Windows.Forms]System.Windows.Forms.Form &#123; .field private class [System]System.ComponentModel.IContainer components .method public static void Main() cil managed &#123; .entrypoint .maxstack 1 //Create New Object of TestForm Class and Call the Constructor newobj instance void MyForm.TestForm::.ctor() call void [System.Windows.Forms] System.Windows.Forms.Application::Run( class [System.Windows.Forms]System.Windows.Forms.Form) ret &#125; 图1.17Windows Form的入口函数 这是整个应用程序的入口函数，首先（在MyForm命名空间里面创建类TestForm后），我们定义了一个IContainer的局部变量（字段）。注意在定义这种字段之前，我们加上了关键字class 。然后在主函数里，我们用newobj指令创建了一个TestForm 类的对象。然后我们调用Application.Run 方法去运行这个应用程序。如果你把它与高级语言相比，你就会发现，它和我们现在用的手法是一样的。现在我们来看看我们类（TestForm）的构造函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445.method public specialname rtspecialname instance void .ctor() cil managed &#123; .maxstack 4 ldarg.0 //Call Base Class Constructor call instance void [System.Windows.Forms] System.Windows.Forms.Form::.ctor() //Initialize the Components ldarg.0 newobj instance void [System]System.ComponentModel.Container::.ctor() stfld class [System]System.ComponentModel.IContainer MyForm.TestForm::components //Set the Title of the Window (Form) ldarg.0 ldstr "This is the Title of the Form...." call instance void [System.Windows.Forms] System.Windows.Forms.Control::set_Text(string) //Set the Back Color of the Form ldarg.0 ldc.i4 0xff ldc.i4 0 ldc.i4 0 call valuetype [System.Drawing]System.Drawing.Color [System.Drawing]System.Drawing.Color::FromArgb( int32, int32, int32) call instance void [System.Windows.Forms] System.Windows.Forms.Control::set_BackColor( valuetype [System.Drawing]System.Drawing.Color) //Maximize the Form using WindowState Property ldarg.0 ldc.i4 2 //2 for Maximize call instance void [System.Windows.Forms] System.Windows.Forms.Form::set_WindowState( valuetype [System.Windows.Forms] System.Windows.Forms.FormWindowState) ret &#125; 图1.18TestForm中的.ctor方法（构造函数） 非常简单，我们只是调用了基类的.ctor方法（构造函数）。然后我们创建一个Container 类的对象，把它当作我们的一个组件对象（类中的字段），窗体初始化到此就完成了。接下来我们为我们的新窗体设置一些属性。首先设置窗体的标题（Text属性）。我们加载一个字符串到栈里面去，然后调用Control类的 set_Text方法（因为Text属性是从Control继承而来的），设置了Text属性后，我们开始去设置BackColor属性。我们调用FromArgb方法从红，绿，蓝中获取颜色值。我们首先加载了三个值到栈里面然后调用Color.FromArgb方法去得到一个Color类的对象，然后把这个Color值赋给BackColor属性。我们跟前面设置Text属性一样的方式来设置BackColor属性。最后我们把WindowState 属性设置为Maximized（最大化），用同样的方式。你可能注意到了我们加载了一个常量到栈里面去，这个常量是一个FormWindowState 的枚举值，每个枚举变量都已经提前定义好了值。 尽管创建窗体的代码已经完成了，但是我们在这里也定义了一个Dispose事件（窗体的析构函数），通过这个事件我们可以移除不需要的对象以此来清理内存。如果你对Dispose事件的代码感兴趣，那么看一下几行代码。 12345678910111213141516.method family virtual instance void Dispose(bool disposing) cil managed &#123; .maxstack 2 ldarg.0 ldfld class [System]System.ComponentModel.IContainer MyForm.TestForm::components callvirt instance void [mscorlib]System.IDisposable::Dispose() //Call Base Class's Dispose Event ldarg.0 ldarg.1 call instance void [System.Windows.Forms] System.Windows.Forms.Form::Dispose(bool) ret &#125; 这个Dispose方法是重载的，所以它被声明为虚方法。我们只需要加载这个对象的引用（this），加载一个组件(component)字段，然后通过IDisposable调用Dispose方法，然后调用我们窗体的Dispose方法就可以了。 所以创建一个用户界面（UI）不是一件非常困难的事（尽管有一点）。从现在开始，你可以在你的窗体里面加上其它的控件比如textBox,lable等等，然后响应它们的事件，你能吗？ 1错误和调试 每门程序语言里面都有错误，IL汇编语言中也不例外。像其它语言中的错误一样，在ＩＬ汇编语言中，你也可以遇到编译错误（语法错误），运行错误，逻辑错误。我不打算去详细的讲解这些错误是什么，因为你们都非常熟悉。这部分的目的是介绍一些帮助你调试程序的工具和技巧。首先在你编译程序的时候可以生成一个ｄｅｂｕｇ文件，在用ILAsm.exe编译代码时加上/debug分支即可，如下 11: ILAsm.exe Test.il /debug 它将会产生一个叫Text.exe的exe文件和一个叫Test.pdb的调试信息文件，在后面调试的过程中，你将会用到这个文件。 你可以用一个工具去检验一下你的应用程序（实际上是程序集），就是PE 验证（peverify.exe），.NET Framework SDK 自带的一个工具，你可以在C:\Program Files\Microsoft .NET\Framework SDK\Bin 目录下（默认的）找到它。peverify工具并不是在源代码中去验证程序集，而是通过exe文件来验证编译器是否产生了无效的代码。在某些情况下，你需要去验证一下你的程序集，比如你使用的第三编译工具来编译你的代码，你就需要验证一下。这个工具用起来很简单，看下面的一个例子： 12 1: peverify.exe Test.exe 要想知道更多的用法，你可以用*peverify.exe* */?*来查看*peverify.exe* 其它的用法。 你可以通过ILDasm.exe来得到任何编译过的Exe或DLL文件的IL代码，ILDasm.exe是另外一个非常有用的.net自带的工具，它可以帮助你在底层分析你的代码。如果你在高级语言中编写代码，而你又想看一下编译器产生的IL代码，那么这个工具也非常有用。你可以在和peverify.exe相同的目录下找到它。你可以用这种方法得到任何.NET下exe文件的IL代码。 11: ILDasm.exe SomeProject.exe /out:SomeProject.il 还有其它很多可以用来调试分析．ＮＥＴ应用程序的工具，比如DbgClr.exe, CorDbg.exe。你可以在MSDN，.NET Framework SDK 或其它第三方网站上找到许多有关的资料。 总结在这篇文章里面，我们学习了IL汇编语言然后用IL汇编语言写了一些程序。我们从IL汇编语言的基础开始。写了一个在控制台输出一个字符串的简单程序。然后学习了一点评估堆栈的知识，用一些简单的代码（两数相加）来演示了它是怎样工作的。接着我们学习了IL数据类型，用这些数据类型声明变量，用来进行条件判断，写循环等等。我们也学会了定义了方法，在那之后，我们转向了创建命名空间和类。我们创建了我们自己的类库，用别的程序调用它。然后又在我们的类里面创建了构造函数和属性。 结论用IL汇编语言写代码并不是一件简单的事情。还有很多东西在这篇文章里面没有讨论，比如数组，异常处理等等。但是当你对IL汇编语言熟悉之后，你可以用它做很多事情。如果你想从底层来分析你的代码，或者计划为.NET开发一个编译器，那么IL汇编语言将会非常有用。如果你是一个.NET的初学者，那么我不建议你去学习它因为学习它之前，你必须对.NET平台有一个很好的理解。但是从另一方面说，它又可以帮助你去了解.NET的运行时（CLR）在幕后是怎样工作的。]]></content>
      <categories>
        <category>硬核C Sharp</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[建造者模式]]></title>
    <url>%2F2019%2F06%2F10%2F%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[[TOC] 原理模型图 示例KFC点餐(C#实现)]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[命令模式]]></title>
    <url>%2F2019%2F05%2F22%2F%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[[TOC] 原理 示例菜单程序(Js实现)需求:假设我们正在编写一个用户界面程序，该用户界面上至少有数十个Button 按钮。因为项目比较复杂，所以我们决定让某个程序员负责绘制这些按钮，而另外一些程序员则负责编写点击按钮后的具体行为，这些行为都将被封装在对象里。在大型项目开发中，这是很正常的分工。对于绘制按钮的程序员来说，他完全不知道某个按钮未来将用来做什么，可能用来刷新菜单界面，也可能用来增加一些子菜单，他只知道点击这个按钮会发生某些事情 这里运用命令模式的理由：点击了按钮之后，必须向某些负责具体行为的对象发送请求，这些对象就是请求的接收者。但是目前并不知道接收者是什么对象，也不知道接收者究竟会做什么。此时我们需要借助命令对象的帮助，以便解开按钮和负责具体行为对象之间的耦合。 设计模式的主题总是把不变的事物和变化的事物分离开来，命令模式也不例外。按下按钮之后会发生一些事情是不变的，而具体会发生什么事情是可变的。通过command 对象的帮助，将来我们可以轻易地改变这种关联，因此也可以在将来再次改变按钮的行为 传统实现下面进入代码编写阶段 首先在页面中完成这些按钮的“绘制”： 1234567891011121314151617181920 &lt;html&gt; &lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;button id="button1"&gt;点击按钮1&lt;/button&gt; &lt;button id="button2"&gt;点击按钮2&lt;/button&gt; &lt;button id="button3"&gt;点击按钮3&lt;/button&gt; &lt;script&gt; var button1 = document.getElementById('button1'), var button2 = document.getElementById('button2'), var button3 = document.getElementById('button3'); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 接下来定义setCommand 函数，setCommand 函数负责往按钮上面安装命令。可以肯定的是，点击按钮会执行某个command 命令，执行命令的动作被约定为调用command 对象的execute()方法。虽然还不知道这些命令究竟代表什么操作，但负责绘制按钮的程序员不关心这些事情，他只需要预留好安装命令的接口，command 对象自然知道如何和正确的对象沟通： 12345var setCommand = function (button, command) &#123; button.onclick = function () &#123; command.execute(); &#125; &#125;; 最后，负责编写点击按钮之后的具体行为的程序员总算交上了他们的成果，他们完成了刷新菜单界面、增加子菜单和删除子菜单这几个功能，这几个功能被分布在MenuBar 和SubMenu 这两个对象中： 12345678910111213var MenuBar = &#123; refresh: function () &#123; console.log('刷新菜单目录'); &#125;&#125;;var SubMenu = &#123; add: function () &#123; console.log('增加子菜单'); &#125;, del: function () &#123; console.log('删除子菜单'); &#125;&#125;; 在让button 变得有用起来之前，我们要先把这些行为都封装在命令类中： 12345678910111213141516171819var RefreshMenuBarCommand = function (receiver) &#123; this.receiver = receiver;&#125;;RefreshMenuBarCommand.prototype.execute = function () &#123; this.receiver.refresh();&#125;;var AddSubMenuCommand = function (receiver) &#123; this.receiver = receiver;&#125;;AddSubMenuCommand.prototype.execute = function () &#123; this.receiver.add();&#125;;var DelSubMenuCommand = function (receiver) &#123; this.receiver = receiver;&#125;;DelSubMenuCommand.prototype.execute = function () &#123; console.log('删除子菜单');&#125;; 最后就是把命令接收者传入到command 对象中，并且把command 对象安装到button 上面： 123456var refreshMenuBarCommand = new RefreshMenuBarCommand(MenuBar);var addSubMenuCommand = new AddSubMenuCommand(SubMenu);var delSubMenuCommand = new DelSubMenuCommand(SubMenu);setCommand(button1, refreshMenuBarCommand);setCommand(button2, addSubMenuCommand);setCommand(button3, delSubMenuCommand); JavaScript 中的命令模式也许我们会感到很奇怪，所谓的命令模式，看起来就是给对象的某个方法取了execute 的名字。引入command 对象和receiver 这两个无中生有的角色无非是把简单的事情复杂化了，即使不用什么模式，用下面寥寥几行代码就可以实现相同的功能： 1234567891011121314151617181920var bindClick = function (button, func) &#123; button.onclick = func;&#125;;var MenuBar = &#123; refresh: function () &#123; console.log('刷新菜单界面'); &#125;&#125;;var SubMenu = &#123; add: function () &#123; console.log('增加子菜单'); &#125;, del: function () &#123; console.log('删除子菜单'); &#125;&#125;;bindClick(button1, MenuBar.refresh);bindClick(button2, SubMenu.add);bindClick(button3, SubMenu.del); 其实,命令模式的由来，其实是回调（callback）函数的一个面向对象的替代品。JavaScript 作为将函数作为一等对象的语言，跟策略模式一样，命令模式也早已融入到了JavaScript 语言之中。运算块不一定要封装在command.execute 方法中，也可以封装在普通函数中。函数作为一等对象，本身就可以被四处传递。即使我们依然需要请求“接收者”，那也未必使用面向对象的方式，闭包可以完成同样的功能。 在面向对象设计中，命令模式的接收者被当成command 对象的属性保存起来，同时约定执行命令的操作调用command.execute 方法。在使用闭包的命令模式实现中，接收者被封闭在闭包产生的环境中，执行命令的操作可以更加简单，仅仅执行回调函数即可。无论接收者被保存为对象的属性，还是被封闭在闭包产生的环境中，在将来执行命令的时候，接收者都能被顺利访问。用闭包实现的命令模式如下代码所示： 1234567891011121314151617var setCommand = function( button, func )&#123; button.onclick = function()&#123; func(); &#125;&#125;;var MenuBar = &#123; refresh: function()&#123; console.log( '刷新菜单界面' ); &#125;&#125;;var RefreshMenuBarCommand = function( receiver )&#123; return function()&#123; receiver.refresh(); &#125;&#125;;var refreshMenuBarCommand = RefreshMenuBarCommand( MenuBar );setCommand( button1, refreshMenuBarCommand ); 如果想更明确地表达当前正在使用命令模式，或者除了执行命令之外，将来有可能还要提供撤销命令等操作。那我们最好还是把执行函数改为调用execute 方法： 1234567891011121314var RefreshMenuBarCommand = function (receiver) &#123; return &#123; execute: function () &#123; receiver.refresh(); &#125; &#125;&#125;;var setCommand = function (button, command) &#123; button.onclick = function () &#123; command.execute(); &#125;&#125;;var refreshMenuBarCommand = RefreshMenuBarCommand(MenuBar);setCommand(button1, refreshMenuBarCommand); 计算器(C#)实现设计一个计算器具有以下功能 可以进行整数的加减乘除 用户可以撤销操作 用户可以重新执行撤销的操作 代码如下: Command类 12345678/// &lt;summary&gt;/// "Command"/// &lt;/summary&gt;public abstract class Command&#123; public abstract void Execute(); public abstract void UnExecute();&#125; Calculator类(Receiver) 123456789101112131415161718/// &lt;summary&gt;/// "Receiver"/// &lt;/summary&gt;public class Calculator&#123; private int _curr = 0; public void Operation(char @operator, int operand) &#123; switch (@operator) &#123; case '+': _curr += operand; break; case '-': _curr -= operand; break; case '*': _curr *= operand; break; case '/': _curr /= operand; break; &#125; Console.WriteLine($"Current value = &#123;_curr&#125; (afterOperating: &#123;@operator&#125; &#123;operand&#125;)"); &#125;&#125; CalculatorCommand类(ConcreteCommand) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/// &lt;summary&gt;/// "ConcreteCommand"/// &lt;/summary&gt;public class CalculatorCommand : Command&#123; private readonly Calculator _calculator; private readonly char _operator; private readonly int _operand; public CalculatorCommand(Calculator calculator, char @operator, int operand) &#123; _operator = @operator; _operand = operand; _calculator = calculator; &#125; public override void Execute() &#123; _calculator.Operation(_operator, _operand); &#125; public override void UnExecute() &#123; _calculator.Operation(Undo(_operator), _operand); &#125; private char Undo(char @operator) &#123; char undo; switch (@operator) &#123; case '+': undo = '-'; break; case '-': undo = '+'; break; case '*': undo = '/'; break; case '/': undo = '*'; break; default: undo = ' '; break; &#125; return undo; &#125;&#125; User类(Invoker) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/// &lt;summary&gt;/// "Invoker"/// &lt;/summary&gt;public class User&#123; private readonly Calculator _calculator = new Calculator(); private readonly List&lt;Command&gt; _commandList = new List&lt;Command&gt;(); private int _current = 0; public void Redo(int levels) &#123; Console.WriteLine("\n---- Redo &#123;0&#125; levels ", levels); for (int i = 0; i &lt; levels; i++) &#123; if (_current &gt;= _commandList.Count - 1) &#123; break; &#125; var command = _commandList[_current++]; command.Execute(); &#125; &#125; public void Undo(int levels) &#123; Console.WriteLine("\n---- Undo &#123;0&#125; levels ", levels); // Perform undo operations for (int i = 0; i &lt; levels; i++) &#123; if (_current &lt;= 0) &#123; break; &#125; var command = _commandList[--_current]; command.UnExecute(); &#125; &#125; public void Compute(char @operator, int operand) &#123; // Create command operation and execute it Command command = new CalculatorCommand(_calculator, @operator, operand); command.Execute(); // Add command to undo list _commandList.Add(command); _current++; &#125;&#125; 客户端 123456789101112131415161718class Program &#123; static void Main(string[] args) &#123; // Create user and let her compute User user = new User(); user.Compute('+', 100); user.Compute('-', 50); user.Compute('*', 10); user.Compute('/', 2); // Undo 4 commands user.Undo(4); // Redo 3 commands user.Redo(3); // Wait for user Console.Read(); &#125; &#125; 盒马生鲜(python实现) David：听说阿里开了一家实体店——盒马鲜生，特别火爆！明天就周末了，我们一起去吃大闸蟹吧！ Tony：吃货！真是味觉的哥伦布啊，哪里的餐饮新店都少不了你的影子。不过听说盒马鲜生到处是黑科技诶，而且海生是自己挑的，还满新奇的。 David：那就说好了，明天 11：00，盒马鲜生，不吃不散！ Tony 和 David 来到杭州上城区的一家分店。这里食客众多，物品丰富，特别是生鲜，从几十块钱的小龙虾到几百块的大青蟹，再到一千多的俄罗斯帝王蟹，应有尽有。帝王蟹是吃不起了，Tony 和 David 挑了一只 900g 的一号大青蟹。 食材挑好了，接下来就是现厂加工。加工的方式有多种，清蒸、姜葱炒、香辣炒、避风塘炒等，可以任意选择，当然不同的方式价格也有所不同。因为我们选的蟹是当时活动推荐的，所以免加工费。选择一种加工方式后进行下单，下单后会给你一个呼叫器，厨师做好了会有专门的服务人员送过来，坐着等就可以了…… 盒马鲜生之所以这么火爆，一方面是因为中国从来就不缺像 David 这样的吃货，另一方面是因为里面的海生很新鲜，而且可以自己挑选。很多人都喜欢吃大闸蟹，但是你有没有注意到一个问题？从你买大闸蟹到吃上大闸蟹的整个过程，可能都没有见过厨师，而你却能享受美味的佳肴。这里有一个很重要的角色就是服务员，她帮你下订单，然后把订单传送给厨师，厨师收到订单后根据订单做餐。我们用代码来模拟一下这个过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172from abc import ABCMeta, abstractmethod# 引入ABCMeta和abstractmethod来定义抽象类和抽象方法class Chef(): "厨师" def steamFood(self, originalMaterial): print(originalMaterial + "清蒸中...") return "清蒸" + originalMaterial def stirFriedFood(self, originalMaterial): print(originalMaterial + "爆炒中...") return "香辣炒" + originalMaterialclass Order(metaclass=ABCMeta): "订单" def __init__(self, name, originalMaterial): self._chef = Chef() self._name = name self._originalMaterial = originalMaterial def getDisplayName(self): return self._name + self._originalMaterial @abstractmethod def processingOrder(self): passclass SteamedOrder(Order): "清蒸" def __init__(self, originalMaterial): super().__init__("清蒸", originalMaterial) def processingOrder(self): if (self._chef is not None): return self._chef.steamFood(self._originalMaterial) return ""class SpicyOrder(Order): "香辣炒" def __init__(self, originalMaterial): super().__init__("香辣炒", originalMaterial) def processingOrder(self): if (self._chef is not None): return self._chef.stirFriedFood(self._originalMaterial) return ""class Waiter: "服务员" def __init__(self, name): self.__name = name self.__order = None def receiveOrder(self, order): self.__order = order print("服务员" + self.__name + "：您的 " + order.getDisplayName() + " 订单已经收到,请耐心等待") def placeOrder(self): food = self.__order.processingOrder() print("服务员" + self.__name + "：您的餐 " + food + " 已经准备好，请您慢用!") 测试代码： 123456789101112131415def testOrder(): waiter = Waiter("Anna") steamedOrder = SteamedOrder("大闸蟹") print("客户David：我要一份" + steamedOrder.getDisplayName()) waiter.receiveOrder(steamedOrder) waiter.placeOrder() print() spicyOrder = SpicyOrder("大闸蟹") print("客户Tony：我要一份" + steamedOrder.getDisplayName()) waiter.receiveOrder(spicyOrder) waiter.placeOrder()testOrder() 测试结果 123456789客户David：我要一份清蒸大闸蟹服务员Anna：您的 清蒸大闸蟹 订单已经收到,请耐心等待大闸蟹清蒸中...服务员Anna：您的餐 清蒸大闸蟹 已经准备好，请您慢用!客户Tony：我要一份清蒸大闸蟹服务员Anna：您的 香辣炒大闸蟹 订单已经收到,请耐心等待大闸蟹爆炒中...服务员Anna：您的餐 香辣炒大闸蟹 已经准备好，请您慢用! 游戏(python实现)在游戏中，有两个最基本的动作，一个是行走（也叫移动），一个是攻击。这几乎是所有游戏都少不了的基础功能，不然就没法玩了！ 现在我们来模拟一下游戏角色（英雄）中的移动和攻击，为简单起见，假设移动只有上移（U）、下移（D）、左移（L）、右移（R）、上跳（J）、下蹲（S）这 6 个动作，而攻击（A）只有 1 种，括号中字符代表每一个动作在键盘中的按键，也就是对应动作的调用，这些动作的命令可以单独使用，但更多的时候会组合在一起使用。比如，弹跳就是上跳 + 下蹲两个的动作的组合，我们用 JP 表示；而弹跳攻击是弹跳 + 攻击的组合（也就是上跳 + 攻击 + 下蹲），我们用 JA 表示；而移动也可以两个方向一起移动，如上移 + 右移，我们用 RU 表示。下面的程序中，为简单起见，这里用标准输入的字符来代表按键输入事件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217import timefrom abc import ABCMeta, abstractmethodclass GameRole: # 每次移动的步距 STEP = 5 def __init__(self): self.__x = 0 self.__y = 0 self.__z = 0 def leftMove(self): self.__x -= self.STEP def rightMove(self): self.__x += self.STEP def upMove(self): self.__y += self.STEP def downMove(self): self.__y -= self.STEP def jumpMove(self): self.__z += self.STEP def squatMove(self): self.__z -= self.STEP def attack(self): print("攻击...") def showPosition(self): print("x:" + str(self.__x) + ", y:" + str(self.__y) + ", z:" + str(self.__z))class GameCommand(metaclass=ABCMeta): "游戏角色的命令类" def __init__(self, role): self._role = role def setRole(self, role): self._role = role @abstractmethod def execute(self): passclass Left(GameCommand): "左移命令" def execute(self): self._role.leftMove() self._role.showPosition()class Right(GameCommand): "右移命令" def execute(self): self._role.rightMove() self._role.showPosition()class Up(GameCommand): "上移命令" def execute(self): self._role.upMove() self._role.showPosition()class Down(GameCommand): "下移命令" def execute(self): self._role.downMove() self._role.showPosition()class Jump(GameCommand): "弹跳命令" def execute(self): self._role.jumpMove() self._role.showPosition() # 跳起后空中停留半秒 time.sleep(0.5)class Squat(GameCommand): "下蹲命令" def execute(self): self._role.squatMove() self._role.showPosition() # 下蹲后伏地半秒 time.sleep(0.5)class Attack(GameCommand): "攻击命令" def execute(self): self._role.attack()class MacroCommand(GameCommand): def __init__(self, role=None): super().__init__(role) self.__commands = [] def addCommand(self, command): # 让所有的命令作用于同一个对象 # command.setRole(self._role) self.__commands.append(command) def removeCommand(self, command): self.__commands.remove(command) def execute(self): for command in self.__commands: command.execute()class GameInvoker: def __init__(self): self.__command = None def setCommand(self, command): self.__command = command return self def action(self): if self.__command is not None: self.__command.execute()def testGame(): role = GameRole() invoker = GameInvoker() while True: strCmd = input("请输入命令：") strCmd = strCmd.upper() if (strCmd == "L"): invoker.setCommand(Left(role)).action() elif (strCmd == "R"): invoker.setCommand(Right(role)).action() elif (strCmd == "U"): invoker.setCommand(Up(role)).action() elif (strCmd == "D"): invoker.setCommand(Down(role)).action() elif (strCmd == "JP"): cmd = MacroCommand() cmd.addCommand(Jump(role)) cmd.addCommand(Squat(role)) invoker.setCommand(cmd).action() elif (strCmd == "A"): invoker.setCommand(Attack(role)).action() elif (strCmd == "LU"): cmd = MacroCommand() cmd.addCommand(Left(role)) cmd.addCommand(Up(role)) invoker.setCommand(cmd).action() elif (strCmd == "LD"): cmd = MacroCommand() cmd.addCommand(Left(role)) cmd.addCommand(Down(role)) invoker.setCommand(cmd).action() elif (strCmd == "RU"): cmd = MacroCommand() cmd.addCommand(Right(role)) cmd.addCommand(Up(role)) invoker.setCommand(cmd).action() elif (strCmd == "RD"): cmd = MacroCommand() cmd.addCommand(Right(role)) cmd.addCommand(Down(role)) invoker.setCommand(cmd).action() elif (strCmd == "LA"): cmd = MacroCommand() cmd.addCommand(Left(role)) cmd.addCommand(Attack(role)) invoker.setCommand(cmd).action() elif (strCmd == "RA"): cmd = MacroCommand() cmd.addCommand(Right(role)) cmd.addCommand(Attack(role)) invoker.setCommand(cmd).action() elif (strCmd == "UA"): cmd = MacroCommand() cmd.addCommand(Up(role)) cmd.addCommand(Attack(role)) invoker.setCommand(cmd).action() elif (strCmd == "DA"): cmd = MacroCommand() cmd.addCommand(Down(role)) cmd.addCommand(Attack(role)) invoker.setCommand(cmd).action() elif (strCmd == "JA"): cmd = MacroCommand() cmd.addCommand(Jump(role)) cmd.addCommand(Attack(role)) cmd.addCommand(Squat(role)) invoker.setCommand(cmd).action() elif (strCmd == "Q"): exit()testGame() 在上面的 Demo 中 MacroCommand 是一种组合命令，也叫宏命令（Macro Command）。宏命令是一个具体命令类，它拥有一个集合属性，在该集合中包含了对其他命令对象的引用，如上面的弹跳命令是上跳、攻击、下蹲 3 个命令的组合，引用了 3 个命令对象。 当调用宏命令的 execute() 方法时，会循环地调用每一个子命令的 execute() 方法。一个宏命令的成员可以是简单命令，还可以继续是宏命令，宏命令将递归地调用它所包含的每个成员命令的 execute() 方法。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[中介者模式]]></title>
    <url>%2F2019%2F05%2F19%2F%E4%B8%AD%E4%BB%8B%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[[TOC] 原理示例泡泡堂(js实现)2人版本游戏之初只支持两个玩家同时进行对战 先定义一个玩家构造函数，它有3 个简单的原型方法：Play.prototype.win、Play.prototype.lose以及表示玩家死亡的Play.prototype.die。 当其中一个玩家死亡的时候游戏便结束, 同时通知它的对手胜利 1234567891011121314151617181920212223function Player(name) &#123; this.name = name this.enemy = null; // 敌人&#125;;Player.prototype.win = function () &#123; console.log(this.name + ' won ');&#125;;Player.prototype.lose = function () &#123; console.log(this.name + ' lost');&#125;;Player.prototype.die = function () &#123; this.lose(); this.enemy.win();&#125;;//接下来创建2 个玩家对象：var player1 = new Player('皮蛋');var player2 = new Player('小乖');//给玩家相互设置敌人：player1.enemy = player2;player2.enemy = player1;player1.die();// 输出：皮蛋 lost、小乖 won 真正的泡泡堂游戏至多可以有8 个玩家，并分成红蓝两队进行游戏。 为游戏增加队伍现在我们改进一下游戏。因为玩家数量变多，用下面的方式来设置队友和敌人无疑很低效： 1234player1.partners= [player1,player2,player3,player4];player1.enemies = [player5,player6,player7,player8];Player5.partners= [player5,player6,player7,player8];Player5.enemies = [player1,player2,player3,player4]; 所以我们定义一个数组players 来保存所有的玩家，在创建玩家之后，循环players 来给每个玩家设置队友和敌人 再改写构造函数Player，使每个玩家对象都增加一些属性，分别是队友列表、敌人列表 、玩家当前状态、角色名字以及玩家所在的队伍颜色： 12345678var players = [];function Player( name, teamColor )&#123;this.partners = []; // 队友列表this.enemies = []; // 敌人列表this.state = 'live'; // 玩家状态this.name = name; // 角色名字this.teamColor = teamColor; // 队伍颜色&#125;; 玩家胜利和失败之后的展现依然很简单，只是在每个玩家的屏幕上简单地弹出提示： 123456Player.prototype.win = function()&#123; // 玩家团队胜利console.log( 'winner: ' + this.name );&#125;;Player.prototype.lose = function()&#123; // 玩家团队失败console.log( 'loser: ' + this.name );&#125;; 玩家死亡的方法要变得稍微复杂一点，我们需要在每个玩家死亡的时候，都遍历其他队友的生存状况，如果队友全部死亡，则这局游戏失败，同时敌人队伍的所有玩家都取得胜利，代码如下： 1234567891011121314151617181920Player.prototype.die = function () &#123; // 玩家死亡 var all_dead = true; this.state = 'dead'; // 设置玩家状态为死亡 for (var i = 0, partner; partner = this.partners[i++];) &#123; // 遍历队友列表 if (partner.state !== 'dead') &#123; // 如果还有一个队友没有死亡，则游戏还未失败 all_dead = false; break; &#125; &#125; if (all_dead === true) &#123; // 如果队友全部死亡 this.lose(); // 通知自己游戏失败 for (var i = 0, partner; partner = this.partners[i++];) &#123; // 通知所有队友玩家游戏失败 partner.lose(); &#125; for (var i = 0, enemy; enemy = this.enemies[i++];) &#123; // 通知所有敌人游戏胜利 enemy.win(); &#125; &#125;&#125;; 最后定义一个工厂来创建玩家： 1234567891011121314var playerFactory = function (name, teamColor) &#123; var newPlayer = new Player(name, teamColor); // 创建新玩家 for (var i = 0, player; player = players[i++];) &#123; // 通知所有的玩家，有新角色加入 if (player.teamColor === newPlayer.teamColor) &#123; // 如果是同一队的玩家 player.partners.push(newPlayer); // 相互添加到队友列表 newPlayer.partners.push(player); &#125; else &#123; player.enemies.push(newPlayer); // 相互添加到敌人列表 newPlayer.enemies.push(player); &#125; &#125; players.push(newPlayer); return newPlayer;&#125;; 用这段代码创建8 个玩家： 12345678910//红队：var player1 = playerFactory('皮蛋', 'red'), player2 = playerFactory('小乖', 'red'), player3 = playerFactory('宝宝', 'red'), player4 = playerFactory('小强', 'red');//蓝队：var player5 = playerFactory('黑妞', 'blue'), player6 = playerFactory('葱头', 'blue'), player7 = playerFactory('胖墩', 'blue'), player8 = playerFactory('海盗', 'blue'); 让红队玩家全部死亡： 1234player1.die();player2.die();player4.die();player3.die(); 执行结果如图 玩家增多带来的困扰现在我们已经可以随意地为游戏增加玩家或者队伍，但问题是，每个玩家和其他玩家都是紧紧耦合在一起的。在此段代码中，每个玩家对象都有两个属性，this.partners 和this.enemies，用来保存其他玩家对象的引用。当每个对象的状态发生改变，比如角色移动、吃到道具或者死亡时，都必须要显式地遍历通知其他对象。 如果在一个大型网络游戏中，画面里有成百上千个玩家，几十支队伍在互相厮杀。如果有一个玩家掉线，必须从所有其他玩家的队友列表和敌人列表中都移除这个玩家。游戏也许还有解除队伍和添加到别的队伍的功能，红色玩家可以突然变成蓝色玩家，这就不再仅仅是循环能够解决的问题了。 用中介者模式改造泡泡堂游戏现在我们开始用中介者模式来改造上面的泡泡堂游戏， 改造后的玩家对象和中介者的关系如图 所示: 首先仍然是定义Player 构造函数和player 对象的原型方法，在player 对象的这些原型方法中，不再负责具体的执行逻辑，而是把操作转交给中介者对象，我们把中介者对象命名为playerDirector： 123456789101112131415161718192021222324252627function Player(name, teamColor) &#123; this.name = name; // 角色名字 this.teamColor = teamColor; // 队伍颜色 this.state = 'alive'; // 玩家生存状态&#125;;Player.prototype.win = function () &#123; console.log(this.name + ' won ');&#125;;Player.prototype.lose = function () &#123; console.log(this.name + ' lost');&#125;;/*******************玩家死亡*****************/Player.prototype.die = function () &#123; this.state = 'dead'; playerDirector.reciveMessage('playerDead', this); // 给中介者发送消息，玩家死亡&#125;;/*******************移除玩家*****************/Player.prototype.remove = function () &#123; playerDirector.reciveMessage('removePlayer', this); // 给中介者发送消息，移除一个玩家&#125;;/*******************玩家换队*****************/Player.prototype.changeTeam = function (color) &#123; playerDirector.reciveMessage('changeTeam', this, color); // 给中介者发送消息，玩家换队&#125;; 再继续改写之前创建玩家对象的工厂函数，可以看到，因为工厂函数里不再需要给创建的玩家对象设置队友和敌人，这个工厂函数几乎失去了工厂的意义： 12345var playerFactory = function (name, teamColor) &#123; var newPlayer = new Player(name, teamColor); // 创造一个新的玩家对象 playerDirector.reciveMessage('addPlayer', newPlayer); // 给中介者发送消息，新增玩家 return newPlayer;&#125;; 最后，我们需要实现这个中介者playerDirector 对象，一般有以下两种方式。 利用发布—订阅模式。将playerDirector 实现为订阅者，各player 作为发布者，一旦player的状态发生改变，便推送消息给playerDirector，playerDirector 处理消息后将反馈发送给其他player。 在 playerDirector中开放一些接收消息的接口，各 player可以直接调用该接口来给playerDirector发送消息，player只需传递一个参数给 playerDirector，这个参数的目的是使 playerDirector可以识别发送者。同样， playerDirector接收到消息之后会将处理结果反馈给其他 player。 在这里我们使用第二种方式，playerDirector 开放一个对外暴露的接口reciveMessage，负责接收player 对象发送的消息，而player 对象发送消息的时候，总是把自身this 作为参数发送给playerDirector，以便playerDirector 识别消息来自于哪个玩家对象，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061var playerDirector = (function () &#123; var players = &#123;&#125;, // 保存所有玩家 operations = &#123;&#125;; // 中介者可以执行的操作 /****************新增一个玩家***************************/ operations.addPlayer = function (player) &#123; var teamColor = player.teamColor; // 玩家的队伍颜色 players[teamColor] = players[teamColor] || []; // 如果该颜色的玩家还没有成立队伍，则 players[teamColor].push(player); // 添加玩家进队伍 &#125;; /****************移除一个玩家***************************/ operations.removePlayer = function (player) &#123; var teamColor = player.teamColor, // 玩家的队伍颜色 teamPlayers = players[teamColor] || []; // 该队伍所有成员 for (var i = teamPlayers.length - 1; i &gt;= 0; i--) &#123; // 遍历删除 if (teamPlayers[i] === player) &#123; teamPlayers.splice(i, 1); &#125; &#125; &#125;; /****************玩家换队***************************/ operations.changeTeam = function (player, newTeamColor) &#123; // 玩家换队 operations.removePlayer(player); // 从原队伍中删除 player.teamColor = newTeamColor; // 改变队伍颜色 operations.addPlayer(player); // 增加到新队伍中 &#125;; operations.playerDead = function (player) &#123; // 玩家死亡 var teamColor = player.teamColor, teamPlayers = players[teamColor]; // 玩家所在队伍 var all_dead = true; for (var i = 0, player; player = teamPlayers[i++];) &#123; if (player.state !== 'dead') &#123; all_dead = false; break; &#125; &#125; if (all_dead === true) &#123; // 全部死亡 teamPlayers.forEach(player =&gt; &#123; // 本队所有玩家lose player.lose(); &#125;); for (var color in players) &#123; if (color !== teamColor) &#123; players[color].forEach(player =&gt; &#123; // 其他队伍所有玩家win player.win(); &#125;); &#125; &#125; &#125; &#125;; var reciveMessage = function () &#123; var message = Array.prototype.shift.call(arguments); // arguments 的第一个参数为消息名称 // arguments删除掉第一个参数剩余的传给实际调用的方法 operations[message].apply(this, arguments); &#125;; return &#123; reciveMessage &#125;&#125;)(); 可以看到，除了中介者本身，没有一个玩家知道其他任何玩家的存在，玩家与玩家之间的耦合关系已经完全解除，某个玩家的任何操作都不需要通知其他玩家，而只需要给中介者发送一个消息，中介者处理完消息之后会把处理结果反馈给其他的玩家对象。我们还可以继续给中介者扩展更多功能，以适应游戏需求的不断变化。 测试注意每次测试重置状态 测试1:红方全部死亡 1234player1.die();player2.die();player3.die();player4.die(); 测试2:皮蛋和小乖掉线 1234player1.remove();player2.remove();player3.die();player4.die(); 测试3:皮蛋从红队叛变到蓝队 1234player1.changeTeam('blue');player2.die();player3.die();player4.die(); 找房子(python实现) 人在江湖漂，岂能顺心如意？与大多数毕业生一样，第一份工作很难持续两年以上。Tony 也在一家公司工作了一年半后，换了一个东家。 在北京这个硕大的城市里，换工作基本就意味着要换房子。不得不说，找房子是一件烦心而累人的工作。 你首先要知道自己要怎样的房子：多大面积（多少平米），什么价位，是否有窗户，是否有独卫。 要去网上查找各种房源信息，找到最匹配的几个户型。 之后要去电话咨询，过滤虚假信息和过时信息。 最后，也是最累人的一步，要去实地考查，看看真实的房子与网上的信息是否相符，房间是否有异味，周围设施是否齐全。这一步你可能会从东城穿越西城，再来到南城，而后又折腾去北城……想想都累！ 最后的最后，你还要与各种脾性的房东进行周旋，去讨价还价。 Tony 想了想，还是找中介算了。在北京这座城市，你几乎找不到一手房东，90%的房源信息都掌握在房屋中介手中！既然都找不到一手房东，还不如找一家正规点的中介。 于是 Tony 找到了我爱我家，认识了里面的职员 Vangie。Vangie 问了他对房子的要求。Tony 说：“18平米左右，要有独卫，要有窗户，最好是朝南，有厨房更好！价位在2000左右。”Vangie 立马就说：“上地西里有一间，但没有厨房；当代城市家园有两间，一间主卧，一间次卧，但卫生间是共用的；美和园有一间，比较适合你，但价格会贵一点。” 真是了如指掌啊！说完就带着 Tony 开始看房了…… 一天就找到了还算合适的房子。但不得不再次吐槽：北京的房子真 TM 贵啊，18平米，精装修，有朝南窗户，一个超小（1m宽不到）的阳台，卫生间5人共用，厨房共用，价格要2600每月。押一付三，加一个月的中介费，一次交了一万多，要开始吃土了，内心滴了无数滴血…… 上面的生活场景中，Tony 通过中介来找房子，因为找房子的过程实在太繁琐了，而且对房源信息不了解。通过中介，他省去了很多麻烦的细节，合同也是直接跟中介签，你甚至可能都不知道房东是谁。 我们将通过程序来模拟一下上面找房子的过程。 源码示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116class HouseInfo: "房源信息" def __init__(self, area, price, hasWindow, bathroom, kitchen, address, owner): self.__area = area self.__price = price self.__window = hasWindow self.__bathroom = bathroom self.__kitchen = kitchen self.__address = address self.__owner = owner def getAddress(self): return self.__address def getOwnerName(self): return self.__owner.getName() def showInfo(self, isShowOwner=True): print("面积:" + str(self.__area) + "平米", "价格:" + str(self.__price) + "元", "窗户:" + ("有" if self.__window else "没有"), "卫生间:" + self.__bathroom, "厨房:" + ("有" if self.__kitchen else "没有"), "地址:" + self.getAddress(), "房东:" + self.getOwnerName() if isShowOwner else "")class HousingAgency: "房屋中介" def __init__(self, name): self.__houseInfos = [] self.__name = name def getName(self): return self.__name def addHouseInfo(self, houseInfo): self.__houseInfos.append(houseInfo) def removeHouseInfo(self, houseInfo): for info in self.__houseInfos: if (info == houseInfo): self.__houseInfos.remove(info) def getSearchCondition(self, description): "这里有一个将用户描述信息转换成搜索条件的逻辑。(为节省篇幅这里原样返回描述)" return description def getMatchInfos(self, searchCondition): "根据房源信息的各个属性查找最匹配的信息。(为节省篇幅这里略去匹配的过程，全部输出)" print(self.getName(), "为您找以下最适合的房源：") for info in self.__houseInfos: info.showInfo(False) return self.__houseInfos def signContract(self, houseInfo, time): "与房东签订协议" print(self.getName(), "与房东", houseInfo.getOwnerName(), "签订", houseInfo.getAddress(), "的房子的的租赁合同，租期", time, "年。 合同期内", self.getName(), "有权对其进行使用和转租！") def signContracts(self, time): for info in self.__houseInfos: self.signContract(info, time)class HouseOwner: "房东" def __init__(self, name, address): self.__name = name self.__address = address self.__houseInfo = None def getName(self): return self.__name def getAddress(self): return self.__address def setHouseInfo(self, area, price, hasWindow, bathroom, kitchen): self.__houseInfo = HouseInfo(area, price, hasWindow, bathroom, kitchen, self.getAddress(), self) def publishHouseInfo(self, agency): agency.addHouseInfo(self.__houseInfo) print(self.getName() + "在", agency.getName(), "发布房源出租信息：") self.__houseInfo.showInfo()class Custom: "房客，租房人" def __init__(self, name): self.__name = name def getName(self): return self.__name def findHouse(self, description, agency): print("我是" + self.getName() + ", 我想要找一个\"" + description + "\"的房子") print() return agency.getMatchInfos(agency.getSearchCondition(description)) def seeHouse(self, houseInfos): "去看房，选择最使用的房子。(这里省略看房的过程)" size = len(houseInfos) return houseInfos[size - 1] def signContract(self, houseInfo, agency, time): "与中介签订协议" print(self.getName(), "与中介", agency.getName(), "签订", houseInfo.getAddress(), "的房子的租赁合同, 租期", time, "年。合同期内", self.__name, "有权对其进行使用！") 测试代码: 123456789101112131415161718192021222324252627def testRenting(): myHome = HousingAgency("我爱我家") zhangsan = HouseOwner("张三", "上地西里") zhangsan.setHouseInfo(20, 2500, 1, "独立卫生间", 0) zhangsan.publishHouseInfo(myHome) lisi = HouseOwner("李四", "当代城市家园") lisi.setHouseInfo(16, 1800, 1, "公用卫生间", 0) lisi.publishHouseInfo(myHome) wangwu = HouseOwner("王五", "金隅美和园") wangwu.setHouseInfo(18, 2600, 1, "独立卫生间", 1) wangwu.publishHouseInfo(myHome) print() myHome.signContracts(3) print() tony = Custom("Tony") houseInfos = tony.findHouse("18平米左右，要有独卫，要有窗户，最好是朝南，有厨房更好！价位在2000左右", myHome) print() print("正在看房，寻找最合适的住巢……") print() AppropriateHouse = tony.seeHouse(houseInfos) tony.signContract(AppropriateHouse, myHome, 1)testRenting() 输出结果: 123456789101112131415161718192021张三在 我爱我家 发布房源出租信息：面积:20平米 价格:2500元 窗户:有 卫生间:独立卫生间 厨房:没有 地址:上地西里 房东:张三李四在 我爱我家 发布房源出租信息：面积:16平米 价格:1800元 窗户:有 卫生间:公用卫生间 厨房:没有 地址:当代城市家园 房东:李四王五在 我爱我家 发布房源出租信息：面积:18平米 价格:2600元 窗户:有 卫生间:独立卫生间 厨房:有 地址:金隅美和园 房东:王五我爱我家 与房东 张三 签订 上地西里 的房子的的租赁合同，租期 3 年。 合同期内 我爱我家 有权对其进行使用和转租！我爱我家 与房东 李四 签订 当代城市家园 的房子的的租赁合同，租期 3 年。 合同期内 我爱我家 有权对其进行使用和转租！我爱我家 与房东 王五 签订 金隅美和园 的房子的的租赁合同，租期 3 年。 合同期内 我爱我家 有权对其进行使用和转租！我是Tony, 我想要找一个&quot;18平米左右，要有独卫，要有窗户，最好是朝南，有厨房更好！价位在2000左右&quot;的房子我爱我家 为您找以下最适合的房源：面积:20平米 价格:2500元 窗户:有 卫生间:独立卫生间 厨房:没有 地址:上地西里 面积:16平米 价格:1800元 窗户:有 卫生间:公用卫生间 厨房:没有 地址:当代城市家园 面积:18平米 价格:2600元 窗户:有 卫生间:独立卫生间 厨房:有 地址:金隅美和园 正在看房，寻找最合适的住巢……Tony 与中介 我爱我家 签订 金隅美和园 的房子的租赁合同, 租期 1 年。合同期内 Tony 有权对其进行使用！ 从网状到星状 类图变化]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[组合模式]]></title>
    <url>%2F2019%2F05%2F17%2F%E7%BB%84%E5%90%88%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[[TOC] 原理 示例绘图(C#实现)旧版实现用绘图这个例子来说明Composite模式，通过一些基本图像元素（直线、圆等）以及一些复合图像元素（由基本图像元素组合而成）构建复杂的图形树。在设计中我们对每一个对象都配备一个Draw()方法，在调用时，会显示相关的图形。可以看到，这里复合图像元素它在充当对象的同时，又是那些基本图像元素的一个容器。先看一下基本的类结构图： 图中橙色的区域表示的是复合图像元素 示意性代码： 12345678910111213141516171819202122232425262728public abstract class Graphics&#123; protected string Name; protected Graphics(string name) &#123; this.Name = name; &#125; public abstract void Draw();&#125;public class Picture : Graphics&#123; public Picture(string name) : base(name) &#123; &#125; public override void Draw() &#123; // &#125; public ArrayList GetChilds() &#123; //返回所有的子对象 return null; &#125;&#125; 而其他作为树枝构件，实现代码如下： 1234567891011121314151617181920212223242526272829303132public class Line : Graphics&#123; public Line(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); &#125;&#125;public class Circle : Graphics&#123; public Circle(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); &#125;&#125;public class Rectangle : Graphics&#123; public Rectangle(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); &#125;&#125; 引入组合模式现在我们要对该图像元素进行处理：在客户端程序中，需要判断返回对象的具体类型到底是基本图像元素，还是复合图像元素。如果是复合图像元素，我们将要用递归去处理，然而这种处理的结果却增加了客户端程序与复杂图像元素内部结构之间的依赖。那么我们如何去解耦这种关系呢？ 我们希望的是客户程序可以像处理基本图像元素一样来处理复合图像元素，这就要引入Composite模式了，需要把对于子对象的管理工作交给复合图像元素，为了进行子对象的管理，它必须提供必要的Add()，Remove()等方法，类结构图如下： 代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public abstract class Graphics&#123; protected string Name; protected Graphics(string name) &#123; this.Name = name; &#125; public abstract void Draw(); public abstract void Add(Graphics g); public abstract void Remove(Graphics g);&#125;public class Picture : Graphics&#123; protected ArrayList PicList = new ArrayList(); public Picture(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); foreach (Graphics g in PicList) &#123; g.Draw(); &#125; &#125; public override void Add(Graphics g) &#123; PicList.Add(g); &#125; public override void Remove(Graphics g) &#123; PicList.Remove(g); &#125;&#125;public class Line : Graphics&#123; public Line(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); &#125; public override void Add(Graphics g) &#123; &#125; public override void Remove(Graphics g) &#123; &#125;&#125;public class Circle : Graphics&#123; public Circle(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); &#125; public override void Add(Graphics g) &#123; &#125; public override void Remove(Graphics g) &#123; &#125;&#125;public class Rectangle : Graphics&#123; public Rectangle(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); &#125; public override void Add(Graphics g) &#123; &#125; public override void Remove(Graphics g) &#123; &#125;&#125; 这样引入Composite模式后，客户端程序不再依赖于复合图像元素的内部实现了。 然而，我们程序中仍然存在着问题，因为Line，Rectangle，Circle已经没有了子对象，它是一个基本图像元素，因此Add()，Remove()的方法对于它来说没有任何意义，而且把这种错误不会在编译的时候报错，把错误放在了运行期。 我们希望能够捕获到这类错误，并加以处理，稍微改进一下我们的程序： 1234567891011121314151617181920public class Line : Graphics &#123; public Line(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); &#125; public override void Add(Graphics g) &#123; //抛出一个我们自定义的异常 &#125; public override void Remove(Graphics g) &#123; //抛出一个我们自定义的异常 &#125; &#125; 这样改进以后，我们可以捕获可能出现的错误，做进一步的处理。 上面的这种实现方法属于透明式的Composite模式。 安全的组合模式如果我们想要更安全的一种做法，就需要把管理子对象的方法声明在树枝构件Picture类里面，这样如果叶子节点Line，Rectangle，Circle使用这些方法时，在编译期就会出错，看一下类结构图： 代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public abstract class Graphics &#123; protected string Name; protected Graphics(string name) &#123; Name = name; &#125; public abstract void Draw(); &#125; public class Picture : Graphics &#123; protected ArrayList PicList = new ArrayList(); public Picture(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); foreach (Graphics g in PicList) &#123; g.Draw(); &#125; &#125; public void Add(Graphics g) &#123; PicList.Add(g); &#125; public void Remove(Graphics g) &#123; PicList.Remove(g); &#125; &#125; public class Line : Graphics &#123; public Line(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); &#125; &#125; public class Circle : Graphics &#123; public Circle(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); &#125; &#125; public class Rectangle : Graphics &#123; public Rectangle(string name) : base(name) &#123; &#125; public override void Draw() &#123; Console.WriteLine($"Draw a&#123;Name&#125;"); &#125; &#125; 这种方式属于安全式的Composite模式 在这种方式下，虽然避免了前面所讨论的错误，但是它也使得叶子节点和树枝构件具有不一样的接口。 这种方式和透明式的Composite各有优劣，具体使用哪一个，需要根据问题的实际情况而定。 通过Composite模式，客户程序在调用Draw()的时候不用再去判断复杂图像元素中的子对象到底是基本图像元素，还是复杂图像元素，看一下简单的客户端调用： 1234567891011121314class Program &#123; static void Main(string[] args) &#123; Picture root = new Picture("Root"); root.Add(new Line("Line")); root.Add(new Circle("Circle")); root.Add(new Rectangle("Rectangle")); root.Draw(); Console.ReadLine(); &#125; &#125; 组装电脑(python实现)只要你对硬件稍微有一些了解，或者打开过机箱换过组件，一定知道 CPU、内存、显卡是插在主板上的，而硬盘也是连在主板上的，在机箱的后面有一排的插口，可以连接鼠标、键盘、耳麦、摄像头等外接配件，而显示器需要单独插电源才能工作。我们可以用代码来模拟台式电脑的组成，这里假设每一个组件都有开始工作和结束工作两个功能，还可以显示自己的信息和组成结构。 代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162class Component: "组件，所有子配件的基类" def __init__(self, name): self._name = name def showInfo(self, indent=""): pass def isComposite(self): return False def startup(self, indent=""): print(indent + self._name + " 准备开始工作...") def shutdown(self, indent=""): print(indent + self._name + " 即将结束工作...")class CPU(Component): "中央处理器" def __init__(self, name): super().__init__(name) def showInfo(self, indent): print(indent, end="") print("CPU:" + self._name + ",可以进行高速计算。")class MemoryCard(Component): "内存条" def __init__(self, name): super().__init__(name) def showInfo(self, indent): print(indent, end="") print("内存:" + self._name + ",可以缓存数据，读写速度快。")class HardDisk(Component): "硬盘" def __init__(self, name): super().__init__(name) def showInfo(self, indent): print(indent, end="") print("硬盘:" + self._name + ",可以永久存储数据，容量大。")class GraphicsCard(Component): "显卡" def __init__(self, name): super().__init__(name) def showInfo(self, indent): print(indent, end="") print("显卡:" + self._name + ",可以高速计算和处理图形图像。")class Battery(Component): "电源" def __init__(self, name): super().__init__(name) def showInfo(self, indent): print(indent, end="") print("电源:" + self._name + ",可以持续给主板和外接配件供电。")class Fan(Component): "风扇" def __init__(self, name): super().__init__(name) def showInfo(self, indent): print(indent, end="") print("风扇:" + self._name + "，辅助CPU散热。")class Displayer(Component): "显示器" def __init__(self, name): super().__init__(name) def showInfo(self, indent): print(indent, end="") print("显示器:" + self._name + "，负责内容的显示。")class Composite(Component): "配件组合器" def __init__(self, name): super().__init__(name) self._components = [] def showInfo(self, indent): print(self._name + ",由以下部件组成:") indent += "\t" for element in self._components: element.showInfo(indent) def isComposite(self): return True def addComponent(self, component): self._components.append(component) def removeComponent(self, component): self._components.remove(component) def startup(self, indent): super().startup(indent) indent += "\t" for element in self._components: element.startup(indent) def shutdown(self, indent): super().startup(indent) indent += "\t" for element in self._components: element.shutdown(indent)class Mainboard(Composite): "主板" def __init__(self, name): super().__init__(name) def showInfo(self, indent): print(indent + "主板:", end="") super().showInfo(indent)class ComputerCase(Composite): "机箱" def __init__(self, name): super().__init__(name) def showInfo(self, indent): print(indent + "机箱:", end="") super().showInfo(indent)class Computer(Composite): "电脑" def __init__(self, name): super().__init__(name) def showInfo(self, indent): print(indent + "电脑:", end="") super().showInfo(indent) 测试代码:12345678910111213141516171819202122232425262728293031def testComputer(): cpu = CPU("Intel Core i5-6600K") memoryCard = MemoryCard("Kingston Fury DDR4") hardDisk = HardDisk("Kingston V300 ") graphicsCard = GraphicsCard("Colorful iGame750") mainBoard = Mainboard("GIGABYTE Z170M M-ATX") mainBoard.addComponent(cpu) mainBoard.addComponent(memoryCard) mainBoard.addComponent(hardDisk) mainBoard.addComponent(graphicsCard) battery = Battery("Antec VP 450P") fan = Fan("DEEPCOOL 120T") computerCase = ComputerCase("SAMA MATX") computerCase.addComponent(battery) computerCase.addComponent(mainBoard) computerCase.addComponent(fan) displayer = Displayer("AOC LV243XIP") computer = Computer("Tony DIY电脑") computer.addComponent(displayer) computer.addComponent(computerCase) computer.showInfo("") print("\n开机过程:") computer.startup("") print("\n关机过程:") computer.shutdown("") testComputer() 输出结果： 12345678910111213141516171819202122232425262728293031323334电脑:Tony DIY电脑,由以下部件组成: 显示器:AOC LV243XIP，负责内容的显示。 机箱:SAMA MATX,由以下部件组成: 电源:Antec VP 450P,可以持续给主板和外接配件供电。 主板:GIGABYTE Z170M M-ATX,由以下部件组成: CPU:Intel Core i5-6600K,可以进行高速计算。 内存:Kingston Fury DDR4,可以缓存数据，读写速度快。 硬盘:Kingston V300 ,可以永久存储数据，容量大。 显卡:Colorful iGame750,可以高速计算和处理图形图像。 风扇:DEEPCOOL 120T，辅助CPU散热。开机过程:Tony DIY电脑 准备开始工作... AOC LV243XIP 准备开始工作... SAMA MATX 准备开始工作... Antec VP 450P 准备开始工作... GIGABYTE Z170M M-ATX 准备开始工作... Intel Core i5-6600K 准备开始工作... Kingston Fury DDR4 准备开始工作... Kingston V300 准备开始工作... Colorful iGame750 准备开始工作... DEEPCOOL 120T 准备开始工作...关机过程:Tony DIY电脑 准备开始工作... AOC LV243XIP 即将结束工作... SAMA MATX 准备开始工作... Antec VP 450P 即将结束工作... GIGABYTE Z170M M-ATX 准备开始工作... Intel Core i5-6600K 即将结束工作... Kingston Fury DDR4 即将结束工作... Kingston V300 即将结束工作... Colorful iGame750 即将结束工作... DEEPCOOL 120T 即将结束工作... 类图如下:]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[抽象工厂模式]]></title>
    <url>%2F2019%2F05%2F14%2F%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[[TOC] 原理 示例计算员工的工资(C#实现)需求中国企业需要一项简单的财务计算：每月月底，财务人员要计算员工的工资。 员工的工资 = (基本工资 + 奖金 - 个人所得税)。这是一个放之四海皆准的运算法则。 为了简化系统，我们假设员工基本工资总是4000美金。 中国企业奖金和个人所得税的计算规则是: 奖金 = 基本工资(4000) * 10% 个人所得税 = (基本工资 + 奖金) * 40% 我们现在要为此构建一个软件系统（代号叫Softo），满足中国企业的需求。 需求分析奖金(Bonus)、个人所得税(Tax)的计算是Softo系统的业务规则(Service)。 工资的计算(Calculator)则调用业务规则(Service)来计算员工的实际工资。 工资的计算作为业务规则的前端(或者客户端Client)将提供给最终使用该系统的用户(财务人员)使用。 针对中国企业为系统建模根据上面的分析，为Softo系统建模如下： 实现代码如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Program &#123; static void Main(string[] args) &#123; ChineseBonus bonus = new ChineseBonus(); double bonusValue = bonus.Calculate(); ChineseTax tax = new ChineseTax(); double taxValue = tax.Calculate(); double salary = Constant.BaseSalary + bonusValue - taxValue; Console.WriteLine("Chinese Salary is：" + salary); Console.ReadLine(); &#125; &#125; /// &lt;summary&gt; /// 公用的常量 /// &lt;/summary&gt; public class Constant &#123; public static double BaseSalary = 4000; &#125; /// &lt;summary&gt; /// 计算中国个人所得税 /// &lt;/summary&gt; public class ChineseTax &#123; public double Calculate() &#123; return (Constant.BaseSalary + (Constant.BaseSalary * 0.1)) * 0.4; &#125; &#125; /// &lt;summary&gt; /// 计算中国个人奖金 /// &lt;/summary&gt; public class ChineseBonus &#123; public double Calculate() &#123; return Constant.BaseSalary * 0.1; &#125; &#125; 运行程序，输入的结果如下： Chinese Salary is：2640 针对美国企业为系统建模为了拓展国际市场，我们要把该系统移植给美国公司使用。 美国企业的工资计算同样是: 员工的工资 = 基本工资 + 奖金 - 个人所得税。 但是他们的奖金和个人所得税的计算规则不同于中国企业: 美国企业奖金和个人所得税的计算规则是: 奖金 = 基本工资 * 15 % 个人所得税 = (基本工资 5% + 奖金 25%) 根据前面为中国企业建模经验，我们仅仅将ChineseTax、ChineseBonus修改为AmericanTax、AmericanBonus。 修改后的模型如下： 实现代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Program &#123; static void Main(string[] args) &#123; AmericanBonus bonus = new AmericanBonus(); double bonusValue = bonus.Calculate(); AmericanTax tax = new AmericanTax(); double taxValue = tax.Calculate(); double salary = Constant.BaseSalary + bonusValue - taxValue; Console.WriteLine("American Salary is：" + salary); Console.ReadLine(); &#125; &#125; /// &lt;summary&gt; /// 公用的常量 /// &lt;/summary&gt; public class Constant &#123; public static double BaseSalary = 4000; &#125; /// &lt;summary&gt; /// 计算美国个人所得税 /// &lt;/summary&gt; public class AmericanTax &#123; public double Calculate() &#123; return Constant.BaseSalary*0.05 + (Constant.BaseSalary * 0.15*0.25); &#125; &#125; /// &lt;summary&gt; /// 计算美国个人奖金 /// &lt;/summary&gt; public class AmericanBonus &#123; public double Calculate() &#123; return Constant.BaseSalary * 0.15; &#125; &#125; 运行程序，输入的结果如下： American Salary is：4250 问题改进我们针对中国和美国分别做了实现,即使整合在一起,卖给中国和美国的企业时,调用处的代码仍然需要改变,违反了开闭原则. 更为致命的问题是：我们需要将这个移植工作转包给一个叫Hippo的软件公司。 由于版权问题，我们并未提供Softo系统的源码给Hippo公司，导致实际上移植工作无法进行。 为此，我们考虑增加一个工具类(命名为Factory)，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class Program&#123; static void Main(string[] args) &#123; Bonus bonus = new Factory().CreateBonus(); double bonusValue = bonus.Calculate(); Tax tax = new Factory().CreateTax(); double taxValue = tax.Calculate(); double salary = Constant.BaseSalary + bonusValue - taxValue; Console.WriteLine("Chinese Salary is：" + salary); Console.ReadLine(); &#125;&#125;/// &lt;summary&gt;/// Factory类/// &lt;/summary&gt;public class Factory&#123; public Tax CreateTax() &#123; return new ChineseTax(); &#125; public Bonus CreateBonus() &#123; return new ChineseBonus(); &#125;&#125;/// &lt;summary&gt;/// 个人所得税抽象类/// &lt;/summary&gt;public abstract class Tax&#123; public abstract double Calculate();&#125;/// &lt;summary&gt;/// 奖金抽象类/// &lt;/summary&gt;public abstract class Bonus&#123; public abstract double Calculate();&#125;/// &lt;summary&gt;/// 计算中国个人所得税/// &lt;/summary&gt;public class ChineseTax : Tax&#123; public override double Calculate() &#123; return (Constant.BaseSalary + (Constant.BaseSalary * 0.1)) * 0.4; &#125;&#125;/// &lt;summary&gt;/// 计算中国个人奖金/// &lt;/summary&gt;public class ChineseBonus : Bonus&#123; public override double Calculate() &#123; return Constant.BaseSalary * 0.1; &#125;&#125;/// &lt;summary&gt;/// 公用的常量/// &lt;/summary&gt;public class Constant&#123; public static double BaseSalary = 4000;&#125; 为系统增加抽象工厂方法如果我们使用上面的工厂方法,移植时还是要增加AmericanTax和AmericanBonus类,Factory类也要作相应修改,工作量并没有任何缩减 从Factory类在系统移植时修改的内容我们可以看出: 实际上它是专属于美国企业或者中国企业的。名称上应该叫AmericanFactory,ChineseFactory更合适. 解决方案是增加一个抽象工厂类AbstractFactory，增加一个静态方法，该方法根据一个配置文件(App.config或者Web.config) 一个项(比如factoryName)动态地判断应该实例化哪个工厂类，这样，我们就把移植工作转移到了对配置文件的修改。修改后的模型如下： AbstractFactory类代码如下: 12345678910111213141516171819202122/// &lt;summary&gt;/// 抽象Factory类/// &lt;/summary&gt;public abstract class AbstractFactory&#123; public static AbstractFactory GetInstance(string factoryName) &#123; AbstractFactory instance = null; if (factoryName != "") &#123; instance = (AbstractFactory)Assembly.GetExecutingAssembly().CreateInstance(factoryName); &#125; return instance; &#125; public abstract Tax CreateTax(); public abstract Bonus CreateBonus();&#125; 全部代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145public class Program&#123; static void Main(string[] args) &#123; var configuration = new ConfigurationBuilder() .AddJsonFile("app.json", true, true).Build(); var factoryName = configuration["factoryName"]; var factory = AbstractFactory.GetInstance(factoryName); var bonus = factory.CreateBonus(); var bonusValue = bonus.Calculate(); var tax = factory.CreateTax(); var taxValue = tax.Calculate(); double salary = Constant.BaseSalary + bonusValue - taxValue; Console.WriteLine($"Salary is：&#123;salary&#125;"); &#125;&#125;/// &lt;summary&gt;/// 抽象Factory类/// &lt;/summary&gt;public abstract class AbstractFactory&#123; public static AbstractFactory GetInstance(string factoryName) &#123; AbstractFactory instance = null; if (factoryName != "") &#123; instance = (AbstractFactory)Assembly.GetExecutingAssembly().CreateInstance(factoryName); &#125; return instance; &#125; public abstract Tax CreateTax(); public abstract Bonus CreateBonus();&#125;/// &lt;summary&gt;/// AmericanFactory类/// &lt;/summary&gt;public class AmericanFactory : AbstractFactory&#123; public override Tax CreateTax() &#123; return new AmericanTax(); &#125; public override Bonus CreateBonus() &#123; return new AmericanBonus(); &#125;&#125;/// &lt;summary&gt;/// ChineseFactory类/// &lt;/summary&gt;public class ChineseFactory : AbstractFactory&#123; public override Tax CreateTax() &#123; return new ChineseTax(); &#125; public override Bonus CreateBonus() &#123; return new ChineseBonus(); &#125;&#125;/// &lt;summary&gt;/// 奖金抽象类/// &lt;/summary&gt;public abstract class Bonus&#123; public abstract double Calculate();&#125;/// &lt;summary&gt;/// 个人所得税抽象类/// &lt;/summary&gt;public abstract class Tax&#123; public abstract double Calculate();&#125;/// &lt;summary&gt;/// 计算中国个人奖金/// &lt;/summary&gt;public class ChineseBonus : Bonus&#123; public override double Calculate() &#123; return Constant.BaseSalary * 0.1; &#125;&#125;/// &lt;summary&gt;/// 计算中国个人所得税/// &lt;/summary&gt;public class ChineseTax : Tax&#123; public override double Calculate() &#123; return (Constant.BaseSalary + (Constant.BaseSalary * 0.1)) * 0.4; &#125;&#125;/// &lt;summary&gt;/// 计算美国奖金/// &lt;/summary&gt;public class AmericanBonus : Bonus&#123; public override double Calculate() &#123; return Constant.BaseSalary * 0.15; &#125;&#125;/// &lt;summary&gt;/// 计算美国个人所得税/// &lt;/summary&gt;public class AmericanTax : Tax&#123; public override double Calculate() &#123; return Constant.BaseSalary * 0.05 + (Constant.BaseSalary * 0.15 * 0.25); &#125;&#125;public class Constant&#123; public static double BaseSalary = 4000;&#125; app.json内容如下: 123&#123; "factoryName": "AbstractFactory.AmericanFactory"&#125; 修改配置文件的工作很简单，只要写一篇幅配置文档说明书提供给移植该系统的团队(比如Hippo公司) 就可以方便地切换使该系统运行在美国或中国企业。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[访问者模式]]></title>
    <url>%2F2019%2F05%2F13%2F%E8%AE%BF%E9%97%AE%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[[TOC] 原理示例学生(C#实现)一个班级里有学霸和学渣两种类型的学生。当出成绩时，学霸开心，学渣不开心。当放假时，学霸不开心，学渣开心。未来不会有第三种类型的学生，但未来可能会有新的事件发生，比如布置作业、春游等。 第一版伪代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Program &#123; static void Main(string[] args) &#123; var students = new List&lt;Student&gt; &#123; new BrainyStudent(), new InferiorStudent() //... &#125;; // 出成绩了 foreach (var student in students) &#123; if (student is BrainyStudent) &#123; Console.WriteLine("出成绩了,开心"); &#125; else &#123; Console.WriteLine("出成绩了,不开心"); &#125; &#125; // 放假了 foreach (var student in students) &#123; if (student is BrainyStudents) &#123; Console.WriteLine("放假了不能学习,不开心"); &#125; else &#123; Console.WriteLine("放假了不用学习,开心"); &#125; &#125; &#125; &#125; /// &lt;summary&gt; /// 抽象学生类 /// &lt;/summary&gt; public abstract class Student &#123; &#125; /// &lt;summary&gt; /// 学霸 /// &lt;/summary&gt; public class BrainyStudents : Student &#123; &#125; /// &lt;summary&gt; /// 学渣 /// &lt;/summary&gt; public class InferiorStudent : Student &#123; &#125; 但这样写明显不符合面向对象的开放封闭原则，怎么改呢？ 初步重构有一个容易想到的方法，就是把出成绩和放假作为一个抽象方法，写到抽象学生类里面，学霸和学渣分别实现这两个方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384 /// &lt;summary&gt; /// 抽象学生类 /// &lt;/summary&gt; public abstract class Student &#123; /// &lt;summary&gt; /// 放假方法 /// &lt;/summary&gt; public abstract void Vacation(); /// &lt;summary&gt; /// 出成绩方法 /// &lt;/summary&gt; public abstract void ReleaseScore(); &#125; class Program &#123; static void Main(string[] args) &#123; var students = new List&lt;Student&gt; &#123; new BrainyStudent(), new InferiorStudent() //... &#125;; foreach (var student in students) &#123; student.ReleaseScore(); student.Vacation(); &#125; &#125; &#125; /// &lt;summary&gt; /// 抽象学生类 /// &lt;/summary&gt; public abstract class Student &#123; /// &lt;summary&gt; /// 放假方法 /// &lt;/summary&gt; public abstract void Vacation(); /// &lt;summary&gt; /// 出成绩方法 /// &lt;/summary&gt; public abstract void ReleaseScore(); &#125; /// &lt;summary&gt; /// 学霸 /// &lt;/summary&gt; public class BrainyStudent : Student &#123; public override void Vacation() &#123; Console.WriteLine("放假了不能学习，不开心"); &#125; public override void ReleaseScore() &#123; Console.WriteLine("出成绩了，开心"); &#125; &#125; /// &lt;summary&gt; /// 学渣 /// &lt;/summary&gt; public class InferiorStudent : Student &#123; public override void Vacation() &#123; Console.WriteLine("放假了不用学习,开心"); &#125; public override void ReleaseScore() &#123; Console.WriteLine("出成绩了,不开心"); &#125; &#125;&#125; 这样看上去很符合开放封闭原则，因为如果有新的类型的学生，只需要添加一个新的学生类，让它去实现两个抽象方法即可，不用改其它类的代码。但仔细看需求，题目说不会有新的类型学生，而是会有新类型的事件！也就是说，如果现在要春游，就需要在抽象学生类里加一个springOuting()方法，然后分别在学霸和学渣里实现。一共需要改三个类！一点也不符合开放封闭原则。所以一定要搞清楚固定的东西是什么，可能会变的东西是什么。 使用访问者模式重构因为会变的是遍历列表时对学生的访问方法（出成绩、放假、春游），而不是学生类型（学霸、学渣），所以这里就不应该像上面那样将学生的行为抽象，而是应该将访问的方式抽象： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/// &lt;summary&gt; /// 抽象学生类 /// &lt;/summary&gt; public abstract class Student &#123; /// &lt;summary&gt; /// 处理Visitor这个抽象事件。 ///（可能是出成绩、放假等。这里抽象化了，以便随时改) /// &lt;/summary&gt; /// &lt;param name="v"&gt;&lt;/param&gt; public abstract void Accept(Visitor v); &#125; public abstract class Visitor &#123; // 抽象的访问方式类 public abstract void VisitBrainyStudent(BrainyStudent brainyStudent); // 访问学霸 public abstract void VisitInferiorStudent(InferiorStudent inferiorStudent); // 访问学渣 &#125; /// &lt;summary&gt; /// 学霸 /// &lt;/summary&gt; public class BrainyStudent : Student &#123; /// &lt;summary&gt; /// 学霸处理事件 /// &lt;/summary&gt; /// &lt;param name="v"&gt;&lt;/param&gt; public override void Accept(Visitor v) &#123; // 调用对学霸的处理方法 v.VisitBrainyStudent(this); &#125; &#125; /// &lt;summary&gt; /// 学渣 /// &lt;/summary&gt; public class InferiorStudent : Student &#123; /// &lt;summary&gt; /// 学渣处理事件 /// &lt;/summary&gt; /// &lt;param name="v"&gt;&lt;/param&gt; public override void Accept(Visitor v) &#123; // 调用对学渣的处理方法 v.VisitInferiorStudent(this); &#125; &#125; 这样写好以后，要添加访问方法（出成绩、放假、春游），只需要继承Visitor类，并实现学霸和学渣分别不同的反应即可。比如： 12345678910111213141516171819202122232425 public class ReleaseScoreVisitor : Visitor &#123; public override void VisitBrainyStudent(BrainyStudent brainyStudent) &#123; Console.WriteLine("出成绩了，开心"); &#125; public override void VisitInferiorStudent(InferiorStudent inferiorStudent) &#123; Console.WriteLine("出成绩了，不开心"); &#125; &#125; public class VacationVisitor : Visitor &#123; public override void VisitBrainyStudent(BrainyStudent brainyStudent) &#123; Console.WriteLine("放假了，不开心"); &#125; public override void VisitInferiorStudent(InferiorStudent inferiorStudent) &#123; Console.WriteLine("放假了，开心"); &#125; &#125; 调用: 1234567891011121314151617181920212223class Program&#123; static void Main(string[] args) &#123; List&lt;Student&gt; students = new List&lt;Student&gt; &#123; new BrainyStudent(), new InferiorStudent() //... &#125;; Visitor v = new ReleaseScoreVisitor(); Visitor v2 = new VacationVisitor(); foreach (var student in students) &#123; student.Accept(v); student.Accept(v2); &#125; Console.ReadKey(); &#125;&#125; 现在要添加访问方法（放假、春游等）,就只需要添加访问方式类，而不需要修改写好的类，这样才真正符合开放封闭原则。在遍历集合时，把访问单个对象的方式抽象出来，这就是访问者模式。所以说，一定要搞清楚固定的东西是什么，可能会变的东西是什么，才好理解访问者模式的思想。 雇员信息(C#实现)]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[策略模式]]></title>
    <url>%2F2019%2F05%2F09%2F%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[[TOC] 原理示例电影票打折方案(C#实现)需求 Sunny软件公司为某电影院开发了一套影院售票系统，在该系统中需要为不同类型的用户提供不同的电影票打折方式，具体打折方案如下： 学生凭学生证可享受票价8折优惠 年龄在10周岁及以下的儿童可享受每张票减免10元的优惠（原始票价需大于等于20元） 影院VIP用户除享受票价半价优惠外还可进行积分，积分累计到一定额度可换取电影院赠送的奖品 该系统在将来可能还要根据需要引入新的打折方式 旧的实现方式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class Program &#123; static void Main(string[] args) &#123; OldMovieTicket oldMovieTicket = new OldMovieTicket &#123; Price = 60.0 //原始票价 &#125;; Console.WriteLine($"原始价为：&#123;oldMovieTicket.Price&#125;"); Console.WriteLine("---------------------------------"); Console.WriteLine($"折后价为：&#123;oldMovieTicket.GetPrice("student")&#125;");//学生票 Console.WriteLine("---------------------------------"); Console.WriteLine($"折后价为：&#123;oldMovieTicket.GetPrice("children")&#125;");//儿童票 Console.WriteLine("---------------------------------"); Console.WriteLine($"折后价为：&#123;oldMovieTicket.GetPrice("vip")&#125;");//vip票 Console.WriteLine("---------------------------------"); &#125; &#125; //电影票类 class OldMovieTicket &#123; /// &lt;summary&gt; /// 电影票价格 /// &lt;/summary&gt; public double Price &#123; get; set; &#125; //计算打折之后的票价 public double GetPrice(string type) &#123; //学生票折后票价计算 if (type.Equals("student", StringComparison.OrdinalIgnoreCase)) &#123; Console.WriteLine("学生票："); return Price * 0.8; &#125; //儿童票折后票价计算 if (type.Equals("children", StringComparison.OrdinalIgnoreCase) &amp;&amp; Price &gt;= 20) &#123; Console.WriteLine("儿童票："); return Price - 10; &#125; //VIP票折后票价计算 if (type.Equals("vip", StringComparison.OrdinalIgnoreCase)) &#123; Console.WriteLine("VIP票："); Console.WriteLine("增加积分！"); return Price * 0.5; &#125; return Price; //如果不满足任何打折要求，则返回原始票价 &#125; &#125; 它至少存在如下三个问题： OldMovieTicket类的GetPrice()方法非常庞大，它包含各种打折算法的实现代码，在代码中出现了较长的if…else…语句，不利于测试和维护 增加新的打折算法或者对原有打折算法进行修改时必须修改OldMovieTicket类的源代码，违反了“开闭原则”，系统的灵活性和可扩展性较差。 算法的复用性差，如果在另一个系统（如商场销售管理系统）中需要重用某些打折算法，只能通过对源代码进行复制粘贴来重用，无法单独重用其中的某个或某些算法（重用较为麻烦）。 重构 MovieTicket充当环境类角色，Discount充当抽象策略角色，StudentDiscount、 ChildrenDiscount 和VIPDiscount充当具体策略角色 人员排序(python实现)有一 Person 类，有年龄（age）、体重（weight）、身高（height）三个属性。现要对 Person 的一组对象进行排序，但并没有确定根据什么规则来排序，有时需要根据年龄进行排序，有时需要根据身高进行排序，有时可能是根据身高和体重的综合情况来排序，还有可能…… 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Person: "人类" def __init__(self, name, age, weight, height): self.name = name self.age = age self.weight = weight self.height = height def showMysef(self): print(self.name + " " + str(self.age) + " years old, " + str(self.weight) + "kg, " + str(self.height) + "m.")class ICompare: "比较算法" def comparable(self, person1, person2): "person1 &gt; person2 返回值&gt;0，person1 == person2 返回0， person1 &lt; person2 返回值小于0" passclass CompareByAge(ICompare): "通过年龄排序" def comparable(self, person1, person2): return person1.age - person2.ageclass CompareByHeight(ICompare): "通过身高进行排序" def comparable(self, person1, person2): return person1.height - person2.heightclass SortPerson: "Person的排序类" def __init__(self, compare): self.__compare = compare def sort(self, personList): "排序算法，这里采用最简单的冒泡排序" n = len(personList) for i in range(0, n - 1): for j in range(0, n - i - 1): if (self.__compare.comparable(personList[j], personList[j + 1]) &gt; 0): tmp = personList[j] personList[j] = personList[j + 1] personList[j + 1] = tmp j += 1 i += 1 测试代码: 1234567891011121314151617181920def testSortPerson(): personList = [ Person("Tony", 2, 54.5, 0.82), Person("Jack", 31, 74.5, 1.80), Person("Nick", 54, 44.5, 1.59), Person("Eric", 23, 62.0, 1.78), Person("Helen", 16, 45.7, 1.60) ] sorter0 = SortPerson(CompareByAge()) sorter0.sort(personList) print("根据年龄进行排序后的结果：") for person in personList: person.showMysef() print() sorter1 = SortPerson(CompareByHeight()) sorter1.sort(personList) print("根据身高进行排序后的结果：") for person in personList: person.showMysef() 输出结果:12345678910111213根据年龄进行排序后的结果：Tony 2 years old, 54.5kg, 0.82m.Helen 16 years old, 45.7kg, 1.6m.Eric 23 years old, 62.0kg, 1.78m.Jack 31 years old, 74.5kg, 1.8m.Nick 54 years old, 44.5kg, 1.59m.根据身高进行排序后的结果：Tony 2 years old, 54.5kg, 0.82m.Nick 54 years old, 44.5kg, 1.59m.Helen 16 years old, 45.7kg, 1.6m.Eric 23 years old, 62.0kg, 1.78m.Jack 31 years old, 74.5kg, 1.8m. 类图表示如下： python本身其实也可以实现 123456789101112131415161718192021222324from operator import itemgetter,attrgetterdef testPersonListInPython(): "用Python的方式对Person进行排序" personList = [ Person("Tony", 2, 54.5, 0.82), Person("Jack", 31, 74.5, 1.80), Person("Nick", 54, 44.5, 1.59), Person("Eric", 23, 62.0, 1.78), Person("Helen", 16, 45.7, 1.60) ] # 使用使用operator模块根据年龄进行排序 print("根据年龄进行排序后的结果：") sortedPerons = sorted(personList, key = attrgetter('age')) for person in sortedPerons: person.showMysef() print() print("根据身高进行排序后的结果：") sortedPerons1 = sorted(personList, key=attrgetter('height')) for person in sortedPerons1: person.showMysef() 为了学习设计模式,我们舍弃了Python本身的语言特性. 另外,Python 语言本身的特性，还是难以实现一些特殊的需求，如要根据身高和体重的综合情况来排序（身高和体重的权重分别是 0.6 和 0.4）。用策略模式就可以很方便地实现，只需要增加一个CompareByHeightAndWeight的策略类就可以，如下面代码： 1234567class CompareByHeightAndWeight(ICompare): "根据身高和体重的综合情况来排序(身高和体重的权重分别是0.6和0.4)" def comparable(self, person1, person2): value1 = person1.height * 0.6 + person1.weight * 0.4 value2 = person2.height * 0.6 + person2.weight * 0.4 return value1 - value2 策略模式(Python实现)12345678910111213141516171819202122232425262728293031323334353637383940"""http://stackoverflow.com/questions/963965/how-is-this-strategy-pattern-written-in-python-the-sample-in-wikipediaIn most of other languages Strategy pattern is implemented via creating some base strategy interface/abstract class andsubclassing it with a number of concrete strategies (as we can see at http://en.wikipedia.org/wiki/Strategy_pattern),however Python supports higher-order functions and allows us to have only one class and inject functions into it'sinstances, as shown in this example."""import typesclass StrategyExample: def __init__(self, func=None): self.name = 'Strategy Example 0' if func is not None: self.execute = types.MethodType(func, self) def execute(self): print(self.name)def execute_replacement1(self): print(self.name + ' from execute 1')def execute_replacement2(self): print(self.name + ' from execute 2')if __name__ == '__main__': strat0 = StrategyExample() strat1 = StrategyExample(execute_replacement1) strat1.name = 'Strategy Example 1' strat2 = StrategyExample(execute_replacement2) strat2.name = 'Strategy Example 2' strat0.execute() strat1.execute() strat2.execute() Javascript中的策略模式在JavaScript 语言中，函数也是对象，所以更简单和直接的做法是把strategy直接定义为函数： 计算奖金很多公司的年终奖是根据员工的工资基数和年底绩效情况来发放的。例如，绩效为S 的人年终奖有4 倍工资，绩效为A 的人年终奖有3 倍工资，而绩效为B 的人年终奖是2 倍工资。假设财务部要求我们提供一段代码，来方便他们计算员工的年终奖。 123456789101112131415161718var strategies = &#123; "S": function( salary )&#123; return salary * 4; &#125;, "A": function( salary )&#123; return salary * 3; &#125;, "B": function( salary )&#123; return salary * 2; &#125; &#125;; var calculateBonus = function( level, salary )&#123; return strategies[ level ]( salary ); &#125;; console.log( calculateBonus( 'S', 20000 ) ); // 输出：80000 console.log( calculateBonus( 'A', 10000 ) ); // 输出：30000 实现缓动动画原理用JavaScript 实现动画效果的原理跟动画片的制作一样，动画片是把一些差距不大的原画以较快的帧数播放，来达到视觉上的动画效果。在JavaScript 中，可以通过连续改变元素的某个CSS属性，比如left、top、background-position 来实现动画效果。下图 就是通过改变节点的background-position，让人物动起来的。 思路和一些准备工作我们目标是编写一个动画类和一些缓动算法，让小球以各种各样的缓动效果在页面中运动。现在来分析实现这个程序的思路。在运动开始之前，需要提前记录一些有用的信息，至少包括以下信息： 动画开始时，小球所在的原始位置； 小球移动的目标位置； 动画开始时的准确时间点； 小球运动持续的时间。 表单校验假设我们正在编写一个注册的页面，在点击注册按钮之前，有如下几条校验逻辑。 用户名不能为空。 密码长度不能少于 6 位。 手机号码必须符合格式。 第一个版本1234567891011121314151617181920212223242526272829&lt;html&gt;&lt;body&gt; &lt;form action="http:// xxx.com/register" id="registerForm" method="post"&gt; 请输入用户名：&lt;input type="text" name="userName"/&gt; 请输入密码：&lt;input type="text" name="password"/&gt; 请输入手机号码：&lt;input type="text" name="phoneNumber"/&gt; &lt;button&gt;提交&lt;/button&gt; &lt;/form&gt; &lt;script&gt; var registerForm = document.getElementById('registerForm'); registerForm.onsubmit = function () &#123; if (registerForm.userName.value === '') &#123; alert('用户名不能为空'); return false; &#125; if (registerForm.password.value.length &lt; 6) &#123; alert('密码长度不能少于6 位'); return false; &#125; if (!/(^1[3|5|8][0-9]&#123;9&#125;$)/.test(registerForm.phoneNumber.value)) &#123; alert('手机号码格式不正确'); return false; &#125; &#125; &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 缺点如下: registerForm.onsubmit函数比较庞大，包含了很多 if-else 语句，这些语句需要覆盖所有的校验规则。 registerForm.onsubmit 函数缺乏弹性，如果增加了一种新的校验规则，或者想把密码的长度校验从 6 改成 8，我们都必须深入registerForm.onsubmit函数的内部实现，这是违反开 放—封闭原则的。 算法的复用性差 用策略模式重构第一步,我们要把这些校验逻辑都封装成策略对象： 1234567891011121314151617var strategies = &#123; isNonEmpty: function (value, errorMsg) &#123; // 不为空 if (value === '') &#123; return errorMsg; &#125; &#125;, minLength: function (value, length, errorMsg) &#123; // 限制最小长度 if (value.length &lt; length) &#123; return errorMsg; &#125; &#125;, isMobile: function (value, errorMsg) &#123; // 手机号码格式 if (!/(^1[3|5|8][0-9]&#123;9&#125;$)/.test(value)) &#123; return errorMsg; &#125; &#125; &#125;; 接下来,我们准备实现Validator 类。Validator类在这里作为 Context，负责接收用户的请求 并委托给 strategy 对象。在给出Validator 类的代码之前，有必要提前了解用户是如何向 Validator类发送请求的，这有助于我们知道如何去编写 Validator 类的代码。代码如下： 123456789101112131415161718var validataFunc = function () &#123; var validator = new Validator(); // 创建一个validator 对象 /***************添加一些校验规则****************/ validator.add(registerForm.userName, 'isNonEmpty', '用户名不能为空'); validator.add(registerForm.password, 'minLength:6', '密码长度不能少于6 位'); validator.add(registerForm.phoneNumber, 'isMobile', '手机号码格式不正确'); var errorMsg = validator.start(); // 获得校验结果 return errorMsg; // 返回校验结果 &#125;var registerForm = document.getElementById('registerForm'); registerForm.onsubmit = function () &#123; var errorMsg = validataFunc(); // 如果errorMsg 有确切的返回值，说明未通过校验 if (errorMsg) &#123; alert(errorMsg); return false; // 阻止表单提交 &#125; &#125;; 从这段代码中可以看到，我们先创建了一个 validator 对象，然后通过 validator.add 方法， 往 validator 对象中添加一些校验规则。validator.add 方法接受 3 个参数，validator.add( registerForm.password, &#39;minLength:6&#39;, &#39;密码长度不能少于 6 位&#39; )这句代码说明： registerForm.password 为参与校验的 input 输入框。 minLength:6是一个以冒号隔开的字符串。冒号前面的minLength代表客户挑选的strategy对象，冒号后面的数字 6 表示在校验过程中所必需的一些参数。&#39;minLength:6&#39;的意思就是 校验 registerForm.password 这个文本输入框的 value 最小长度为 6。如果这个字符串中不 包含冒号，说明校验过程中不需要额外的参数信息，比如&#39;isNonEmpty&#39;。 第 3 个参数是当校验未通过时返回的错误信息。 当我们往 validator 对象里添加完一系列的校验规则之后，会调用 validator.start()方法来 启动校验。如果 validator.start()返回了一个确切的 errorMsg 字符串当作返回值，说明该次校验 没有通过，此时需让 registerForm.onsubmit 方法返回 false 来阻止表单的提交。 最后,是 Validator 类的实现： 12345678910111213141516171819202122var Validator = function () &#123; this.cache = []; // 保存校验规则 &#125;; Validator.prototype.add = function (dom, rule, errorMsg) &#123; var ary = rule.split(':'); // 把strategy 和参数分开 this.cache.push(function () &#123; // 把校验的步骤用空函数包装起来，并且放入cache var strategy = ary.shift(); // 用户挑选的strategy ary.unshift(dom.value); // 把input 的value 添加进参数列表 ary.push(errorMsg); // 把errorMsg 添加进参数列表 return strategies[strategy].apply(dom, ary); &#125;); &#125;; Validator.prototype.start = function () &#123; for (var i = 0, validatorFunc; validatorFunc = this.cache[i++];) &#123; var msg = validatorFunc(); // 开始校验，并取得校验后的返回信息 if (msg) &#123; // 如果有确切的返回值，说明校验没有通过 return msg; &#125; &#125; &#125;; 使用策略模式重构代码之后，我们仅仅通过“配置”的方式就可以完成一个表单的校验，这些校验规则也可以复用在程序的任何地方，还能作为插件的形式，方便地被移植到其他项 目中。 在修改某个校验规则的时候，只需要编写或者改写少量的代码。比如我们想将用户名输入框 的校验规则改成用户名不能少于 4 个字符。可以看到，这时候的修改是毫不费力的。代码如下： 123validator.add( registerForm.userName, 'isNonEmpty', '用户名不能为空' ); // 改成： validator.add( registerForm.userName, 'minLength:10', '用户名长度不能小于 10 位' ); 给某个文本输入框添加多种校验规则为了让读者把注意力放在策略模式的使用上，目前我们的表单校验实现留有一点小遗憾：一 个文本输入框只能对应一种校验规则，比如，用户名输入框只能校验输入是否为空： 1validator.add( registerForm.userName, 'isNonEmpty', '用户名不能为空' ); 如果我们既想校验它是否为空，又想校验它输入文本的长度不小于 10 呢？我们期望以这样 的形式进行校验： 1234567validator.add(registerForm.userName, [&#123; strategy: 'isNonEmpty', errorMsg: '用户名不能为空' &#125;, &#123; strategy: 'minLength:6', errorMsg: '用户名长度不能小于10 位' &#125;]); 实现如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&lt;html&gt;&lt;body&gt; &lt;form action="http:// xxx.com/register" id="registerForm" method="post"&gt; 请输入用户名：&lt;input type="text" name="userName"/&gt; 请输入密码：&lt;input type="text" name="password"/&gt; 请输入手机号码：&lt;input type="text" name="phoneNumber"/&gt; &lt;button&gt;提交&lt;/button&gt; &lt;/form&gt; &lt;script&gt; /***********************策略对象**************************/ var strategies = &#123; isNonEmpty: function (value, errorMsg) &#123; if (value === '') &#123; return errorMsg; &#125; &#125;, minLength: function (value, length, errorMsg) &#123; if (value.length &lt; length) &#123; return errorMsg; &#125; &#125;, isMobile: function (value, errorMsg) &#123; if (!/(^1[3|5|8][0-9]&#123;9&#125;$)/.test(value)) &#123; return errorMsg; &#125; &#125; &#125;; /***********************Validator 类**************************/ var Validator = function () &#123; this.cache = []; &#125;; Validator.prototype.add = function (dom, rules) &#123; var self = this; for (var i = 0, rule; rule = rules[i++];) &#123; (function (rule) &#123; var strategyAry = rule.strategy.split(':'); var errorMsg = rule.errorMsg; self.cache.push(function () &#123; var strategy = strategyAry.shift(); strategyAry.unshift(dom.value); strategyAry.push(errorMsg); return strategies[strategy].apply(dom, strategyAry); &#125;); &#125;)(rule) &#125; &#125;; Validator.prototype.start = function () &#123; for (var i = 0, validatorFunc; validatorFunc = this.cache[i++];) &#123; var errorMsg = validatorFunc(); if (errorMsg) &#123; return errorMsg; &#125; &#125; &#125;; /***********************客户调用代码**************************/ var registerForm = document.getElementById('registerForm'); var validataFunc = function () &#123; var validator = new Validator(); validator.add(registerForm.userName, [&#123; strategy: 'isNonEmpty', errorMsg: '用户名不能为空' &#125;, &#123; strategy: 'minLength:6', errorMsg: '用户名长度不能小于10 位' &#125;]); validator.add(registerForm.password, [&#123; strategy: 'minLength:6', errorMsg: '密码长度不能小于6 位' &#125;]); var errorMsg = validator.start(); return errorMsg; &#125; registerForm.onsubmit = function () &#123; var errorMsg = validataFunc(); if (errorMsg) &#123; alert(errorMsg); return false; &#125; &#125;; &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[装饰者模式]]></title>
    <url>%2F2019%2F05%2F04%2F%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[原理装饰模式（Decorator Pattern）：动态地给一个对象增加一些额外的职责（Responsibility），就增加对象功能来说，装饰模式比生成子类实现更为灵活。 结构图 示例穿衣服(python实现)给自己搭配了一套着装：一条卡其色休闲裤、一双深色休闲皮鞋、一条银色针扣头的黑色腰带、一件紫红色针织毛衣、一件白色衬衫、一副方形黑框眼镜。但类似的着装也可以穿在其他的人身上，比如一个老师也可以这样穿：一双深色休闲皮鞋、一件白色衬衫、一副方形黑框眼镜。 我们就用程序来模拟这样一个情景。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113class Person: "人" def __init__(self, name): self.__name = name def getName(self): return self.__name def wear(self): print("我的着装是：")class Engineer(Person): "工程师" def __init__(self, name, skill): super().__init__(name) self.__skill = skill def getSkill(self): return self.__skill def wear(self): print("我是" + self.getSkill() + "工程师" + self.getName()) super().wear()class Teacher(Person): "教师" def __init__(self, name, title): super().__init__(name) self.__title = title def getTitle(self): return self.__title def wear(self): print("我是" + self.getName() + self.getTitle()) super().wear()class ClothingDecorator(Person): "服装装饰器" def __init__(self, person): self._decorated = person def wear(self): self._decorated.wear()class CasualPantDecorator(ClothingDecorator): "休闲裤" def __init__(self, person): super().__init__(person) def wear(self): super().wear() print("一条卡其色休闲裤")class BeltDecorator(ClothingDecorator): "腰带" def __init__(self, person): super().__init__(person) def wear(self): super().wear() print("一条银色针扣头的黑色腰带")class LeatherShoesDecorator(ClothingDecorator): "皮鞋" def __init__(self, person): super().__init__(person) def wear(self): super().wear() print("一双深色休闲皮鞋")class KnittedSweaterDecorator(ClothingDecorator): "针织毛衣" def __init__(self, person): super().__init__(person) def wear(self): super().wear() print("一件紫红色针织毛衣")class WhiteShirtDecorator(ClothingDecorator): "白色衬衫" def __init__(self, person): super().__init__(person) def wear(self): super().wear() print("一件白色衬衫")class GlassesDecorator(ClothingDecorator): "眼镜" def __init__(self, person): super().__init__(person) def wear(self): super().wear() print("一副方形黑框眼镜") 客户端:12345678910111213def testDecorator(): tony = Engineer("Tony", "客户端") pant = CasualPantDecorator(tony) belt = BeltDecorator(pant) shoes = LeatherShoesDecorator(belt) shirt = WhiteShirtDecorator(shoes) sweater = KnittedSweaterDecorator(shirt) glasses = GlassesDecorator(sweater) glasses.wear() print() decorateTeacher = GlassesDecorator(WhiteShirtDecorator(LeatherShoesDecorator(Teacher("wells", "教授")))) decorateTeacher.wear() decorateTeacher = GlassesDecorator(WhiteShirtDecorator(LeatherShoesDecorator(Teacher(&quot;wells&quot;, &quot;教授&quot;))))这个写法，大家不要觉得奇怪，它其实就是将多个对象的创建过程合在了一起，其实是一种优雅的写法（是不是少了好几行代码？）。创建的 Teacher 对象又通过参数传给 LeatherShoesDecorator的构造函数，而创建的 LeatherShoesDecorator 对象又通过参数传给WhiteShirtDecorator的构造函数，以此类推…… 输出结果： 1234567891011121314我是客户端工程师Tony我的着装是：一条卡其色休闲裤一条银色针扣头的黑色腰带一双深色休闲皮鞋一件白色衬衫一件紫红色针织毛衣一副方形黑框眼镜我是wells教授我的着装是：一双深色休闲皮鞋一件白色衬衫一副方形黑框眼镜 装饰关系VS继承关系装饰关系: 继承关系: 装饰模式的特点 可灵活地给一个对象增加职责或拓展功能 如上面的示例中，可任意地穿上自己想穿的衣服。不管穿上什么衣服，你还是那个你，但穿上不同的衣服你就会有不同的外表。 可增加任意多个装饰 你可以只穿一件衣服，也可以只穿一条裤子，也可以衣服和裤子各种搭配的穿，全随你意！ 装饰的顺序不同，可能产生不同的效果 在上面的示例中，Tony 是针织毛衣穿在外面，白色衬衫穿在里面。当然，如果你愿意（或因为怕冷），也可以针织毛衣穿在里面，白色衬衫穿在外面。但两种着装穿出来的效果，给人的感觉肯定是完全不一样的，自己脑补一下，哈哈！ 使用装饰模式的方式，想要改变装饰的顺序，也是非常简单的。只要把测试代码稍微改动一下即可，如下： 123456789def testDecorator2(): tony = Engineer("Tony", "客户端") pant = CasualPantDecorator(tony) belt = BeltDecorator(pant) shoes = LeatherShoesDecorator(belt) sweater = KnittedSweaterDecorator(shoes) shirt = WhiteShirtDecorator(sweater) glasses = GlassesDecorator(shirt) glasses.wear() 结果如下： 12345678我是客户端工程师Tony我的着装是：一条卡其色休闲裤一条银色针扣头的黑色腰带一双深色休闲皮鞋一件紫红色针织毛衣一件白色衬衫一副方形黑框眼镜 模型抽象 上图中的 Component 是一个抽象类，代表具有某中功能（function）的组件，ComponentImplA 和 ComponentImplB 分别是其具体的实现子类。Decorator 是 Component 装饰器，里面有一个 Component 的对象 decorated，这就是被装饰的对象，装饰器可为被装饰对象添加额外的功能或行为（addBehavior）。DecoratorImplA 和 DecoratorImplB 分别是两个具体的装饰器（实现子类）。 这样一种模式很好地将装饰器与被装饰的对象进行解耦。 Js中的装饰模式(Js实现)用 AOP 装饰函数12345678910111213141516171819// 用 AOP 装饰函数Function.prototype.before = function (beforefn) &#123; var __self = this; // 保存原函数的引用 return function () &#123; // 返回包含了原函数和新函数的"代理"函数 beforefn.apply(this, arguments); // 执行新函数，且保证this 不被劫持，新函数接受的参数 // 也会被原封不动地传入原函数，新函数在原函数之前执行 return __self.apply(this, arguments); // 执行原函数并返回原函数的执行结果， // 并且保证this 不被劫持 &#125;&#125;;Function.prototype.after = function (afterfn) &#123; var __self = this; return function () &#123; var ret = __self.apply(this, arguments); afterfn.apply(this, arguments); return ret; &#125;&#125;; 给 window 绑定 onload 事件不使用AOP12345678window.onload = function()&#123; alert (1); &#125; var _onload = window.onload || function()&#123;&#125;; window.onload = function()&#123; _onload(); alert (2); &#125; 使用AOP12345678910window.onload = function()&#123; alert (1); &#125; window.onload = ( window.onload || function()&#123;&#125; ).after(function()&#123; alert (2); &#125;).after(function()&#123; alert (3); &#125;).after(function()&#123; alert (4); &#125;); 不污染原型1234567891011var before = function( fn, beforefn )&#123; return function()&#123; beforefn.apply( this, arguments ); return fn.apply( this, arguments ); &#125; &#125; var a = before( function()&#123;alert (3)&#125;, function()&#123;alert (2)&#125; ); a = before( a, function()&#123;alert (1);&#125; ); a(); AOP 的应用实例数据统计上报页面中有一个登录 button，点击这个 button 会弹出登录浮层，与此同时要进行数据上报，来统计有多少用户点击了这个登录 button： 不使用AOP1234567891011121314&lt;html&gt;&lt;button tag="login" id="button"&gt;点击打开登录浮层&lt;/button&gt;&lt;script&gt; var showLogin = function()&#123; console.log( '打开登录浮层' ); log( this.getAttribute( 'tag' ) ); &#125; var log = function( tag )&#123; console.log( '上报标签为: ' + tag );// (new Image).src = 'http:// xxx.com/report?tag=' + tag; // 真正的上报代码略&#125;document.getElementById( 'button' ).onclick = showLogin;&lt;/script&gt;&lt;/html&gt; 在 showLogin 函数里，既要负责打开登录浮层，又要负责数据上报，这是两个层面的功能，在此处却被耦合在一个函数里 使用AOP123456789101112131415161718192021&lt;html&gt;&lt;button tag="login" id="button"&gt;点击打开登录浮层&lt;/button&gt;&lt;script&gt; Function.prototype.after = function (afterfn) &#123; var __self = this; return function () &#123; var ret = __self.apply(this, arguments); afterfn.apply(this, arguments); return ret; &#125; &#125;; var showLogin = function () &#123; console.log('打开登录浮层'); &#125; var log = function () &#123; console.log('上报标签为: ' + this.getAttribute('tag')); &#125; showLogin = showLogin.after(log); // 打开登录浮层之后上报数据 document.getElementById('button').onclick = showLogin;&lt;/script&gt; 动态改变函数的参数观察Function.prototype.before 方法： 1234567Function.prototype.before = function (beforefn) &#123; var __self = this; return function () &#123; beforefn.apply(this, arguments); // (1) return __self.apply(this, arguments); // (2) &#125; &#125; 从这段代码的(1)处和(2)处可以看到，beforefn 和原函数__self 共用一组参数列表arguments，当我们在beforefn 的函数体内改变arguments 的时候，原函数__self 接收的参数列表自然也会变化。 1234567var func = function( param )&#123;console.log( param ); &#125;func = func.before( function( param )&#123;param.b = 'b';&#125;);func( &#123;a: 'a'&#125; );// 输出： &#123;a: "a", b: "b"&#125; 在ajax请求中增加token 12345678910111213var getToken = function () &#123; return 'Token'; &#125; var ajax = function (type, url, param) &#123; console.dir(param); // 发送ajax 请求的代码略 &#125;; ajax = ajax.before(function (type, url, param) &#123; param.token = getToken(); &#125;); ajax('get', 'http:// xxx.com/userinfo', &#123; name: 'sven' &#125;); //&#123; name: "sven", token: "Token" &#125; 从ajax 函数打印的log 可以看到，token 参数已经被附加到了ajax 请求的参数中：{name: &quot;sven&quot;, Token: &quot;Token&quot;}明显可以看到，用AOP 的方式给ajax 函数动态装饰上Token 参数，保证了ajax 函数是一个相对纯净的函数，提高了ajax 函数的可复用性，它在被迁往其他项目的时候，不需要做任何修改。 插件式的表单验证我们很多人都写过许多表单验证的代码，在一个Web 项目中，可能存在非常多的表单，如注册、登录、修改用户信息等。在表单数据提交给后台之前，常常要做一些校验，比如登录的时候需要验证用户名和密码是否为空，代码如下： 123456789101112131415161718192021222324252627282930313233&lt;html&gt;&lt;body&gt; 用户名：&lt;input id="username" type="text" /&gt; 密码： &lt;input id="password" type="password" /&gt; &lt;input id="submitBtn" type="button" value="提交"&gt;&lt;/button&gt;&lt;/body&gt;&lt;script&gt; var username = document.getElementById('username'), password = document.getElementById('password'), submitBtn = document.getElementById('submitBtn'); var formSubmit = function () &#123; if (username.value === '') &#123; return alert('用户名不能为空'); &#125; if (password.value === '') &#123; return alert('密码不能为空'); &#125; var param = &#123; username: username.value, password: password.value &#125; ajax('http:// xxx.com/login', param); // ajax 具体实现略 &#125; submitBtn.onclick = function () &#123; formSubmit(); &#125;&lt;/script&gt; &lt;/html&gt; formSubmit 函数在此处承担了两个职责，除了提交ajax 请求之外，还要验证用户输入的合法性。这种代码一来会造成函数臃肿，职责混乱，二来谈不上任何可复用性。 可以将校验输入的逻辑放到validata函数中，并且约定当validata 函数返回false 的时候，表示校验未通过，代码如下： 123456789101112131415161718192021var validata = function () &#123; if (username.value === '') &#123; alert('用户名不能为空'); return false; &#125; if (password.value === '') &#123; alert('密码不能为空'); return false; &#125; &#125; var formSubmit = function () &#123; if (validata() === false) &#123; // 校验未通过 return; &#125; var param = &#123; username: username.value, password: password.value &#125; ajax('http:// xxx.com/login', param); &#125; 现在的代码已经有了一些改进，我们把校验的逻辑都放到了validata 函数中，但formSubmit函数的内部还要计算validata 函数的返回值，因为返回值的结果表明了是否通过校验。接下来进一步优化这段代码，使validata 和formSubmit 完全分离开来。首先要改写Function.prototype.before， 如果beforefn 的执行结果返回false，表示不再执行后面的原函数，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738submitBtn.onclick = function () &#123; formSubmit(); &#125; Function.prototype.before = function (beforefn) &#123; var __self = this; return function () &#123; if (beforefn.apply(this, arguments) === false) &#123; // beforefn 返回false 的情况直接return，不再执行后面的原函数 return; &#125; return __self.apply(this, arguments); &#125; &#125; var validata = function () &#123; if (username.value === '') &#123; alert('用户名不能为空'); return false; &#125; if (password.value === '') &#123; alert('密码不能为空'); return false; &#125; &#125; var formSubmit = function () &#123; var param = &#123; username: username.value, password: password.value &#125; ajax('http:// xxx.com/login', param); &#125; formSubmit = formSubmit.before(validata); submitBtn.onclick = function () &#123; formSubmit(); &#125; 在这段代码中，校验输入和提交表单的代码完全分离开来，它们不再有任何耦合关系，formSubmit = formSubmit.before( validata )这句代码，如同把校验规则动态接在formSubmit 函数之前，validata 成为一个即插即用的函数，它甚至可以被写成配置文件的形式，这有利于我们分开维护这两个函数。再利用策略模式稍加改造，我们就可以把这些校验规则都写成插件的形式，用在不同的项目当中 日志记录(C#实现)现在要求我们开发的记录日志的组件，除了要支持数据库记录DatabaseLog和文本文件记录TextFileLog两种方式外，我们还需要在不同的应用环境中增加一些额外的功能，比如需要记录日志信息的错误严重级别，需要记录日志信息的优先级别，还有日志信息的扩展属性等功能。在这里，如果我们不去考虑设计模式，解决问题的方法其实很简单，可以通过继承机制去实现，日志类结构图如下： 实现如下: 1234567891011121314151617181920public abstract class Log&#123; public abstract void Write(string log);&#125;public class DatabaseLog : Log&#123; public override void Write(string log) &#123; Console.WriteLine("记录到数据库"); &#125;&#125;public class TextFileLog : Log&#123; public override void Write(string log) &#123; Console.WriteLine("记录到文本"); &#125;&#125; 需要记录日志信息的错误严重级别功能和记录日志信息优先级别的功能，只要在原来子类DatabaseLog和TextFileLog的基础上再生成子类即可，同时需要引进两个新的接口IError和IPriority，类结构图如下： 实现代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public interface IError&#123; void SetError();&#125;public interface IPriority&#123; void SetPriority();&#125;public class DBErrorLog : DatabaseLog, IError&#123; public override void Write(string log) &#123; base.Write(log); &#125; public void SetError() &#123; Console.WriteLine("......功能扩展，实现了记录错误严重级别"); &#125;&#125;public class DBPriorityLog : DatabaseLog, IPriority&#123; public override void Write(string log) &#123; base.Write(log); &#125; public void SetPriority() &#123; Console.WriteLine("......功能扩展，实现了记录优先级别"); &#125;&#125;public class TFErrorLog : TextFileLog, IError&#123; public override void Write(string log) &#123; base.Write(log); &#125; public void SetError() &#123; Console.WriteLine("......功能扩展，实现了记录错误严重级别"); &#125;&#125;public class TFPriorityLog : TextFileLog, IPriority&#123; public override void Write(string log) &#123; base.Write(log); &#125; public void SetPriority() &#123; Console.WriteLine("......功能扩展，实现了记录优先级别"); &#125;&#125; 此时可以看到，如果需要相应的功能，直接使用这些子类就可以了。这里我们采用了类的继承方式来解决了对象功能的扩展问题，这种方式是可以达到我们预期的目的。 然而，它却带来了一系列的问题: 首先，前面的分析只是进行了一种功能的扩展，如果既需要记录错误严重级别，又需要记录优先级时，子类就需要进行接口的多重继承，这在某些情况下会违反类的单一职责原则，注意下图中的蓝色区域： 实现代码： 1234567891011121314151617181920212223242526272829303132333435363738public class DBEPLog : DatabaseLog, IError, IPriority&#123; public override void Write(string log) &#123; SetError();SetPriority(); base.Write(log); &#125; public void SetError() &#123; //......功能扩展，实现了记录错误严重级别 &#125; public void SetPriority() &#123; //......功能扩展，实现了记录优先级别 &#125;&#125;public class TFEPLog : DatabaseLog, IError, IPriority &#123; public override void Write(string log) &#123; SetError(); SetPriority(); base.Write(log); &#125; public void SetError() &#123; //......功能扩展，实现了记录错误严重级别 &#125; public void SetPriority() &#123; //......功能扩展，实现了记录优先级别 &#125;&#125; 其次，随着以后扩展功能的增多，子类会迅速的膨胀，可以看到，子类的出现其实是DatabaseLog和TextFileLog两个子类与新增加的接口的一种排列组合关系，所以类结构会变得很复杂而难以维护,“子类复子类，子类何其多” 最后，这种方式的扩展是一种静态的扩展方式，并没有能够真正实现扩展功能的动态添加，客户程序不能选择添加扩展功能的方式和时机。 现在又该是Decorator模式出场的时候了，解决方案是把Log对象嵌入到另一个对象中，由这个对象来扩展功能。首先我们要定义一个抽象的包装类LogWrapper，让它继承于Log类，结构图如下： 实现代码如下：1234567891011121314public class LogWrapper: Log &#123; private readonly Log _log; public LogWrapper(Log log) &#123; _log = log; &#125; public override void Write(string log) &#123; _log.Write(log); &#125; &#125; 现在对于每个扩展的功能，都增加一个包装类的子类，让它们来实现具体的扩展功能，如下图中绿色的区域： 实现如下 1234567891011121314151617181920212223242526272829303132333435public class LogErrorWrapper : LogWrapper&#123; public LogErrorWrapper(Log _log) : base(_log) &#123; &#125; public override void Write(string log) &#123; SetError(); base.Write(log); &#125; public void SetError() &#123; Console.WriteLine("......功能扩展，实现了记录错误严重级别"); &#125;&#125;public class LogPriorityWrapper : LogWrapper&#123; public LogPriorityWrapper(Log log) : base(log) &#123; &#125; public override void Write(string log) &#123; SetPriority(); base.Write(log); &#125; public void SetPriority() &#123; Console.WriteLine("......功能扩展,实现了记录优先级别"); &#125;&#125; 到这里，LogErrorWrapper类和LogPriorityWrapper类真正实现了对错误严重级别和优先级别的功能的扩展。我们来看一下客户程序如何去调用它： 12345678910111213141516171819202122class Program&#123; static void Main(string[] args) &#123; Log dbLog = new DatabaseLog(); LogWrapper logErrorWrapper1 = new LogErrorWrapper(dbLog); //扩展了记录错误严重级别 logErrorWrapper1.Write("Log Message"); Console.WriteLine(); LogPriorityWrapper logPriorityWrapper1 = new LogPriorityWrapper(dbLog); //扩展了记录优先级别 logPriorityWrapper1.Write("Log Message"); Console.WriteLine(); LogWrapper logErrorWrapper2 = new LogErrorWrapper(dbLog); LogPriorityWrapper logPriorityWrapper2 = new LogPriorityWrapper(logErrorWrapper2); //同时扩展了错误严重级别和优先级别 logPriorityWrapper2.Write("Log Message"); &#125;&#125; 注意在上面程序中的第三段装饰才真正体现出了Decorator模式的精妙所在，这里总共包装了两次：第一次对log对象进行错误严重级别的装饰，变成了logErrorWrapper2对象，第二次再对logErrorWrapper2对象进行装饰，于是变成了logPriorityWrapper2对象，此时的logPriorityWrapper2对象同时扩展了错误严重级别和优先级别的功能。也就是说我们需要哪些功能，就可以这样继续包装下去。到这里也许有人会说LogPriorityWrapper类的构造函数接收的是一个Log对象，为什么这里可以传入LogErrorWrapper对象呢？通过类结构图就能发现，LogErrorWrapper类其实也是Log类的一个子类。 我们分析一下这样会带来什么好处？首先对于扩展功能已经实现了真正的动态增加，只在需要某种功能的时候才进行包装；其次，如果再出现一种新的扩展功能，只需要增加一个对应的包装子类（注意：这一点任何时候都是避免不了的），而无需再进行很多子类的继承，不会出现子类的膨胀，同时Decorator模式也很好的符合了面向对象设计原则中的“优先使用对象组合而非继承”和“开放-封闭”原则。 .NET中的Stream(C#实现) 可以看到， BufferedStream和CryptoStream其实就是两个包装类，这里的Decorator模式省略了抽象装饰角色（Decorator），示例代码如下： 123456789101112131415class Program&#123; public static void Main(string[] args) &#123; MemoryStream ms = new MemoryStream(new byte[] &#123; 100, 112, 114, 222, 255 &#125;); //扩展了缓冲的功能 BufferedStream buff = new BufferedStream(ms); //扩展了缓冲，加密的功能 CryptoStream crypto = new CryptoStream(buff, null, CryptoStreamMode.Write); &#125;&#125; 其中BufferedStream类的代码（只列出部分），它是继承于Stream类： 123456789101112public sealed class BufferedStream : Stream&#123; // Methods private BufferedStream(); public BufferedStream(Stream stream); public BufferedStream(Stream stream, int bufferSize); // Fields private int _bufferSize; private Stream _s;&#125; java中也是如此,java IO中的 FilterInputStream 和 FilterOutputStream的实现其实就是一个装饰模式。FilterInputStream(FilterOutputStream) 就是一个装饰器，而 InputStream(OutputStream) 就是被装饰的对象。 12DataInputStream dataInputStream = new DataInputStream(new FileInputStream("C:/text.txt"));DataOutputStream dataOutputStream = new DataOutputStream(new FileOutputStream("C:/text.txt")); 优缺点装饰模式的优点： 使用装饰模式来实现扩展比继承更加灵活，它可以在不需要创造更多子类的情况下，将对象的功能加以扩展。 可以动态地给一个对象附加更多的功能。 可以用不同的装饰器进行多重装饰，装饰的顺序不同，可能产生不同的效果。 装饰类和被装饰类可以独立发展，不会相互耦合；装饰模式相当于是继承的一个替代模式。 装饰模式的缺点： 与继承相比，用装饰的方式拓展功能更加容易出错，排错也更困难。对于多次装饰的对象，调试时寻找错误可能需要逐级排查，较为烦琐。 应用场景 有大量独立的扩展，为支持每一种组合将产生大量的子类，使得子类数目呈爆炸性增长。 需要动态地增加或撤销功能时。 不能采用生成子类的方法进行扩充时，如类定义不能用于生成子类。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[职责链模式]]></title>
    <url>%2F2019%2F05%2F02%2F%E8%81%8C%E8%B4%A3%E9%93%BE%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[[TOC] 原理定义从生活中的例子可以发现，某个请求可能需要几个人的审批，即使技术经理审批完了，还需要上一级的审批。这样的例子，还有公司中的请假，少于3天的，直属Leader就可以批准，3天到7天之内就需要项目经理批准，多余7天的就需要技术总监的批准了。介绍了这么多生活中责任链模式的例子的，下面具体给出面向对象中责任链模式的定义。 责任链模式指的是——某个请求需要多个对象进行处理，从而避免请求的发送者和接收之间的耦合关系。将这些对象连成一条链子，并沿着这条链子传递该请求，直到有对象处理它为止。 结构图 主要涉及两个角色： 抽象处理者角色（Handler）：定义出一个处理请求的接口。这个接口通常由接口或抽象类来实现。 具体处理者角色（ConcreteHandler）：具体处理者接受到请求后，可以选择将该请求处理掉，或者将请求传给下一个处理者。因此，每个具体处理者需要保存下一个处理者的引用，以便把请求传递下去。 示例采购商品(C#实现)公司规定， 采购架构总价在1万之内，经理级别的人批准即可， 总价大于1万小于2万5的则还需要副总进行批准， 总价大于2万5小于10万的需要还需要总经理批准， 而大于总价大于10万的则需要组织一个会议进行讨论 对于这样一个需求，最直观的方法就是设计一个方法，参数是采购的总价，然后在这个方法内对价格进行调整判断，然后针对不同的条件交给不同级别的人去处理，这样确实可以解决问题，但这样一来，我们就需要多重if-else语句来进行判断，但当加入一个新的条件范围时，我们又不得不去修改原来设计的方法来再添加一个条件判断，这样的设计显然违背了“开-闭”原则。这时候，可以采用责任链模式来解决这样的问题。具体实现代码如下所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108// 采购请求 public class PurchaseRequest &#123; // 金额 public double Amount &#123; get; set; &#125; // 产品名字 public string ProductName &#123; get; set; &#125; public PurchaseRequest(double amount, string productName) &#123; Amount = amount; ProductName = productName; &#125; &#125; // 审批人,Handler public abstract class Approver &#123; public Approver NextApprover &#123; get; set; &#125; public string Name &#123; get; set; &#125; public Approver(string name) &#123; this.Name = name; &#125; public abstract void ProcessRequest(PurchaseRequest request); &#125; // ConcreteHandler public class Manager : Approver &#123; public Manager(string name) : base(name) &#123; &#125; public override void ProcessRequest(PurchaseRequest request) &#123; if (request.Amount &lt; 10000.0) &#123; Console.WriteLine("&#123;0&#125;-&#123;1&#125; approved the request of purshing &#123;2&#125;", this, Name, request.ProductName); &#125; else if (NextApprover != null) &#123; NextApprover.ProcessRequest(request); &#125; &#125; &#125; // ConcreteHandler,副总 public class VicePresident : Approver &#123; public VicePresident(string name) : base(name) &#123; &#125; public override void ProcessRequest(PurchaseRequest request) &#123; if (request.Amount &lt; 25000.0) &#123; Console.WriteLine("&#123;0&#125;-&#123;1&#125; approved the request of purshing &#123;2&#125;", this, Name, request.ProductName); &#125; else if (NextApprover != null) &#123; NextApprover.ProcessRequest(request); &#125; &#125; &#125; // ConcreteHandler，总经理 public class President :Approver &#123; public President(string name) : base(name) &#123; &#125; public override void ProcessRequest(PurchaseRequest request) &#123; if (request.Amount &lt; 100000.0) &#123; Console.WriteLine("&#123;0&#125;-&#123;1&#125; approved the request of purshing &#123;2&#125;", this, Name, request.ProductName); &#125; else &#123; Console.WriteLine("Request需要组织一个会议讨论"); &#125; &#125; &#125; class Program &#123; static void Main(string[] args) &#123; PurchaseRequest requestTelphone = new PurchaseRequest(4000.0, "Telphone"); PurchaseRequest requestSoftware = new PurchaseRequest(10000.0, "Visual Studio"); PurchaseRequest requestComputers = new PurchaseRequest(40000.0, "Computers"); Approver manager = new Manager("LearningHard"); Approver Vp = new VicePresident("Tony"); Approver Pre = new President("BossTom"); // 设置责任链 manager.NextApprover = Vp; Vp.NextApprover = Pre; // 处理请求 manager.ProcessRequest(requestTelphone); manager.ProcessRequest(requestSoftware); manager.ProcessRequest(requestComputers); Console.ReadLine(); &#125; &#125; 购买商品(js实现)购买规则: 已支付500元定金的用户会收到100元的商城优惠券 200元定金的用户会收到50元的优惠券 没有支付定金的用户只能普通购买,没有优惠券不一定能买到 有以下参数 orderType 订单类型 1:500定金用户 2:200定金用户 3:普通用户 pay true:已支付 false:已支付 stock:库存 只有普通用户受限 普通代码123456789101112131415161718192021222324252627282930313233var order = function (orderType, pay, stock) &#123; if (orderType === 1) &#123; // 500 元定金购买模式 if (pay === true) &#123; // 已支付定金 console.log('500 元定金预购, 得到100 优惠券'); &#125; else &#123; // 未支付定金，降级到普通购买模式 if (stock &gt; 0) &#123; // 用于普通购买的手机还有库存 console.log('普通购买, 无优惠券'); &#125; else &#123; console.log('手机库存不足'); &#125; &#125; &#125; else if (orderType === 2) &#123; // 200 元定金购买模式 if (pay === true) &#123; console.log('200 元定金预购, 得到50 优惠券'); &#125; else &#123; if (stock &gt; 0) &#123; console.log('普通购买, 无优惠券'); &#125; else &#123; console.log('手机库存不足'); &#125; &#125; &#125; else if (orderType === 3) &#123; if (stock &gt; 0) &#123; console.log('普通购买, 无优惠券'); &#125; else &#123; console.log('手机库存不足'); &#125; &#125;&#125;;order(1, true, 500); // 输出： 500 元定金预购, 得到100 优惠券 用职责链模式重构12345678910111213141516171819202122232425262728293031var order500 = function (orderType, pay, stock) &#123; if (orderType === 1 &amp;&amp; pay === true) &#123; console.log('500 元定金预购, 得到100 优惠券'); &#125; else &#123; order200(orderType, pay, stock); // 将请求传递给200 元订单 &#125;&#125;;// 200 元订单var order200 = function (orderType, pay, stock) &#123; if (orderType === 2 &amp;&amp; pay === true) &#123; console.log('200 元定金预购, 得到50 优惠券'); &#125; else &#123; orderNormal(orderType, pay, stock); // 将请求传递给普通订单 &#125;&#125;;// 普通购买订单var orderNormal = function (orderType, pay, stock) &#123; if (stock &gt; 0) &#123; console.log('普通购买, 无优惠券'); &#125; else &#123; console.log('手机库存不足'); &#125;&#125;;// 测试结果：order500(1, true, 500); // 输出：500 元定金预购, 得到100 优惠券order500(1, false, 500); // 输出：普通购买, 无优惠券order500(2, true, 500); // 输出：200 元定金预购, 得到500 优惠券order500(3, false, 500); // 输出：普通购买, 无优惠券order500(3, false, 0); // 输出：手机库存不足 好了很多仍然有一些问题 职责链的顺序非常僵硬 灵活拆分职责链节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354var order500 = function (orderType, pay, stock) &#123; if (orderType === 1 &amp;&amp; pay === true) &#123; console.log('500 元定金预购，得到100 优惠券'); &#125; else &#123; return 'nextSuccessor'; // 我不知道下一个节点是谁，反正把请求往后面传递 &#125;&#125;;var order200 = function (orderType, pay, stock) &#123; if (orderType === 2 &amp;&amp; pay === true) &#123; console.log('200 元定金预购，得到50 优惠券'); &#125; else &#123; return 'nextSuccessor'; // 我不知道下一个节点是谁，反正把请求往后面传递 &#125;&#125;;var orderNormal = function (orderType, pay, stock) &#123; if (stock &gt; 0) &#123; console.log('普通购买，无优惠券'); &#125; else &#123; console.log('手机库存不足'); &#125;&#125;;// Chain.prototype.setNextSuccessor 指定在链中的下一个节点// Chain.prototype.passRequest 传递请求给某个节点var Chain = function (fn) &#123; this.fn = fn; this.successor = null;&#125;;Chain.prototype.setNextSuccessor = function (successor) &#123; return this.successor = successor;&#125;;Chain.prototype.passRequest = function () &#123; var ret = this.fn.apply(this, arguments); if (ret === 'nextSuccessor') &#123; return this.successor &amp;&amp; this.successor.passRequest.apply(this.successor, arguments); &#125; return ret;&#125;;var chainOrder500 = new Chain(order500);var chainOrder200 = new Chain(order200);var chainOrderNormal = new Chain(orderNormal);chainOrder500.setNextSuccessor(chainOrder200);chainOrder200.setNextSuccessor(chainOrderNormal);chainOrder500.passRequest(1, true, 500); // 输出：500 元定金预购，得到100 优惠券chainOrder500.passRequest(2, true, 500); // 输出：200 元定金预购，得到50 优惠券chainOrder500.passRequest(3, true, 500); // 输出：普通购买，无优惠券chainOrder500.passRequest(1, false, 0); // 输出：手机库存不足 用AOP实现职责链123456789101112131415Function.prototype.after = function (fn) &#123; var self = this; return function () &#123; var ret = self.apply(this, arguments); if (ret === 'nextSuccessor') &#123; return fn.apply(this, arguments); &#125; return ret; &#125;&#125;;var order = order500yuan.after(order200yuan).after(orderNormal);order(1, true, 500); // 输出：500 元定金预购，得到100 优惠券order(2, true, 500); // 输出：200 元定金预购，得到50 优惠券order(1, false, 500); // 输出：普通购买，无优惠券 获取文件上传对象(js实现)用职责链模式实现之前迭代器获取文件对象的例子 12345678910111213141516171819202122var getActiveUploadObj = function () &#123; try &#123; return new ActiveXObject("TXFTNActiveX.FTNUpload"); // IE 上传控件 &#125; catch (e) &#123; return false; &#125;&#125;;var getFlashUploadObj = function () &#123; if (supportFlash()) &#123; // supportFlash 函数未提供 var str = '&lt;object type="application/x-shockwave-flash"&gt;&lt;/object&gt;'; return $(str).appendTo($('body')); &#125; return false;&#125;;var getFormUploadObj = function () &#123; var str = '&lt;input name="file" type="file" class="ui-file"/&gt;'; // 表单上传 return $(str).appendTo($('body'));&#125;;var getUploadObj = getActiveUploadObj.after(getFlashUploadObj).after(getFormUploadObj); 职责链模式(python实现)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# http://www.testingperspective.com/wiki/doku.php/collaboration/chetan/designpatternsinpython/chain-of-responsibilitypatternclass Handler: def successor(self, successor): self.successor = successorclass ConcreteHandler1(Handler): def handle(self, request): if request &gt; 0 and request &lt;= 10: print("in handler1") else: self.successor.handle(request)class ConcreteHandler2(Handler): def handle(self, request): if request &gt; 10 and request &lt;= 20: print("in handler2") else: self.successor.handle(request)class ConcreteHandler3(Handler): def handle(self, request): if request &gt; 20 and request &lt;= 30: print("in handler3") else: print('end of chain, no handler for &#123;&#125;'.format(request))class Client: def __init__(self): h1 = ConcreteHandler1() h2 = ConcreteHandler2() h3 = ConcreteHandler3() h1.successor(h2) h2.successor(h3) requests = [2, 5, 14, 22, 18, 3, 35, 27, 20] for request in requests: h1.handle(request)if __name__ == "__main__": client = Client() 结果: 优缺点责任链模式的优点不言而喻，主要有以下点： 降低了请求的发送者和接收者之间的耦合。 把多个条件判定分散到各个处理类中，使得代码更加清晰，责任更加明确。 责任链模式也具有一定的缺点，如： 在找到正确的处理对象之前，所有的条件判定都要执行一遍，当责任链过长时，可能会引起性能的问题 可能导致某个请求不被处理。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]Distributed systems for fun and profit_6延伸阅读]]></title>
    <url>%2F2019%2F04%2F30%2F%5B%E8%AF%91%5DDistributed%20systems%20for%20fun%20and%20profit_6%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[六. Further reading and appendixIf you’ve made it this far, thank you. If you liked the book, follow me on Github (or Twitter). I love seeing that I’ve had some kind of positive impact. “Create more value than you capture” and all that. Many many thanks to: logpath, alexras, globalcitizen, graue, frankshearar, roryokane, jpfuentes2, eeror, cmeiklejohn, stevenproctor eos2102 and steveloughran for their help! Of course, any mistakes and omissions that remain are my fault! It’s worth noting that my chapter on eventual consistency is fairly Berkeley-centric; I’d like to change that. I’ve also skipped one prominent use case for time: consistent snapshots. There are also a couple of topics which I should expand on: namely, an explicit discussion of safety and liveness properties and a more detailed discussion of consistent hashing. However, I’m off to Strange Loop 2013, so whatever. If this book had a chapter 6, it would probably be about the ways in which one can make use of and deal with large amounts of data. It seems that the most common type of “big data” computation is one in which a large dataset is passed through a single simple program. I’m not sure what the subsequent chapters would be (perhaps high performance computing, given that the current focus has been on feasibility), but I’ll probably know in a couple of years. Books about distributed systemsDistributed Algorithms (Lynch)This is probably the most frequently recommended book on distributed algorithms. I’d also recommend it, but with a caveat. It is very comprehensive, but written for a graduate student audience, so you’ll spend a lot of time reading about synchronous systems and shared memory algorithms before getting to things that are most interesting to a practitioner. Introduction to Reliable and Secure Distributed Programming (Cachin, Guerraoui &amp; Rodrigues)For a practitioner, this is a fun one. It’s short and full of actual algorithm implementations. Replication: Theory and PracticeIf you’re interested in replication, this book is amazing. The chapter on replication is largely based on a synthesis of the interesting parts of this book plus more recent readings. Distributed Systems: An Algorithmic Approach (Ghosh)Introduction to Distributed Algorithms (Tel)Transactional Information Systems: Theory, Algorithms, and the Practice of Concurrency Control and Recovery (Weikum &amp; Vossen)This book is on traditional transactional information systems, e.g. local RDBMS’s. There are two chapters on distributed transactions at the end, but the focus of the book is on transaction processing. Transaction Processing: Concepts and Techniques by Gray and ReuterA classic. I find that Weikum &amp; Vossen is more up to date. Seminal papersEach year, the Edsger W. Dijkstra Prize in Distributed Computing is given to outstanding papers on the principles of distributed computing. Check out the link for the full list, which includes classics such as: “Time, Clocks and Ordering of Events in a Distributed System“ - Leslie Lamport “Impossibility of Distributed Consensus With One Faulty Process“ - Fisher, Lynch, Patterson “Unreliable failure detectors and reliable distributed systems“ - Chandra and Toueg Microsoft Academic Search has a list of top publications in distributed &amp; parallel computing ordered by number of citations - this may be an interesting list to skim for more classics. Here are some additional lists of recommended papers: Nancy Lynch’s recommended reading list from her course on Distributed systems. NoSQL Summer paper list - a curated list of papers related to this buzzword. A Quora question on seminal papers in distributed systems. Systems The Google File System - Ghemawat, Gobioff and Leung MapReduce: Simplified Data Processing on Large Clusters - Dean and Ghemawat Dynamo: Amazon’s Highly Available Key-value Store - DeCandia et al. Bigtable: A Distributed Storage System for Structured Data - Chang et al. The Chubby Lock Service for Loosely-Coupled Distributed Systems - Burrows ZooKeeper: Wait-free coordination for Internet-scale systems - Hunt, Konar, Junqueira, Reed, 2010]]></content>
      <categories>
        <category>微服务理论文章阅读学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]Distributed systems for fun and profit_5复制 弱一致性模型协议]]></title>
    <url>%2F2019%2F04%2F30%2F%5B%E8%AF%91%5DDistributed%20systems%20for%20fun%20and%20profit_5%E5%A4%8D%E5%88%B6-%E5%BC%B1%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[五. Replication: weak consistency model protocols复制 弱一致性模型协议Now that we’ve taken a look at protocols that can enforce single-copy consistency under an increasingly realistic set of supported failure cases, let’s turn our attention at the world of options that opens up once we let go of the requirement of single-copy consistency. 第四章中提到的各个算法都是为了保证single-copy，也就是强一致性。在这一章节里，我们关注第二种策略：处理“不同”。即我们允许出现不同（而且不同是正常的），我们要做的就是如何处理这些不同然后得出正确的结果。 By and large, it is hard to come up with a single dimension that defines or characterizes the protocols that allow for replicas to diverge. Most such protocols are highly available, and the key issue is more whether or not the end users find the guarantees, abstractions and APIs useful for their purpose in spite of the fact that the replicas may diverge when node and/or network failures occur. 总的来说，很难想出一个单一维度来定义或表述允许副本分歧的协议。 大多数此类协议都是度可用的，而且关键在于发生节点或网络故障时是否能为用户提供有用的保证，抽象和API使他们达到原本的目的。 Why haven’t weakly consistent systems been more popular? 为什么弱一致系统没火起来? As I stated in the introduction, I think that much of distributed programming is about dealing with the implications of two consequences of distribution: that information travels at the speed of light that independent things fail independently 如图我在介绍章节说的那样，多数的分布式程序都是为了解决分布式导致的两个问题： 信息光速传播 独立事务独立失败 The implication that follows from the limitation on the speed at which information travels is that nodes experience the world in different, unique ways. Computation on a single node is easy, because everything happens in a predictable global total order. Computation on a distributed system is difficult, because there is no global total order. 信息传播速度限制所带来的意义是节点以不同的，独特的方式体验世界。 单个节点上的计算很容易，因为一切都以可预测的全局顺序发生。 而分布式系统上的计算很困难，因为没有全局顺序。 For the longest while (i.e. decades of research), we’ve solved this problem by introducing a global total order. I’ve discussed the many methods for achieving strong consistency by creating order (in a fault-tolerant manner) where there is no naturally occurring total order. 在最长的时间里（即数十年的研究），我们通过引入全局顺序来解决这个问题。 我已经讨论了通过创建顺序（以容错方式）实现强一致性的许多方法，其中没有自然发生的总顺序。 Of course, the problem is that enforcing order is expensive. This breaks down in particular with large scale internet systems, where a system needs to remain available. A system enforcing strong consistency doesn’t behave like a distributed system: it behaves like a single system, which is bad for availability during a partition. 但是很明显，问题在于执行顺序是代价很大。 特别是在大型互联网系统中，系统需要保持高可用状态。 强制一致性的系统的行为不太像分布式系统,而是类似于单个系统，这对分区可用性不利。 Furthermore, for each operation, often a majority of the nodes must be contacted - and often not just once, but twice (as you saw in the discussion on 2PC). This is particularly painful in systems that need to be geographically distributed to provide adequate performance for a global user base. 此外，对于每个操作，通常必须在大多数节点间通讯 - 通常不仅仅是一次，而是两次（正如您在2PC的讨论中看到的那样）。 在需要为全球用户群提供高性能服务的系统中，这尤其痛苦。 So behaving like a single system by default is perhaps not desirable. 因此，默认情况下表现得像单个系统可能并不理想 Perhaps what we want is a system where we can write code that doesn’t use expensive coordination, and yet returns a “usable” value. Instead of having a single truth, we will allow different replicas to diverge from each other - both to keep things efficient but also to tolerate partitions - and then try to find a way to deal with the divergence in some manner. 也许我们想要的是这样一个系统，可以编写不用编写复杂的资源调度代码，但仍然返回“可用”值。相比获取唯一的正确值, 我们更希望允许不同的副本彼此分离 -这 不仅保持了数据的有效性而且做到了了容忍分区 ,并且试图找到一种以某种方式处理分歧的方法。 Eventual consistency expresses this idea: that nodes can for some time diverge from each other, but that eventually they will agree on the value. 最终的一致性表达了这个想法：节点可以在一段时间内相互分离，但最终会达成一致值。 Within the set of systems providing eventual consistency, there are two types of system designs: 有两种类型的系统设计可以提供最终一致性： Eventual consistency with probabilistic guarantees. This type of system can detect conflicting writes at some later point, but does not guarantee that the results are equivalent to some correct sequential execution. In other words, conflicting updates will sometimes result in overwriting a newer value with an older one and some anomalies can be expected to occur during normal operation (or during partitions). 概率保证的最终一致性。 这种类型的系统可以在稍后的某点检测到冲突的写入，但不保证结果等同于某些正确的顺序执行。 换句话说，冲突的更新有时会导致用较旧的值覆盖较新的值，并且在正常操作期间（或在分区期间）可能会发生一些异常。 In recent years, the most influential system design is Amazon’s Dynamo, which I will discuss as an example of a system that offers eventual consistency with probabilistic guarantees. 近年来，最具影响力的系统设计是亚马逊的Dynamo，我将作为一个系统的例子进行讨论，该系统最终与概率保证保持一致。 Eventual consistency with strong guarantees. This type of system guarantees that the results converge to a common value equivalent to some correct sequential execution. In other words, such systems do not produce any anomalous results; without any coordination you can build replicas of the same service, and those replicas can communicate in any pattern and receive the updates in any order, and they will eventually agree on the end result as long as they all see the same information. 强保证的最终一致性。这种类型的系统保证结果收敛到相当于某些正确顺序执行的公共值。换句话说，这种系统不会产生任何异常结果;在没有任何协调的情况下，您可以构建相同服务的副本，并且这些副本可以以任何模式进行通信并以任何顺序接收更新，并且只要它们都看到相同的信息，它们最终将同意最终结果。 CRDT’s (convergent replicated data types) are data types that guarantee convergence to the same value in spite of network delays, partitions and message reordering. They are provably convergent, but the data types that can be implemented as CRDT’s are limited. CRDT（会聚复制数据类型）是数据类型，可以保证在网络延迟，分区和消息重新排序的情况下收敛到相同的值。它们可以证明是收敛的，但可以作为CRDT实现的数据类型是有限的。 The CALM (consistency as logical monotonicity) conjecture is an alternative expression of the same principle: it equates logical monotonicity with convergence. If we can conclude that something is logically monotonic, then it is also safe to run without coordination. Confluence analysis - in particular, as applied for the Bloom programming language - can be used to guide programmer decisions about when and where to use the coordination techniques from strongly consistent systems and when it is safe to execute without coordination. CALM（作为逻辑单调性的一致性）猜想是相同原理的另一种表达：它将逻辑单调性与收敛等同起来。如果我们可以断定某些东西在逻辑上是单调的，那么在没有协调的情况下运行也是安全的。汇流分析 - 特别是应用于Bloom编程语言 - 可用于指导程序员决定何时何地使用来自强一致系统的协调技术，以及何时可以安全地执行而无需协调。 Reconciling different operation orders 协调不同操作顺序What does a system that does not enforce single-copy consistency look like? Let’s try to make this more concrete by looking at a few examples. 不强制执行单拷贝一致性的系统是什么样的？ 让我们试着通过一些例子来说明这一点。 Perhaps the most obvious characteristic of systems that do not enforce single-copy consistency is that they allow replicas to diverge from each other. This means that there is no strictly defined pattern of communication: replicas can be separated from each other and yet continue to be available and accept writes. 也许不强制实现单拷贝一致性的系统最明显的特征是它们允许副本彼此分离。 这意味着没有严格定义的通信模式：副本可以彼此分离，但仍然可用并接受写入。 Let’s imagine a system of three replicas, each of which is partitioned from the others. For example, the replicas might be in different datacenters and for some reason unable to communicate. Each replica remains available during the partition, accepting both reads and writes from some set of clients: 让我们设想一个由三个副本组成的系统，每个副本都与其他副本分开。 例如，副本可能位于不同的数据中心，并且由于某种原因无法进行通信。 每个副本在分区期间仍然可用，接受来自某些客户端的读取和写入： 123456789[Clients] - &gt; [A]--- Partition ---[Clients] - &gt; [B]--- Partition ---[Clients] - &gt; [C] After some time, the partitions heal and the replica servers exchange information. They have received different updates from different clients and have diverged each other, so some sort of reconciliation needs to take place. What we would like to happen is that all of the replicas converge to the same result.一段时间后，分区会恢复，副本服务器会交换信息。 他们收到了来自不同客户的不同更新，并且相互分歧，因此需要进行某种协调。 我们希望发生的是所有副本都得到相同的结果。 12345[A] \ --&gt; [merge][B] / | |[C] ----[merge]---&gt; result Another way to think about systems with weak consistency guarantees is to imagine a set of clients sending messages to two replicas in some order. Because there is no coordination protocol that enforces a single total order, the messages can get delivered in different orders at the two replicas:考虑具有弱一致性保证的系统的另一种方法是想象一组客户端以某种顺序向两个副本发送消息。 由于没有强制执行单个总顺序的协调协议，因此可以在两个副本的不同订单中传递消息： 12[Clients] --&gt; [A] 1, 2, 3[Clients] --&gt; [B] 2, 3, 1 This is, in essence, the reason why we need coordination protocols. For example, assume that we are trying to concatenate a string and the operations in messages 1, 2 and 3 are: 1231: &#123; operation: concat(&apos;Hello &apos;) &#125;2: &#123; operation: concat(&apos;World&apos;) &#125;3: &#123; operation: concat(&apos;!&apos;) &#125; Then, without coordination, A will produce “Hello World!”, and B will produce “World!Hello “. 然后，在没有协调的情况下，A将产生“Hello World！”，B将产生“World！Hello”。 12A: concat(concat(concat(&apos;&apos;, &apos;Hello &apos;), &apos;World&apos;), &apos;!&apos;) = &apos;Hello World!&apos;B: concat(concat(concat(&apos;&apos;, &apos;World&apos;), &apos;!&apos;), &apos;Hello &apos;) = &apos;World!Hello &apos; This is, of course, incorrect. Again, what we’d like to happen is that the replicas converge to the same result. 这当然是不正确的。 同样，我们想要发生的是分区收敛到相同的结果。 Keeping these two examples in mind, let’s look at Amazon’s Dynamo first to establish a baseline, and then discuss a number of novel approaches to building systems with weak consistency guarantees, such as CRDT’s and the CALM theorem. 记住这两个例子，让我们首先看看亚马逊的Dynamo建立基线，然后讨论一些新的方法来构建具有弱一致性保证的系统，例如CRDT和CALM定理。 Amazon’s DynamoAmazon’s Dynamo system design (2007) is probably the best-known system that offers weak consistency guarantees but high availability. It is the basis for many other real world systems, including LinkedIn’s Voldemort, Facebook’s Cassandra and Basho’s Riak. Dynamo is an eventually consistent, highly available key-value store. A key value store is like a large hash table: a client can set values via set(key, value) and retrieve them by key using get(key). A Dynamo cluster consists of N peer nodes; each node has a set of keys which it is responsible for storing. Dynamo prioritizes availability over consistency; it does not guarantee single-copy consistency. Instead, replicas may diverge from each other when values are written; when a key is read, there is a read reconciliation phase that attempts to reconcile differences between replicas before returning the value back to the client. For many features on Amazon, it is more important to avoid outages than it is to ensure that data is perfectly consistent, as an outage can lead to lost business and a loss of credibility. Furthermore, if the data is not particularly important, then a weakly consistent system can provide better performance and higher availability at a lower cost than a traditional RDBMS. Since Dynamo is a complete system design, there are many different parts to look at beyond the core replication task. The diagram below illustrates some of the tasks; notably, how a write is routed to a node and written to multiple replicas. 1234567891011121314151617[ Client ] |( Mapping keys to nodes ) | V[ Node A ] | \( Synchronous replication task: minimum durability ) | \[ Node B] [ Node C ] A |( Conflict detection; asynchronous replication task: ensuring that partitioned / recovered nodes recover ) | V[ Node D] After looking at how a write is initially accepted, we’ll look at how conflicts are detected, as well as the asynchronous replica synchronization task. This task is needed because of the high availability design, in which nodes may be temporarily unavailable (down or partitioned). The replica synchronization task ensures that nodes can catch up fairly rapidly even after a failure. Consistent hashingWhether we are reading or writing, the first thing that needs to happen is that we need to locate where the data should live on the system. This requires some type of key-to-node mapping. In Dynamo, keys are mapped to nodes using a hashing technique known as consistent hashing (which I will not discuss in detail). The main idea is that a key can be mapped to a set of nodes responsible for it by a simple calculation on the client. This means that a client can locate keys without having to query the system for the location of each key; this saves system resources as hashing is generally faster than performing a remote procedure call. Partial quorumsOnce we know where a key should be stored, we need to do some work to persist the value. This is a synchronous task; the reason why we will immediately write the value onto multiple nodes is to provide a higher level of durability (e.g. protection from the immediate failure of a node). Just like Paxos or Raft, Dynamo uses quorums for replication. However, Dynamo’s quorums are sloppy (partial) quorums rather than strict (majority) quorums. Informally, a strict quorum system is a quorum system with the property that any two quorums (sets) in the quorum system overlap. Requiring a majority to vote for an update before accepting it guarantees that only a single history is admitted since each majority quorum must overlap in at least one node. This was the property that Paxos, for example, relied on. Partial quorums do not have that property; what this means is that a majority is not required and that different subsets of the quorum may contain different versions of the same data. The user can choose the number of nodes to write to and read from: the user can choose some number W-of-N nodes required for a write to succeed; and the user can specify the number of nodes (R-of-N) to be contacted during a read. W and R specify the number of nodes that need to be involved to a write or a read. Writing to more nodes makes writes slightly slower but increases the probability that the value is not lost; reading from more nodes increases the probability that the value read is up to date. The usual recommendation is that R + W &gt; N, because this means that the read and write quorums overlap in one node - making it less likely that a stale value is returned. A typical configuration is N = 3 (e.g. a total of three replicas for each value); this means that the user can choose between: 123R = 1, W = 3;R = 2, W = 2 orR = 3, W = 1 More generally, again assuming R + W &gt; N: R = 1, W = N: fast reads, slow writes R = N, W = 1: fast writes, slow reads R = N/2 and W = N/2 + 1: favorable to both N is rarely more than 3, because keeping that many copies of large amounts of data around gets expensive! As I mentioned earlier, the Dynamo paper has inspired many other similar designs. They all use the same partial quorum based replication approach, but with different defaults for N, W and R: Basho’s Riak (N = 3, R = 2, W = 2 default) Linkedin’s Voldemort (N = 2 or 3, R = 1, W = 1 default) Apache’s Cassandra (N = 3, R = 1, W = 1 default) There is another detail: when sending a read or write request, are all N nodes asked to respond (Riak), or only a number of nodes that meets the minimum (e.g. R or W; Voldemort). The “send-to-all” approach is faster and less sensitive to latency (since it only waits for the fastest R or W nodes of N) but also less efficient, while the “send-to-minimum” approach is more sensitive to latency (since latency communicating with a single node will delay the operation) but also more efficient (fewer messages / connections overall). What happens when the read and write quorums overlap, e.g. (R + W &gt; N)? Specifically, it is often claimed that this results in “strong consistency”. Is R + W &gt; N the same as “strong consistency”?No. It’s not completely off base: a system where R + W &gt; N can detect read/write conflicts, since any read quorum and any write quorum share a member. E.g. at least one node is in both quorums: 12 1 2 N/2+1 N/2+2 N[...] [R] [R + W] [W] [...] This guarantees that a previous write will be seen by a subsequent read. However, this only holds if the nodes in N never change. Hence, Dynamo doesn’t qualify, because in Dynamo the cluster membership can change if nodes fail. Dynamo is designed to be always writable. It has a mechanism which handles node failures by adding a different, unrelated server into the set of nodes responsible for certain keys when the original server is down. This means that the quorums are no longer guaranteed to always overlap. Even R = W = N would not qualify, since while the quorum sizes are equal to N, the nodes in those quorums can change during a failure. Concretely, during a partition, if a sufficient number of nodes cannot be reached, Dynamo will add new nodes to the quorum from unrelated but accessible nodes. Furthermore, Dynamo doesn’t handle partitions in the manner that a system enforcing a strong consistency model would: namely, writes are allowed on both sides of a partition, which means that for at least some time the system does not act as a single copy. So calling R + W &gt; N “strongly consistent” is misleading; the guarantee is merely probabilistic - which is not what strong consistency refers to. Conflict detection and read repairSystems that allow replicas to diverge must have a way to eventually reconcile two different values. As briefly mentioned during the partial quorum approach, one way to do this is to detect conflicts at read time, and then apply some conflict resolution method. But how is this done? In general, this is done by tracking the causal history of a piece of data by supplementing it with some metadata. Clients must keep the metadata information when they read data from the system, and must return back the metadata value when writing to the database. We’ve already encountered a method for doing this: vector clocks can be used to represent the history of a value. Indeed, this is what the original Dynamo design uses for detecting conflicts. However, using vector clocks is not the only alternative. If you look at many practical system designs, you can deduce quite a bit about how they work by looking at the metadata that they track. No metadata. When a system does not track metadata, and only returns the value (e.g. via a client API), it cannot really do anything special about concurrent writes. A common rule is that the last writer wins: in other words, if two writers are writing at the same time, only the value from the slowest writer is kept around. Timestamps. Nominally, the value with the higher timestamp value wins. However, if time is not carefully synchronized, many odd things can happen where old data from a system with a faulty or fast clock overwrites newer values. Facebook’s Cassandra is a Dynamo variant that uses timestamps instead of vector clocks. Version numbers. Version numbers may avoid some of the issues related with using timestamps. Note that the smallest mechanism that can accurately track causality when multiple histories are possible are vector clocks, not version numbers. Vector clocks. Using vector clocks, concurrent and out of date updates can be detected. Performing read repair then becomes possible, though in some cases (concurrent changes) we need to ask the client to pick a value. This is because if the changes are concurrent and we know nothing more about the data (as is the case with a simple key-value store), then it is better to ask than to discard data arbitrarily. When reading a value, the client contacts R of N nodes and asks them for the latest value for a key. It takes all the responses, discards the values that are strictly older (using the vector clock value to detect this). If there is only one unique vector clock + value pair, it returns that. If there are multiple vector clock + value pairs that have been edited concurrently (e.g. are not comparable), then all of those values are returned. As is obvious from the above, read repair may return multiple values. This means that the client / application developer must occasionally handle these cases by picking a value based on some use-case specific criterion. In addition, a key component of a practical vector clock system is that the clocks cannot be allowed to grow forever - so there needs to be a procedure for occasionally garbage collecting the clocks in a safe manner to balance fault tolerance with storage requirements. Replica synchronization: gossip and Merkle treesGiven that the Dynamo system design is tolerant of node failures and network partitions, it needs a way to deal with nodes rejoining the cluster after being partitioned, or when a failed node is replaced or partially recovered. Replica synchronization is used to bring nodes up to date after a failure, and for periodically synchronizing replicas with each other. Gossip is a probabilistic technique for synchronizing replicas. The pattern of communication (e.g. which node contacts which node) is not determined in advance. Instead, nodes have some probability p of attempting to synchronize with each other. Every t seconds, each node picks a node to communicate with. This provides an additional mechanism beyond the synchronous task (e.g. the partial quorum writes) which brings the replicas up to date. Gossip is scalable, and has no single point of failure, but can only provide probabilistic guarantees. In order to make the information exchange during replica synchronization efficient, Dynamo uses a technique called Merkle trees, which I will not cover in detail. The key idea is that a data store can be hashed at multiple different levels of granularity: a hash representing the whole content, half the keys, a quarter of the keys and so on. By maintaining this fairly granular hashing, nodes can compare their data store content much more efficiently than a naive technique. Once the nodes have identified which keys have different values, they exchange the necessary information to bring the replicas up to date. Dynamo in practice: probabilistically bounded staleness (PBS)And that pretty much covers the Dynamo system design: consistent hashing to determine key placement partial quorums for reading and writing conflict detection and read repair via vector clocks and gossip for replica synchronization How might we characterize the behavior of such a system? A fairly recent paper from Bailis et al. (2012) describes an approach called PBS (probabilistically bounded staleness) uses simulation and data collected from a real world system to characterize the expected behavior of such a system. PBS estimates the degree of inconsistency by using information about the anti-entropy (gossip) rate, the network latency and local processing delay to estimate the expected level of consistency of reads. It has been implemented in Cassandra, where timing information is piggybacked on other messages and an estimate is calculated based on a sample of this information in a Monte Carlo simulation. Based on the paper, during normal operation eventually consistent data stores are often faster and can read a consistent state within tens or hundreds of milliseconds. The table below illustrates amount of time required from a 99.9% probability of consistent reads given different R and W settings on empirical timing data from LinkedIn (SSD and 15k RPM disks) and Yammer: For example, going from R=1, W=1 to R=2, W=1 in the Yammer case reduces the inconsistency window from 1352 ms to 202 ms - while keeping the read latencies lower (32.6 ms) than the fastest strict quorum (R=3, W=1; 219.27 ms). For more details, have a look at the PBS website and the associated paper. Disorderly programmingLet’s look back at the examples of the kinds of situations that we’d like to resolve. The first scenario consisted of three different servers behind partitions; after the partitions healed, we wanted the servers to converge to the same value. Amazon’s Dynamo made this possible by reading from R out of N nodes and then performing read reconciliation. In the second example, we considered a more specific operation: string concatenation. It turns out that there is no known technique for making string concatenation resolve to the same value without imposing an order on the operations (i.e. without expensive coordination). However, there are operations which can be applied safely in any order, where a simple register would not be able to do so. As Pat Helland wrote: … operation-centric work can be made commutative (with the right operations and the right semantics) where a simple READ/WRITE semantic does not lend itself to commutativity. For example, consider a system that implements a simple accounting system with the debit and credit operations in two different ways: using a register with read and write operations, and using a integer data type with native debit and credit operations The latter implementation knows more about the internals of the data type, and so it can preserve the intent of the operations in spite of the operations being reordered. Debiting or crediting can be applied in any order, and the end result is the same: 12100 + credit(10) + credit(20) = 130 and100 + credit(20) + credit(10) = 130 However, writing a fixed value cannot be done in any order: if writes are reordered, the one of the writes will overwrite the other: 12100 + write(110) + write(130) = 130 but100 + write(130) + write(110) = 110 Let’s take the example from the beginning of this chapter, but use a different operation. In this scenario, clients are sending messages to two nodes, which see the operations in different orders: 12[Clients] --&gt; [A] 1, 2, 3[Clients] --&gt; [B] 2, 3, 1 Instead of string concatenation, assume that we are looking to find the largest value (e.g. MAX()) for a set of integers. The messages 1, 2 and 3 are: 1231: &#123; operation: max(previous, 3) &#125;2: &#123; operation: max(previous, 5) &#125;3: &#123; operation: max(previous, 7) &#125; Then, without coordination, both A and B will converge to 7, e.g.: 12A: max(max(max(0, 3), 5), 7) = 7B: max(max(max(0, 5), 7), 3) = 7 In both cases, two replicas see updates in different order, but we are able to merge the results in a way that has the same result in spite of what the order is. The result converges to the same answer in both cases because of the merge procedure (max) we used. It is likely not possible to write a merge procedure that works for all data types. In Dynamo, a value is a binary blob, so the best that can be done is to expose it and ask the application to handle each conflict. However, if we know that the data is of a more specific type, handling these kinds of conflicts becomes possible. CRDT’s are data structures designed to provide data types that will always converge, as long as they see the same set of operations (in any order). CRDTs: Convergent replicated data typesCRDTs (convergent replicated datatypes) exploit knowledge regarding the commutativity and associativity of specific operations on specific datatypes. In order for a set of operations to converge on the same value in an environment where replicas only communicate occasionally, the operations need to be order-independent and insensitive to (message) duplication/redelivery. Thus, their operations need to be: Associative (a+(b+c)=(a+b)+c), so that grouping doesn’t matter Commutative (a+b=b+a), so that order of application doesn’t matter Idempotent (a+a=a), so that duplication does not matter It turns out that these structures are already known in mathematics; they are known as join or meet semilattices. A lattice is a partially ordered set with a distinct top (least upper bound) and a distinct bottom (greatest lower bound). A semilattice is like a lattice, but one that only has a distinct top or bottom. A join semilattice is one with a distinct top (least upper bound) and a meet semilattice is one with a distinct bottom (greatest lower bound). Any data type that can be expressed as a semilattice can be implemented as a data structure which guarantees convergence. For example, calculating the max() of a set of values will always return the same result regardless of the order in which the values were received, as long as all values are eventually received, because the max() operation is associative, commutative and idempotent. For example, here are two lattices: one drawn for a set, where the merge operator is union(items) and one drawn for a strictly increasing integer counter, where the merge operator is max(values): 12345 &#123; a, b, c &#125; 7 / | \ / \&#123;a, b&#125; &#123;b,c&#125; &#123;a,c&#125; 5 7 | \ / | / / | \ &#123;a&#125; &#123;b&#125; &#123;c&#125; 3 5 7 With data types that can be expressed as semilattices, you can have replicas communicate in any pattern and receive the updates in any order, and they will eventually agree on the end result as long as they all see the same information. That is a powerful property that can be guaranteed as long as the prerequisites hold. However, expressing a data type as a semilattice often requires some level of interpretation. Many data types have operations which are not in fact order-independent. For example, adding items to a set is associative, commutative and idempotent. However, if we also allow items to be removed from a set, then we need some way to resolve conflicting operations, such as add(A) and remove(A). What does it mean to remove an element if the local replica never added it? This resolution has to be specified in a manner that is order-independent, and there are several different choices with different tradeoffs. This means that several familiar data types have more specialized implementations as CRDT’s which make a different tradeoff in order to resolve conflicts in an order-independent manner. Unlike a key-value store which simply deals with registers (e.g. values that are opaque blobs from the perspective of the system), someone using CRDTs must use the right data type to avoid anomalies. Some examples of the different data types specified as CRDT’s include: Counters Grow-only counter (merge = max(values); payload = single integer) Positive-negative counter (consists of two grow counters, one for increments and another for decrements) Registers Last Write Wins -register (timestamps or version numbers; merge = max(ts); payload = blob) Multi-valued -register (vector clocks; merge = take both) Sets Grow-only set (merge = union(items); payload = set; no removal) Two-phase set (consists of two sets, one for adding, and another for removing; elements can be added once and removed once) Unique set (an optimized version of the two-phase set) Last write wins set (merge = max(ts); payload = set) Positive-negative set (consists of one PN-counter per set item) Observed-remove set Graphs and text sequences (see the paper) To ensure anomaly-free operation, you need to find the right data type for your specific application - for example, if you know that you will only remove an item once, then a two-phase set works; if you will only ever add items to a set and never remove them, then a grow-only set works. Not all data structures have known implementations as CRDTs, but there are CRDT implementations for booleans, counters, sets, registers and graphs in the recent (2011) survey paper from Shapiro et al. Interestingly, the register implementations correspond directly with the implementations that key value stores use: a last-write-wins register uses timestamps or some equivalent and simply converges to the largest timestamp value; a multi-valued register corresponds to the Dynamo strategy of retaining, exposing and reconciling concurrent changes. For the details, I recommend that you take a look at the papers in the further reading section of this chapter. The CALM theoremThe CRDT data structures were based on the recognition that data structures expressible as semilattices are convergent. But programming is about more than just evolving state, unless you are just implementing a data store. Clearly, order-independence is an important property of any computation that converges: if the order in which data items are received influences the result of the computation, then there is no way to execute a computation without guaranteeing order. However, there are many programming models in which the order of statements does not play a significant role. For example, in the MapReduce model, both the Map and the Reduce tasks are specified as stateless tuple-processing tasks that need to be run on a dataset. Concrete decisions about how and in what order data is routed to the tasks is not specified explicitly, instead, the batch job scheduler is responsible for scheduling the tasks to run on the cluster. Similarly, in SQL one specifies the query, but not how the query is executed. The query is simply a declarative description of the task, and it is the job of the query optimizer to figure out an efficient way to execute the query (across multiple machines, databases and tables). Of course, these programming models are not as permissive as a general purpose programming language. MapReduce tasks need to be expressible as stateless tasks in an acyclic dataflow program; SQL statements can execute fairly sophisticated computations but many things are hard to express in it. However, it should be clear from these two examples that there are many kinds of data processing tasks which are amenable to being expressed in a declarative language where the order of execution is not explicitly specified. Programming models which express a desired result while leaving the exact order of statements up to an optimizer to decide often have semantics that are order-independent. This means that such programs may be possible to execute without coordination, since they depend on the inputs they receive but not necessarily the specific order in which the inputs are received. The key point is that such programs may be safe to execute without coordination. Without a clear rule that characterizes what is safe to execute without coordination, and what is not, we cannot implement a program while remaining certain that the result is correct. This is what the CALM theorem is about. The CALM theorem is based on a recognition of the link between logical monotonicity and useful forms of eventual consistency (e.g. confluence / convergence). It states that logically monotonic programs are guaranteed to be eventually consistent. Then, if we know that some computation is logically monotonic, then we know that it is also safe to execute without coordination. To better understand this, we need to contrast monotonic logic (or monotonic computations) with non-monotonic logic (or non-monotonic computations). Monotony if sentence φ is a consequence of a set of premises Γ, then it can also be inferred from any set Δ of premises extending Γ Most standard logical frameworks are monotonic: any inferences made within a framework such as first-order logic, once deductively valid, cannot be invalidated by new information. A non-monotonic logic is a system in which that property does not hold - in other words, if some conclusions can be invalidated by learning new knowledge. Within the artificial intelligence community, non-monotonic logics are associated with defeasible reasoning - reasoning, in which assertions made utilizing partial information can be invalidated by new knowledge. For example, if we learn that Tweety is a bird, we’ll assume that Tweety can fly; but if we later learn that Tweety is a penguin, then we’ll have to revise our conclusion. Monotonicity concerns the relationship between premises (or facts about the world) and conclusions (or assertions about the world). Within a monotonic logic, we know that our results are retraction-free: monotone computations do not need to be recomputed or coordinated; the answer gets more accurate over time. Once we know that Tweety is a bird (and that we’re reasoning using monotonic logic), we can safely conclude that Tweety can fly and that nothing we learn can invalidate that conclusion. While any computation that produces a human-facing result can be interpreted as an assertion about the world (e.g. the value of “foo” is “bar”), it is difficult to determine whether a computation in a von Neumann machine based programming model is monotonic, because it is not exactly clear what the relationship between facts and assertions are and whether those relationships are monotonic. However, there are a number of programming models for which determining monotonicity is possible. In particular, relational algebra (e.g. the theoretical underpinnings of SQL) and Datalog provide highly expressive languages that have well-understood interpretations. Both basic Datalog and relational algebra (even with recursion) are known to be monotonic. More specifically, computations expressed using a certain set of basic operators are known to be monotonic (selection, projection, natural join, cross product, union and recursive Datalog without negation), and non-monotonicity is introduced by using more advanced operators (negation, set difference, division, universal quantification, aggregation). This means that computations expressed using a significant number of operators (e.g. map, filter, join, union, intersection) in those systems are logically monotonic; any computations using those operators are also monotonic and thus safe to run without coordination. Expressions that make use of negation and aggregation, on the other hand, are not safe to run without coordination. It is important to realize the connection between non-monotonicity and operations that are expensive to perform in a distributed system. Specifically, both distributed aggregation and coordination protocols can be considered to be a form of negation. As Joe Hellerstein writes: To establish the veracity of a negated predicate in a distributed setting, an evaluation strategy has to start “counting to 0” to determine emptiness, and wait until the distributed counting process has definitely terminated. Aggregation is the generalization of this idea. and: This idea can be seen from the other direction as well. Coordination protocols are themselves aggregations, since they entail voting: Two-Phase Commit requires unanimous votes, Paxos consensus requires majority votes, and Byzantine protocols require a 2/3 majority. Waiting requires counting. If, then we can express our computation in a manner in which it is possible to test for monotonicity, then we can perform a whole-program static analysis that detects which parts of the program are eventually consistent and safe to run without coordination (the monotonic parts) - and which parts are not (the non-monotonic ones). Note that this requires a different kind of language, since these inferences are hard to make for traditional programming languages where sequence, selection and iteration are at the core. Which is why the Bloom language was designed. What is non-monotonicity good for?The difference between monotonicity and non-monotonicity is interesting. For example, adding two numbers is monotonic, but calculating an aggregation over two nodes containing numbers is not. What’s the difference? One of these is a computation (adding two numbers), while the other is an assertion (calculating an aggregate). How does a computation differ from an assertion? Let’s consider the query “is pizza a vegetable?”. To answer that, we need to get at the core: when is it acceptable to infer that something is (or is not) true? There are several acceptable answers, each corresponding to a different set of assumptions regarding the information that we have and the way we ought to act upon it - and we’ve come to accept different answers in different contexts. In everyday reasoning, we make what is known as the open-world assumption: we assume that we do not know everything, and hence cannot make conclusions from a lack of knowledge. That is, any sentence may be true, false or unknown. 123456 OWA + | OWA + Monotonic logic | Non-monotonic logicCan derive P(true) | Can assert P(true) | Cannot assert P(true)Can derive P(false) | Can assert P(false) | Cannot assert P(true)Cannot derive P(true) | Unknown | Unknownor P(false) When making the open world assumption, we can only safely assert something we can deduce from what is known. Our information about the world is assumed to be incomplete. Let’s first look at the case where we know our reasoning is monotonic. In this case, any (potentially incomplete) knowledge that we have cannot be invalidated by learning new knowledge. So if we can infer that a sentence is true based on some deduction, such as “things that contain two tablespoons of tomato paste are vegetables” and “pizza contains two tablespoons of tomato paste”, then we can conclude that “pizza is a vegetable”. The same goes for if we can deduce that a sentence is false. However, if we cannot deduce anything - for example, the set of knowledge we have contains customer information and nothing about pizza or vegetables - then under the open world assumption we have to say that we cannot conclude anything. With non-monotonic knowledge, anything we know right now can potentially be invalidated. Hence, we cannot safely conclude anything, even if we can deduce true or false from what we currently know. However, within the database context, and within many computer science applications we prefer to make more definite conclusions. This means assuming what is known as the closed-world assumption: that anything that cannot be shown to be true is false. This means that no explicit declaration of falsehood is needed. In other words, the database of facts that we have is assumed to be complete (minimal), so that anything not in it can be assumed to be false. For example, under the CWA, if our database does not have an entry for a flight between San Francisco and Helsinki, then we can safely conclude that no such flight exists. We need one more thing to be able to make definite assertions: logical circumscription. Circumscription is a formalized rule of conjecture. Domain circumscription conjectures that the known entities are all there are. We need to be able to assume that the known entities are all there are in order to reach a definite conclusion. 1234567 CWA + | CWA + Circumscription + | Circumscription + Monotonic logic | Non-monotonic logicCan derive P(true) | Can assert P(true) | Can assert P(true)Can derive P(false) | Can assert P(false) | Can assert P(false)Cannot derive P(true) | Can assert P(false) | Can assert P(false)or P(false) In particular, non-monotonic inferences need this assumption. We can only make a confident assertion if we assume that we have complete information, since additional information may otherwise invalidate our assertion. What does this mean in practice? First, monotonic logic can reach definite conclusions as soon as it can derive that a sentence is true (or false). Second, nonmonotonic logic requires an additional assumption: that the known entities are all there is. So why are two operations that are on the surface equivalent different? Why is adding two numbers monotonic, but calculating an aggregation over two nodes not? Because the aggregation does not only calculate a sum but also asserts that it has seen all of the values. And the only way to guarantee that is to coordinate across nodes and ensure that the node performing the calculation has really seen all of the values within the system. Thus, in order to handle non-monotonicity one needs to either use distributed coordination to ensure that assertions are made only after all the information is known or make assertions with the caveat that the conclusion can be invalidated later on. Handling non-monotonicity is important for reasons of expressiveness. This comes down to being able to express non-monotone things; for example, it is nice to be able to say that the total of some column is X. The system must detect that this kind of computation requires a global coordination boundary to ensure that we have seen all the entities. Purely monotone systems are rare. It seems that most applications operate under the closed-world assumption even when they have incomplete data, and we humans are fine with that. When a database tells you that a direct flight between San Francisco and Helsinki does not exist, you will probably treat this as “according to this database, there is no direct flight”, but you do not rule out the possibility that that in reality such a flight might still exist. Really, this issue only becomes interesting when replicas can diverge (e.g. during a partition or due to delays during normal operation). Then there is a need for a more specific consideration: whether the answer is based on just the current node, or the totality of the system. Further, since nonmonotonicity is caused by making an assertion, it seems plausible that many computations can proceed for a long time and only apply coordination at the point where some result or assertion is passed to a 3rd party system or end user. Certainly it is not necessary for every single read and write operation within a system to enforce a total order, if those reads and writes are simply a part of a long running computation. The Bloom languageThe Bloom language is a language designed to make use of the CALM theorem. It is a Ruby DSL which has its formal basis in a temporal logic programming language called Dedalus. In Bloom, each node has a database consisting of collections and lattices. Programs are expressed as sets of unordered statements which interact with collections (sets of facts) and lattices (CRDTs). Statements are order-independent by default, but one can also write non-monotonic functions. Have a look at the Bloom website and tutorials to learn more about Bloom. Further readingThe CALM theorem, confluence analysis and BloomJoe Hellerstein’s talk @RICON 2012 is a good introduction to the topic, as is Neil Conway’s talk @Basho. For Bloom in particular, see Peter Alvaro’s talk@Microsoft. The Declarative Imperative: Experiences and Conjectures in Distributed Logic - Hellerstein, 2010 Consistency Analysis in Bloom: a CALM and Collected Approach - Alvaro et al., 2011 Logic and Lattices for Distributed Programming - Conway et al., 2012 Dedalus: Datalog in Time and Space - Alvaro et al., 2011 CRDTsMarc Shapiro’s talk @ Microsoft is a good starting point for understanding CRDT’s. CRDTs: Consistency Without Concurrency Control - Letitia et al., 2009 A comprehensive study of Convergent and Commutative Replicated Data Types, Shapiro et al., 2011 An Optimized conflict-free Replicated Set - Bieniusa et al., 2012 Dynamo; PBS; optimistic replication Dynamo: Amazon’s Highly Available Key-value Store - DeCandia et al., 2007 PNUTS: Yahoo!’s Hosted Data Serving Platform - Cooper et al., 2008 The Bayou Architecture: Support for Data Sharing among Mobile Users - Demers et al. 1994 Probabilistically Bound Staleness for Practical Partial Quorums - Bailis et al., 2012 Eventual Consistency Today: Limitations, Extensions, and Beyond - Bailis &amp; Ghodsi, 2013 Optimistic replication - Saito &amp; Shapiro, 2005]]></content>
      <categories>
        <category>微服务理论文章阅读学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]Distributed systems for fun and profit_4复制]]></title>
    <url>%2F2019%2F04%2F29%2F%5B%E8%AF%91%5DDistributed%20systems%20for%20fun%20and%20profit_4%E5%A4%8D%E5%88%B6%2F</url>
    <content type="text"><![CDATA[四. Replication 复制The replication problem is one of many problems in distributed systems. I’ve chosen to focus on it over other problems such as leader election, failure detection, mutual exclusion, consensus and global snapshots because it is often the part that people are most interested in. One way in which parallel databases are differentiated is in terms of their replication features, for example. Furthermore, replication provides a context for many subproblems, such as leader election, failure detection, consensus and atomic broadcast. 复制是分布式系统中非常关键的一个点。下面将会从leader election、failure detection、mutual exclusion、consensus及global snapshots这些大家关心的问题展开介绍。区别并行数据库的一种方式是更具复制集的特征。 Replication is a group communication problem. What arrangement and communication pattern gives us the performance and availability characteristics we desire? How can we ensure fault tolerance, durability and non-divergence in the face of network partitions and simultaneous node failure? 复制是一组通信的问题。当网络发生分区或者节点同时崩溃时，怎样保证系统的容错性、持久性和非发散性？ Again, there are many ways to approach replication. The approach I’ll take here just looks at high level patterns that are possible for a system with replication. Looking at this visually helps keep the discussion focused on the overall pattern rather than the specific messaging involved. My goal here is to explore the design space rather than to explain the specifics of each algorithm. 复制的方式有很多种，这里并不对每种算法进行详细的介绍，只是进行一个方法的汇总讨论 Let’s first define what replication looks like. We assume that we have some initial database, and that clients make requests which change the state of the database. 首先，到底什么是复制集呢？我们认为，当我们拥有初始数据库时，客户通过一定的请求操作会改变数据库的状态 The arrangement and communication pattern can then be divided into several stages: (Request) The client sends a request to a server (Sync) The synchronous portion of the replication takes place (Response) A response is returned to the client (Async) The asynchronous portion of the replication takes place This model is loosely based on this article. Note that the pattern of messages exchanged in each portion of the task depends on the specific algorithm: I am intentionally trying to get by without discussing the specific algorithm. Given these stages, what kind of communication patterns can we create? And what are the performance and availability implications of the patterns we choose? 这个时候的处理和通信模式罗分为下面几个阶段： （Request）客户向服务发送一个请求 （Sync）复制中的同步部分发生 （Responses）向客户返回答复 （Async）复制中的异步部分发生 在这些阶段中，通信模式是怎样的呢？ Synchronous replication 同步复制The first pattern is synchronous replication (also known as active, or eager, or push, or pessimistic replication). Let’s draw what that looks like: 同步复制：悲观复制，接受到客户端的请求之后，各节点先同步并返回到此节点之后，再响应客户端 Here, we can see three distinct stages: first, the client sends the request. Next, what we called the synchronous portion of replication takes place. The term refers to the fact that the client is blocked - waiting for a reply from the system. 同步复制中有三个明确的阶段：首先，用户发送请求；接着，同步部分发生。这个时候用户是阻塞的-等待系统的回复的。 During the synchronous phase, the first server contacts the two other servers and waits until it has received replies from all the other servers. Finally, it sends a response to the client informing it of the result (e.g. success or failure). 在同步阶段，第一个服务器和另两个服务器之间进行通信，并且等到来自这两个服务器的回复后，最后将一个统一的结果告知客户（成功还是失败） All this seems straightforward. What can we say of this specific arrangement of communication patterns, without discussing the details of the algorithm during the synchronous phase? First, observe that this is a write N - of - N approach: before a response is returned, it has to be seen and acknowledged by every server in the system. 可以观察到，同步复制必须要当结果被所有的服务节点认可后才会返回一个值给客户 From a performance perspective, this means that the system will be as fast as the slowest server in it. The system will also be very sensitive to changes in network latency, since it requires every server to reply before proceeding. 从性能的角度来看，这意味着系统最快的反应时间取决于最慢的一个服务节点。这样的系统会对网络延迟非常敏感。 Given the N-of-N approach, the system cannot tolerate the loss of any servers. When a server is lost, the system can no longer write to all the nodes, and so it cannot proceed. It might be able to provide read-only access to the data, but modifications are not allowed after a node has failed in this design. 这就是N-of-N write架构，只有等所有N个节点成功写，才返回写成功给client。系统将不能忍受任一个服务节点发生数据丢失的情况。一旦有一个服务节点发生丢失，系统将不允许对数据进行写操作。同时修改操作也不会被允许，只会允许进行读操作 This arrangement can provide very strong durability guarantees: the client can be certain that all N servers have received, stored and acknowledged the request when the response is returned. In order to lose an accepted update, all N copies would need to be lost, which is about as good a guarantee as you can make. 这样能提供一个非常长的持久性保证：客户能够确保所有的服务节点都接收到了请求，并且对请求做出响应。只有当所有节点上的副本都丢失了，更新的数据才会丢失 Asynchronous replication 异步复制Let’s contrast this with the second pattern - asynchronous replication (a.k.a. passive replication, or pull replication, or lazy replication). As you may have guessed, this is the opposite of synchronous replication: 异步复制：积极复制、拉复制、惰性复制，接受到客户端的请求之后，先响应客户端，再进行各节点先同步并且返回到此节点 Here, the master (/leader / coordinator) immediately sends back a response to the client. It might at best store the update locally, but it will not do any significant work synchronously and the client is not forced to wait for more rounds of communication to occur between the servers. 这里，主节点先快速响应客户端。先局部快速存储更新，但是不会严格同步。客户端不需要等待所有结果完成后才能收到回复 At some later stage, the asynchronous portion of the replication task takes place. Here, the master contacts the other servers using some communication pattern, and the other servers update their copies of the data. The specifics depend on the algorithm in use. 在接下来的阶段，复制的异步部分发生。主节点利用通信模式对其它服务节点进行交流，接着其它节点对复制集进行更新。具体实施过程依赖于所选择的算法 What can we say of this specific arrangement without getting into the details of the algorithm? Well, this is a write 1 - of - N approach: a response is returned immediately and update propagation occurs sometime later. 那么抽象而言，异步复制就是一个 write 1 - of - N 的架构：立刻响应客户端，再接着进行各节点的更新同步 From a performance perspective, this means that the system is fast: the client does not need to spend any additional time waiting for the internals of the system to do their work. The system is also more tolerant of network latency, since fluctuations in internal latency do not cause additional waiting on the client side. 从性能的角度来看，这样的系统通常更快，因为客户端不需要等到所有的节点同步完成后才得到响应。同时这样的系统对于网络延迟的容忍性更高。 This arrangement can only provide weak, or probabilistic durability guarantees. If nothing goes wrong, the data is eventually replicated to all N machines. However, if the only server containing the data is lost before this can take place, the data is permanently lost. 这样的操作只能提供一个弱的，某种程度上的持久性保证。如果没有任何故障发生，数据最终在N台机器上的复制集都会保持一致。但一旦数据只在一台服务器上，这个服务器发生数据丢失的话，数据将会永久丢失（持久性：对数据所做的更改能够永久保存下来） Given the 1-of-N approach, the system can remain available as long as at least one node is up (at least in theory, though in practice the load will probably be too high). A purely lazy approach like this provides no durability or consistency guarantees; you may be allowed to write to the system, but there are no guarantees that you can read back what you wrote if any faults occur. 在异步复制中，系统只需要得到一个节点的响应就能保证它的可用性（理论上是这样，但是在现实中，实践中负载通常会很高，单位时间内承受的工作量大）。像这样的惰性复制不会保证持久性和一致性：你能够对数据进行写操作，但是一旦错误发生，不一定保证你能读到你所写的东西 Finally, it’s worth noting that passive replication cannot ensure that all nodes in the system always contain the same state. If you accept writes at multiple locations and do not require that those nodes synchronously agree, then you will run the risk of divergence: reads may return different results from different locations (particularly after nodes fail and recover), and global constraints (which require communicating with everyone) cannot be enforced. 积极复制不能保证系统中所有的节点都能保持在同样的状态。如果你在多个机器进行写操作，并且又不要求这些节点同步做出确定，那么很有可能造成数据不一致的结果：从不同位置上读取数据可能返回的是不同的值（尤其是节点发生故障后恢复之后），并且不能进行强制全局限制（需要各个节点进行通信） I haven’t really mentioned the communication patterns during a read (rather than a write), because the pattern of reads really follows from the pattern of writes: during a read, you want to contact as few nodes as possible. We’ll discuss this a bit more in the context of quorums. We’ve only discussed two basic arrangements and none of the specific algorithms. Yet we’ve been able to figure out quite a bit of about the possible communication patterns as well as their performance, durability guarantees and availability characteristics. An overview of major replication approaches 主要的复制方法Having discussed the two basic replication approaches: synchronous and asynchronous replication, let’s have a look at the major replication algorithms. 刚刚讨论了什么是同步复制和异步复制，现在来介绍相关的主要的复制算法 There are many, many different ways to categorize replication techniques. The second distinction (after sync vs. async) I’d like to introduce is between: Replication methods that prevent divergence (single copy systems) and Replication methods that risk divergence (multi-master systems) 两种复制方法： single copy systems 单拷贝系统：阻止数据不一致的复制方法 multi-master systems 多master系统：有数据不一致风险的复制方法 The first group of methods has the property that they “behave like a single system”. In particular, when partial failures occur, the system ensures that only a single copy of the system is active. Furthermore, the system ensures that the replicas are always in agreement. This is known as the consensus problem. 单拷贝系统复制方法表现的形式像单个系统一样。当局部发生故障时，系统保证只有一个系统单副本处于活跃状态。进一步说，就是系统保证复制集永远处于一致的状态。 Several processes (or computers) achieve consensus if they all agree on some value. More formally: Agreement: Every correct process must agree on the same value. Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process. Termination: All processes eventually reach a decision. Validity: If all correct processes propose the same value V, then all correct processes decide V. 服务进程统一统一某个值后才达成一致性，一致性定义具体如下： Agreement：所有正确进程必须做出相同的决议 Integrity：每个正确的进程最多承认一个值，一旦一个进程决定了某一个值，那么这个值肯定是被其它进程提出的 Termination：所有正确的进程最终都会同意某个值 Validity：如果所有正确的进程提出相同的值V，那么所有正确的节点进程达成一致，承认值V Mutual exclusion, leader election, multicast and atomic broadcast are all instances of the more general problem of consensus. Replicated systems that maintain single copy consistency need to solve the consensus problem in some way. leader election、failure detection、mutual exclusion、consensus及global snapshots这些都会涉及到一致性问题。保证单拷贝一致性的复制系统需要解决这些问题 The replication algorithms that maintain single-copy consistency include: 1n messages (asynchronous primary/backup) 2n messages (synchronous primary/backup) 4n messages (2-phase commit, Multi-Paxos) 6n messages (3-phase commit, Paxos with repeated leader election) 单拷贝一致性复制算法包括： （这些算法的容错性都各自不同。根据执行过程中的信息交互数量对这些算法进行了一个分类） 1n messages（异步 主/备） 2n messages（同步 主/备） 4n messages（2阶段提交，多-Paxos） 6n messages（3阶段提交，进行leader选举的Paxos） These algorithms vary in their fault tolerance (e.g. the types of faults they can tolerate). I’ve classified these simply by the number of messages exchanged during an execution of the algorithm, because I think it is interesting to try to find an answer to the question “what are we buying with the added message exchanges?” The diagram below, adapted from Ryan Barret at Google, describes some of the aspects of the different options: Ryan Barret对这些算法进行了一个不同层面的比较： The consistency, latency, throughput, data loss and failover characteristics in the diagram above can really be traced back to the two different replication methods: synchronous replication (e.g. waiting before responding) and asynchronous replication. When you wait, you get worse performance but stronger guarantees. The throughput difference between 2PC and quorum systems will become apparent when we discuss partition (and latency) tolerance. 上图中从一致性、延迟性、吞吐量、数据丢失、失败等特征通过对同步和异步复制过程来对这些算法进行了一个比较。当你处于等待状态时，系统性能不高但是有高保证。 In that diagram, algorithms enforcing weak (/eventual) consistency are lumped up into one category (“gossip”). However, I will discuss replication methods for weak consistency - gossip and (partial) quorum systems - in more detail. The “transactions” row really refers more to global predicate evaluation, which is not supported in systems with weak consistency (though local predicate evaluation can be supported). 在这个图中，算法保证低一致性（最终一致性）都集中在一种类型（gossip）上。这一节不讨论低一致性。（The “transactions” row really refers more to global predicate evaluation, which is not supported in systems with weak consistency (though local predicate evaluation can be supported谓词运算：True or False+运算操作） It is worth noting that systems enforcing weak consistency requirements have fewer generic algorithms, and more techniques that can be selectively applied. Since systems that do not enforce single-copy consistency are free to act like distributed systems consisting of multiple nodes, there are fewer obvious objectives to fix and the focus is more on giving people a way to reason about the characteristics of the system that they have. 弱一致性通用的算法很少，一般都是根据需求选择。当系统不再强制要求单拷贝一致性时，它会更加灵活，表现得更加像分布式系统。 For example: Client-centric consistency models attempt to provide more intelligible consistency guarantees while allowing for divergence. CRDTs (convergent and commutative replicated datatypes) exploit semilattice properties (associativity, commutativity, idempotency) of certain state and operation-based data types. Confluence analysis (as in the Bloom language) uses information regarding the monotonicity of computations to maximally exploit disorder. PBS (probabilistically bounded staleness) uses simulation and information collected from a real world system to characterize the expected behavior of partial quorum systems. I’ll talk about all of these a bit further on, first; let’s look at the replication algorithms that maintain single-copy consistency. 例如： 客户端为中心的一致性模型当发生数据不一致的时候，会尝试提供更易于理解的一致性保证 CRDTs（一致复制集数据类型）：一种拥有结合性、交换性、幂等性的操作数据类型 整合分析（如在Bloom语言中）使用关于计算的单调性的信息来最大限度地利用无序。 PBS（probabilistically bounded staleness）使用仿真和从真实世界系统收集的信息来刻画部分仲裁系统的预期行为。 后面会进行更深入的讨论。第四节主要关注单拷贝一致性的复制算法 Primary/backup replication 主备复制Primary/backup replication (also known as primary copy replication master-slave replication or log shipping) is perhaps the most commonly used replication method, and the most basic algorithm. All updates are performed on the primary, and a log of operations (or alternatively, changes) is shipped across the network to the backup replicas. There are two variants: asynchronous primary/backup replication and synchronous primary/backup replication 主备复制（主从复制、日志传输复制）是最常用的复制算法，也是最基本的复制算法。所有的更新都在主节点上进行，然后通过网络传送日志操作到其他节点副本中。这里有两种类型： 异步主从复制：只需一条信息（更新） 同步主从复制：需要两条信息（更新+确定收到） The synchronous version requires two messages (“update” + “acknowledge receipt”) while the asynchronous version could run with just one (“update”). P/B is very common. For example, by default MySQL replication uses the asynchronous variant. MongoDB also uses P/B (with some additional procedures for failover). All operations are performed on one master server, which serializes them to a local log, which is then replicated asynchronously to the backup servers. 主从复制例子：MySQL（异步）、MongoDB（有一些额外的故障备援方案）。所有的操作都在主服务器上进行，连载操作的日志信息然后异步更新到其它的备份节点中 As we discussed earlier in the context of asynchronous replication, any asynchronous replication algorithm can only provide weak durability guarantees. In MySQL replication this manifests as replication lag: the asynchronous backups are always at least one operation behind the primary. If the primary fails, then the updates that have not yet been sent to the backups are lost. 像之前讨论的异步复制中，任何异步复制的算法都只能保证弱持久性。在MySQL复制中它的表现形式为复制滞后：异步复制的节点备份通常在主节点操作之后，一旦主机发生故障，数据更新日志可能发生丢失 The synchronous variant of primary/backup replication ensures that writes have been stored on other nodes before returning back to the client - at the cost of waiting for responses from other replicas. However, it is worth noting that even this variant can only offer weak guarantees. Consider the following simple failure scenario: the primary receives a write and sends it to the backup the backup persists and ACKs the write and then primary fails before sending ACK to the client 主从复制的同步类型能够保证写操作在返回消息给客户端之前，每个节点上都会存储写信息，同时带来的副作用是需要等待复制的时间。但是尽管这种同步类型的主从复制系统提供的是弱保证，也没有什么影响。思考下面这些失败的场景案例： 主节点收到写操作，并且将这个命令发送给从节点 从节点收到并且对这个写操作做出命令正确应答的回应（ACKs） 主节点在发送ACK（命令正确应答）给客户端之前发生故障 The client now assumes that the commit failed, but the backup committed it; if the backup is promoted to primary, it will be incorrect. Manual cleanup may be needed to reconcile the failed primary or divergent backups. 这个时候客户端会猜测这次提交失败了，但是其实从节点已经正确提交了；如果备份节点提交了，但是主节点没有提交，即备份节点比主节点超前了，这样是错误的。这种矛盾需要进行协调 I am simplifying here of course. While all primary/backup replication algorithms follow the same general messaging pattern, they differ in their handling of failover, replicas being offline for extended periods and so on. However, it is not possible to be resilient to inopportune failures of the primary in this scheme. 之前提到过，系统都会有自己的失效备援操作，但是当主节点在这种场景下发生故障，系统是无法进行复原的 What is key in the log-shipping / primary/backup based schemes is that they can only offer a best-effort guarantee (e.g. they are susceptible to lost updates or incorrect updates if nodes fail at inopportune times). Furthermore, P/B schemes are susceptible to split-brain, where the failover to a backup kicks in due to a temporary network issue and causes both the primary and backup to be active at the same time. 主从复制中还有一个重要的关键点：日志传输。日志传输使得主从复制只能保证尽可能的正确，如果节点在不适宜的时间失败，可能会导致更新失败或数据丢失 To prevent inopportune failures from causing consistency guarantees to be violated; we need to add another round of messaging, which gets us the two phase commit protocol (2PC). 为了防止主从复制的不足，需要多加一轮消息传送认证，因此需要两个阶段提交协议（2PC） Two phase commit (2PC) 两阶段提交Two phase commit (2PC) is a protocol used in many classic relational databases. For example, MySQL Cluster (not to be confused with the regular MySQL) provides synchronous replication using 2PC. The diagram below illustrates the message flow: 两阶段提交（2PC）协议是关系数据库中最经典的一种协议。例如在MySQL集群中，就利用2PC协议来进行同步复制。下图中阐述了该复制过程中信息流的变化： 12345[ Coordinator ] -&gt; OK to commit? [ Peers ] &lt;- Yes / No[ Coordinator ] -&gt; Commit / Rollback [ Peers ] &lt;- ACK In the first phase (voting), the coordinator sends the update to all the participants. Each participant processes the update and votes whether to commit or abort. When voting to commit, the participants store the update onto a temporary area (the write-ahead log). Until the second phase completes, the update is considered temporary. 在第一个表决阶段（voting），协调者向所有的参与者发送更新消息。每一个参与者处理更新操作并表决是否提交或是终止，当表决结果是提交时，这些参与者实际上会将更新放在一个暂时区域（写操作日志头）中，这些更新始终都是暂时的，直到第二个阶段完成 In the second phase (decision), the coordinator decides the outcome and informs every participant about it. If all participants voted to commit, then the update is taken from the temporary area and made permanent. 在第二个决策阶段（decision），协调者决定最后的结果，并且告知所有的参与者。如果所有的参与者第一阶段的表决都是提交，那么更新操作才会进行持久化。 Having a second phase in place before the commit is considered permanent is useful, because it allows the system to roll back an update when a node fails. In contrast, in primary/backup (“1PC”), there is no step for rolling back an operation that has failed on some nodes and succeeded on others, and hence the replicas could diverge. 在提交被认为是永久的之前，第二个阶段是非常有用的，因为它允许系统在节点失败时回滚更新。相反，在主/备份（“1PC”）中，没有回滚步骤，若发生在某些节点上失败而在其他节点上成功的情况，副本可能会出现分歧。 2PC is prone to blocking, since a single node failure (participant or coordinator) blocks progress until the node has recovered. Recovery is often possible thanks to the second phase, during which other nodes are informed about the system state. Note that 2PC assumes that the data in stable storage at each node is never lost and that no node crashes forever. Data loss is still possible if the data in the stable storage is corrupted in a crash. 2PC容易造成拥塞，因为一旦一个节点失败，无论是协调者或是参与者，都会导致进程拥堵，直到节点修复。修复过程一般需要依靠第二个阶段中其它节点被告知系统状态。2PC是假设各个节点中的数据处于一个稳定的存储状态中，数据永远不会丢失并且不会发生节点崩溃。但是数据就算存储稳定，仍然会发生丢失如果发生崩溃的现象 The details of the recovery procedures during node failures are quite complicated so I won’t get into the specifics. The major tasks are ensuring that writes to disk are durable (e.g. flushed to disk rather than cached) and making sure that the right recovery decisions are made (e.g. learning the outcome of the round and then redoing or undoing an update locally). 节点故障期间恢复过程非常复杂，因此我不想详细介绍。主要任务是确保对磁盘的写入是持久的（例如，刷新到磁盘而不是缓存），并确保做出正确的恢复决策（例如，得到回合的结果，然后在本地重做或撤消更新） As we learned in the chapter regarding CAP, 2PC is a CA - it is not partition tolerant. The failure model that 2PC addresses does not include network partitions; the prescribed way to recover from a node failure is to wait until the network partition heals. There is no safe way to promote a new coordinator if one fails; rather a manual intervention is required. 2PC is also fairly latency-sensitive, since it is a write N-of-N approach in which writes cannot proceed until the slowest node acknowledges them. 正如我们在介绍CAP的章节中了解到的，2PC是一个CA算法，它没有分区容忍。2PC的故障模型中不包括网络分区；从节点故障恢复的指定方法是等待网络分区恢复。如果一个协调员失败了，就没有安全的方法来生成一个新的协调员；相反，需要人工干预。2PC还相当容易延迟，因为它是一种N对N的写方法，在最慢的节点确认之前，写操作无法继续。 2PC strikes a decent balance between performance and fault tolerance, which is why it has been popular in relational databases. However, newer systems often use a partition tolerant consensus algorithm, since such an algorithm can provide automatic recovery from temporary network partitions as well as more graceful handling of increased between-node latency. 2PC在性能和容错之间取得了相当好的平衡，这就是它在关系数据库中流行的原因。（因为失败了有回滚吗？？？）但是，较新的系统通常使用允许分区的一致性算法，因为这样的算法可以自动恢复由于临时网络分区造成的故障，以及能够对增加的节点间延迟时间进行更好的处理 Let’s look at partition tolerant consensus algorithms next. 接着来看分区容忍共识算法 Partition tolerant consensus algorithms 分区容忍一致性算法Partition tolerant consensus algorithms are as far as we’re going to go in terms of fault-tolerant algorithms that maintain single-copy consistency. There is a further class of fault tolerant algorithms: algorithms that tolerate arbitrary (Byzantine) faults; these include nodes that fail by acting maliciously. Such algorithms are rarely used in commercial systems, because they are more expensive to run and more complicated to implement - and hence I will leave them out. 在保证单拷贝一致性中，考虑分区容忍一致性算法是我们将要讨论的内容。还有一类容错算法：允许任意（拜占庭式）错误的算法；这些算法来处理恶意操作导致节点失败的情况。这种算法很少在商业系统中使用，因为它们运行起来更昂贵，实现起来也更复杂——因此我将把它们排除在外。 When it comes to partition tolerant consensus algorithms, the most well-known algorithm is the Paxos algorithm. It is, however, notoriously difficult to implement and explain, so I will focus on Raft, a recent (~early 2013) algorithm designed to be easier to teach and implement. Let’s first take a look at network partitions and the general characteristics of partition tolerant consensus algorithms. 涉及到分区容忍一致性算法时，最著名的算法是Paxos算法。然而，众所周知，它很难实现和解释，所以我将重点关注Raft，这是一种最近（2013年初）的算法，旨在更易于教学和实现。让我们首先看一下网络分区和允许分区的共识算法的一般特性。 What is a network partition?什么是网络分区？A network partition is the failure of a network link to one or several nodes. The nodes themselves continue to stay active, and they may even be able to receive requests from clients on their side of the network partition. As we learned earlier - during the discussion of the CAP theorem - network partitions do occur and not all systems handle them gracefully. 网络分区是指到一个或多个节点之间的网络链接失败。节点本身继续保持活动状态，甚至可以从网络分区的客户端接收请求。正如我们之前在讨论CAP定理中讨论到的，当网络分区发生后，并不是所有系统都能很好地处理它们。 Network partitions are tricky because during a network partition, it is not possible to distinguish between a failed remote node and the node being unreachable. If a network partition occurs but no nodes fail, then the system is divided into two partitions which are simultaneously active. The two diagrams below illustrate how a network partition can look similar to a node failure. 网络分区很棘手，因为在网络分区期间，无法区分远程节点是发生了故障还是无法访问。如果一个网络分区出现但没有节点失败，那么系统将被划分为两个同时处于活动状态的分区。下面的两个图说明了网络分区看起来如何类似于节点故障。 A system of 2 nodes, with a failure vs. a network partition: 下图展示了由两个节点组成的系统。网络分区VS存在故障： A system that enforces single-copy consistency must have some method to break symmetry: otherwise, it will split into two separate systems, which can diverge from each other and can no longer maintain the illusion of a single copy. 一个强制实现单一拷贝一致性的系统必须有某种方法来打破对称性：否则，它将分裂成两个独立的系统，这两个系统可以彼此分离，并且不能再保持单一拷贝的假象。 Network partition tolerance for systems that enforce single-copy consistency requires that during a network partition, only one partition of the system remains active since during a network partition it is not possible to prevent divergence (e.g. CAP theorem). 对于强制执行单一拷贝一致性的系统，网络分区容忍要求在网络分区期间，只有一个系统分区保持活跃状态，因为在网络分区不能保证数据不发生分歧（CAP定理） Majority decisions 主要决策This is why partition tolerant consensus algorithms rely on a majority vote. Requiring a majority of nodes - rather than all of the nodes (as in 2PC) - to agree on updates allows a minority of the nodes to be down, or slow, or unreachable due to a network partition. As long as (N/2 + 1)-of-N nodes are up and accessible, the system can continue to operate. 分区容忍一致性算法需要多数的投票 不同于2PC算法需要所有的节点都达到一致，这里只需要大多数节点能够同意更新并且容许少部分节点宕机、慢或者由于网络分区而无法接触，这样系统仍然能够继续运行 Partition tolerant consensus algorithms use an odd number of nodes (e.g. 3, 5 or 7). With just two nodes, it is not possible to have a clear majority after a failure. For example, if the number of nodes is three, then the system is resilient to one node failure; with five nodes the system is resilient to two node failures. 分区容忍一致性算法使用的是奇数节点（类似于3，5，7） 如果使用2个节点的话，那么就难以对多数投票中的多数进行定义。举例说明：如果节点数量是3个，那么系统能够允许一个节点失败；如果节点数是5个，系统能够允许2个节点失败 When a network partition occurs, the partitions behave asymmetrically. One partition will contain the majority of the nodes. Minority partitions will stop processing operations to prevent divergence during a network partition, but the majority partition can remain active. This ensures that only a single copy of the system state remains active. 当发生网络分区时，各个区的表现是不一致的。一个区可能包含大多数节点。小部分区会停止程序运行操作来阻止网络分区带来的数据分歧，但多数区仍然保持活跃状态。这能保证系统中只有一个单副本是活跃的 Majorities are also useful because they can tolerate disagreement: if there is a perturbation or failure, the nodes may vote differently. However, since there can be only one majority decision, a temporary disagreement can at most block the protocol from proceeding (giving up liveness) but it cannot violate the single-copy consistency criterion (safety property). 因为系统能够允许存在不一致，使得多数节点仍然是可用的：如果有变动和失败发生，节点可能投票不一致。但由于只可能有一个多数决策存在，暂时的分歧最多可以阻止协议继续进行（放弃活跃性），但不能违反单一副本一致性标准（安全属性） Roles 角色There are two ways one might structure a system: all nodes may have the same responsibilities, or nodes may have separate, distinct roles. 构造一个系统，系统中所有的节点要么含有相同的目标，要么有明确的分工和扮演的角色 Consensus algorithms for replication generally opt for having distinct roles for each node. Having a single fixed leader or master server is an optimization that makes the system more efficient, since we know that all updates must pass through that server. Nodes that are not the leader just need to forward their requests to the leader. 复制的一致性算法通常会对每个节点分配不同的角色。一个系统中有一个固定的领导节点或者主服务能够使这个系统更加高效，所有的更新操作都要经过这个主服务。不是领导的节点只需要将它们的请求附送给领导节点 Note that having distinct roles does not preclude the system from recovering from the failure of the leader (or any other role). Just because roles are fixed during normal operation doesn’t mean that one cannot recover from failure by reassigning the roles after a failure (e.g. via a leader election phase). Nodes can reuse the result of a leader election until node failures and/or network partitions occur. 注意，明确的角色不代表系统中所有的节点一直扮演一个类型的角色。如果系统从节点失败中恢复过来，那么节点的角色可能会被从新分配，这个时候会存在一个领导节点选举的阶段。节点扮演它们的角色，到节点发生失败或者网络发生分区的时候，角色将又重新定义。 Both Paxos and Raft make use of distinct node roles. In particular, they have a leader node (“proposer” in Paxos) that is responsible for coordination during normal operation. During normal operation, the rest of the nodes are followers (“acceptors” or “voters” in Paxos). 无论是Paxos算法还是Raft算法，它们都有明确的节点分工。具体而言，它们都有一个领导节点leader node（在Paxos中被称为proposer），负责正常系统运行操作中的协调作用。在正常运行中，剩下的节点是跟随节点follows（Paxos中称为acceptors或者voters） Epochs 训练周期Each period of normal operation in both Paxos and Raft is called an epoch (“term” in Raft). During each epoch only one node is the designated leader (a similar system is used in Japan where era names change upon imperial succession). 无论是Paxos还是Raft算法中，正常运行的每一个阶段都被称为一个训练周期（Raft中被称为‘term’）。在每一个周期中，只有一个节点被标记为领导节点(Leader) After a successful election, the same leader coordinates until the end of the epoch. As shown in the diagram above (from the Raft paper), some elections may fail, causing the epoch to end immediately. 成功选举后，在这个周期中领导节点不变，直到下一次选举发生。如图所示，可能会存在选举失败的情况，这个时候会导致这个周期很快结束 Epochs act as a logical clock, allowing other nodes to identify when an outdated node starts communicating - nodes that were partitioned or out of operation will have a smaller epoch number than the current one, and their commands are ignored. 周期纪元表现形式想逻辑锁一样，它允许其它节点识别出一个过期节点何时开始通信-分区或不运行的节点相比于正常的节点，周期会落后，能被识别出来并且它们的需求就直接被忽略 Leader changes via duels 领导节点变化During normal operation, a partition-tolerant consensus algorithm is rather simple. As we’ve seen earlier, if we didn’t care about fault tolerance, we could just use 2PC. Most of the complexity really arises from ensuring that once a consensus decision has been made, it will not be lost and the protocol can handle leader changes as a result of a network or node failure. 在正常运行操作中，分区容忍一致性算法是很简单的。正如我们之前所说，如果我们不关心容错性，我们可以直接使用2PC算法。大部分情况下，算法复杂的原因就是因为我们需要保证一旦做出了一个一致性决策，它不能被丢失，并且有协议能够处理由于网络或节点崩溃带来的领导节点变化的情况 All nodes start as followers; one node is elected to be a leader at the start. During normal operation, the leader maintains a heartbeat which allows the followers to detect if the leader fails or becomes partitioned. 首先，刚开始的时候所有节点都是followers跟随者，然后其中一个节点被选举为leader领导者。在一个正常运行阶段，这个领导节点一直有心跳，允许其它跟随节点检测到它是否失败或者是被分区了 When a node detects that a leader has become non-responsive (or, in the initial case, that no leader exists), it switches to an intermediate state (called “candidate” in Raft) where it increments the term/epoch value by one, initiates a leader election and competes to become the new leader. 当一个节点检测到领导节点变得不响应时（或者，在最初的情况下，没有领导者存在），它会切换到一个中间状态（在raft中称为“候选人”），在这个状态中，它将周期纪元值增加一个，启动一个领导者选举，并竞争成为新的领导者 In order to be elected a leader, a node must receive a majority of the votes. One way to assign votes is to simply assign them on a first-come-first-served basis; this way, a leader will eventually be elected. Adding a random amount of waiting time between attempts at getting elected will reduce the number of nodes that are simultaneously attempting to get elected. 节点在投票过程中必须要获得多数投票才能成为一个领导节点。分配选票的最简单的一种方法是按先到先得的原则进行；这样，最终将选出一位领导节点。在选举过程中之间添加随机的等待时间将减少同时尝试当选的节点数 Numbered proposals within an epoch 一个周期中的编号提案During each epoch, the leader proposes one value at a time to be voted upon. Within each epoch, each proposal is numbered with a unique strictly increasing number. The followers (voters / acceptors) accept the first proposal they receive for a particular proposal number. 在每一个周期纪元中，领导节点会依次提出一个值，用来供以表决。在每个周期中，每个提案都有一个唯一的严格递增的数字编号。追随者（投票者/接受者）接受他们收到的针对特定提案编号的第一个提案 Normal operation 正常运行During normal operation, all proposals go through the leader node. When a client submits a proposal (e.g. an update operation), the leader contacts all nodes in the quorum. If no competing proposals exist (based on the responses from the followers), the leader proposes the value. If a majority of the followers accept the value, then the value is considered to be accepted. 在正常运行中，所有的提案会经过领导节点。当一个客户端提交一个提案（例如：一个更新操作），这个领导节点会联系仲裁中的所有的节点。如果没有竞争提案（基于追随者的响应），领导节点会提出一个值。如果大多数追随者接受这个值，那么这个值就被认为是被接受的 Since it is possible that another node is also attempting to act as a leader, we need to ensure that once a single proposal has been accepted, its value can never change. Otherwise a proposal that has already been accepted might for example be reverted by a competing leader. Lamport states this as: P2: If a proposal with value v is chosen, then every higher-numbered proposal that is chosen has value v. 由于另一个节点也可能试图充当领导者，因此我们需要确保一旦接受了一个建议，它的值就永远不会改变。否则，一个已经被接受的提议可能会被竞争领导的节点回复。Lamport描述如下： P2: 如果一个提议的值一旦被确定为v，任何更高编号的提议也会选择这个值 Ensuring that this property holds requires that both followers and proposers are constrained by the algorithm from ever changing a value that has been accepted by a majority. Note that “the value can never change” refers to the value of a single execution (or run / instance / decision) of the protocol. A typical replication algorithm will run multiple executions of the algorithm, but most discussions of the algorithm focus on a single run to keep things simple. We want to prevent the decision history from being altered or overwritten. 确保追随者和提议者都不能改变已被大多数人接受的值。这里“值永远不能更改”是指协议的单个执行（或运行/实例/决策）的值。一个典型的复制算法将运行该算法的多个执行，但大多数关于该算法的讨论集中在一次运行上。我们希望防止更改或覆盖决策历史记录。 In order to enforce this property, the proposers must first ask the followers for their (highest numbered) accepted proposal and value. If the proposer finds out that a proposal already exists, then it must simply complete this execution of the protocol, rather than making its own proposal. Lamport states this as: P2b. If a proposal with value v is chosen, then every higher-numbered proposal issued by any proposer has value v. 为了强制执行此属性，提案人必须首先向追随者询问他们（编号最高）接受的提案和值。如果提案人发现提案已经存在，那么它必须简单地完成协议的执行，而不是自己提出提案。Lamport描述如下： P2b. 如果一个提议的值一旦被确定为v，任何提议人发布的任何更高编号的提议都具有值v More specifically: P2c. For any v and n, if a proposal with value v and number n is issued [by a leader], then there is a set S consisting of a majority of acceptors [followers] such that either (a) no acceptor in S has accepted any proposal numbered less than n, or (b) v is the value of the highest-numbered proposal among all proposals numbered less than n accepted by the followers in S. 更具体地说： P2c. 对于任何v和n，如果一个值为v编号为n的提案被[领导节点]发布，那么存在一个由大多数接受者[追随者]组成的集合S，其中，没有接受者接受编号比n小的提案，v是S中所有编号小于n的提案中编号最大的提案的值 This is the core of the Paxos algorithm, as well as algorithms derived from it. The value to be proposed is not chosen until the second phase of the protocol. Proposers must sometimes simply retransmit a previously made decision to ensure safety (e.g. clause b in P2c) until they reach a point where they know that they are free to impose their own proposal value (e.g. clause a). 这是paxos算法的核心，也是从中派生出来的算法的核心。在协议的第二阶段之前，不会对建议的值进行选择。提案人有时必须简单地重新传输先前做出的决定，以确保安全（例如P2c中的条款b），直到他们知道自己可以自由地强加自己的提案值（例如条款a） If multiple previous proposals exist, then the highest-numbered proposal value is proposed. Proposers may only attempt to impose their own value if there are no competing proposals at all. 如果存在多个以前的建议，则建议使用编号最高的建议值。只有在完全没有竞争性提案的情况下，提案人才能试图强加自己的值 To ensure that no competing proposals emerge between the time the proposer asks each acceptor about its most recent value, the proposer asks the followers not to accept proposals with lower proposal numbers than the current one. 为了确保在提案人向每个接受人询问其最新值时不会出现竞争性提案，提案人要求跟随者不要接受提案编号低于当前提案编号的提案 Putting the pieces together, reaching a decision using Paxos requires two rounds of communication: 把这些部分放在一起，使用Paxos算法做出决定需要两轮通信： 123456789[ Proposer ] -&gt; Prepare(n) [ Followers ] &lt;- Promise(n; previous proposal number and previous value if accepted a proposal in the past)[ Proposer ] -&gt; AcceptRequest(n, own value or the value [ Followers ] associated with the highest proposal number reported by the followers) &lt;- Accepted(n, value) The prepare stage allows the proposer to learn of any competing or previous proposals. The second phase is where either a new value or a previously accepted value is proposed. In some cases - such as if two proposers are active at the same time (dueling); if messages are lost; or if a majority of the nodes have failed - then no proposal is accepted by a majority. But this is acceptable, since the decision rule for what value to propose converges towards a single value (the one with the highest proposal number in the previous attempt). 准备阶段允许提案人了解任何竞争或以前的提案。第二阶段是提出新值或先前接受的值。在某些情况下，例如，如果两个提议者同时处于活动状态（决斗）；如果消息丢失；或者如果大多数节点都失败，则多数人不会接受任何提议。但这是可以接受的，因为建议的值的决策规则收敛到一个值（上一次尝试中建议数最高的值）。 Indeed, according to the FLP impossibility result, this is the best we can do: algorithms that solve the consensus problem must either give up safety or liveness when the guarantees regarding bounds on message delivery do not hold. Paxos gives up liveness: it may have to delay decisions indefinitely until a point in time where there are no competing leaders, and a majority of nodes accept a proposal. This is preferable to violating the safety guarantees. 事实上，根据FLP不可能的结果，这是我们所能做的最好的：解决共识问题的算法必须在消息传递边界的保证不成立时放弃安全性或活跃性。Paxos放弃了活跃性：它可能不得不无限期地推迟决策，直到某个时间点没有竞争的领导者，并且大多数节点都接受了一个提议。这比违反安全保证更可取。 Of course, implementing this algorithm is much harder than it sounds. There are many small concerns which add up to a fairly significant amount of code even in the hands of experts. These are issues such as: practical optimizations: avoiding repeated leader election via leadership leases (rather than heartbeats) avoiding repeated propose messages when in a stable state where the leader identity does not change ensuring that followers and proposers do not lose items in stable storage and that results stored in stable storage are not subtly corrupted (e.g. disk corruption) enabling cluster membership to change in a safe manner (e.g. base Paxos depends on the fact that majorities always intersect in one node, which does not hold if the membership can change arbitrarily) procedures for bringing a new replica up to date in a safe and efficient manner after a crash, disk loss or when a new node is provisioned procedures for snapshotting and garbage collecting the data required to guarantee safety after some reasonable period (e.g. balancing storage requirements and fault tolerance requirements) Google’s Paxos Made Live paper details some of these challenges. 当然，实现这个算法要比听起来困难得多。有许多小问题，即使是在专家的手中，加起来也相当可观的代码量。这些问题包括： 实际优化： – 通过领导权租赁（而不是心跳）避免重复的领导选举 – 避免在领导节点身份不变的稳定状态下重复建议消息 确保追随者和提议者不会丢失稳定存储中的项目，并且存储在稳定存储中的结果不会被损坏（例如磁盘损坏）。 允许集群成员以安全的方式进行更改（例如，基本Paxos依赖于一个事实：大多数成员总是在一个节点中相交，如果成员可以任意更改，那么这个节点就不起作用） 在崩溃、磁盘丢失或配置新节点后以安全有效的方式使新副本更新的过程 快照和垃圾收集程序：在一段合理的时间后，为确保安全所需的数据（例如，平衡存储要求和容错要求） 谷歌的Paxos制作了实况文件，详细介绍了其中的一些挑战 Partition-tolerant consensus algorithms: Paxos, Raft, ZAB分区容忍一致性算法：Paxos、Raft、ZABHopefully, this has given you a sense of how a partition-tolerant consensus algorithm works. I encourage you to read one of the papers in the further reading section to get a grasp of the specifics of the different algorithms. 这一节想让您了解了一个允许分区的共识算法是如何工作的。鼓励大家阅读下一阅读部分中的文章，以了解不同算法的具体情况。 Paxos. Paxos is one of the most important algorithms when writing strongly consistent partition tolerant replicated systems. It is used in many of Google’s systems, including the Chubby lock manager used by BigTable/Megastore, the Google File System as well as Spanner. Paxos-是在编写强一致性的容错分区复制系统时最重要的算法之一。它被用于谷歌的许多系统，包括Bigtable/MegaStore使用的丰满版锁管理器、谷歌文件系统以及Spanner Paxos is named after the Greek island of Paxos, and was originally presented by Leslie Lamport in a paper called “The Part-Time Parliament” in 1998. It is often considered to be difficult to implement, and there have been a series of papers from companies with considerable distributed systems expertise explaining further practical details (see the further reading). You might want to read Lamport’s commentary on this issue here and here. Paxos以希腊的帕克斯岛命名，最初由莱斯利·兰波特于1998年在一份名为“兼职议会”的论文中提出。它通常被认为是很难实现的，具有相当多分布式系统专业知识的公司的一系列论文对其具体实施的实际细节进行了进一步的解释（请参阅进一步阅读） The issues mostly relate to the fact that Paxos is described in terms of a single round of consensus decision making, but an actual working implementation usually wants to run multiple rounds of consensus efficiently. This has led to the development of many extensions on the core protocol that anyone interested in building a Paxos-based system still needs to digest. Furthermore, there are additional practical challenges such as how to facilitate cluster membership change. 我们使用一轮共识决策来描述Paxos算法，但是实际的工作执行通常希望高效地运行多轮共识。这导致了核心协议上的许多扩展的开发，任何对构建基于Paxos的系统感兴趣的人仍然需要消化这些扩展。此外，还有一些额外的实际挑战，例如如何促进集群成员资格的改变。 ZAB. ZAB - the Zookeeper Atomic Broadcast protocol is used in Apache Zookeeper. Zookeeper is a system which provides coordination primitives for distributed systems, and is used by many Hadoop-centric distributed systems for coordination (e.g. HBase, Storm, Kafka). Zookeeper is basically the open source community’s version of Chubby. Technically speaking atomic broadcast is a problem different from pure consensus, but it still falls under the category of partition tolerant algorithms that ensure strong consistency. ZAB-在Apache ZooKeeper中使用了ZooKeeper原子广播协议。ZooKeeper是为分布式系统提供协调原语的系统，许多以Hadoop为中心的分布式系统（如HBase、Storm、Kafka）都使用它进行协调。ZooKeeper基本上是开源社区的丰满版本。从技术上讲，原子广播是一个不同于纯一致共识的问题，但它仍然属于保证强一致性的分区容忍算法范畴。 Raft. Raft is a recent (2013) addition to this family of algorithms. It is designed to be easier to teach than Paxos, while providing the same guarantees. In particular, the different parts of the algorithm are more clearly separated and the paper also describes a mechanism for cluster membership change. It has recently seen adoption in etcd inspired by ZooKeeper. Raft-Raft是最近（2013年）对该算法系列的一个补充。它被设计成比Paxos更容易教学，同时提供相同的保证。特别是算法的不同部分之间的分离更加清晰，本文还描述了一种集群成员关系变化的机制。它最近在受ZooKeeper启发的etcd中被采用。 Replication methods with strong consistency 强一致性的复制方法In this chapter, we took a look at replication methods that enforce strong consistency. Starting with a contrast between synchronous work and asynchronous work, we worked our way up to algorithms that are tolerant of increasingly complex failures. Here are some of the key characteristics of each of the algorithms: 在本章中，我们介绍强制实现强一致性的复制方法。从同步工作和异步工作之间的对比开始，我们了解能够容忍日益复杂的故障的算法。以下是每种算法的一些关键特性： Primary/Backup Single, static master Replicated log, slaves are not involved in executing operations No bounds on replication delay Not partition tolerant Manual/ad-hoc failover, not fault tolerant, “hot backup” 主/备算法 单，静态主机 复制的日志，从服务器不参与执行操作 复制延迟没有限制 不允许分区 手动/故障转移，不容错，“热备份” 2PC Unanimous vote: commit or abort Static master 2PC cannot survive simultaneous failure of the coordinator and a node during a commit Not partition tolerant, tail latency sensitive 2PC 一致表决：同意或放弃 静态主机 2PC在提交过程中无法承受协调器和节点的同时失败。 不允许分区，尾延迟敏感 Paxos Majority vote Dynamic master Robust to n/2-1 simultaneous failures as part of protocol Less sensitive to tail latency Paxos 多数投票机制 动态主机 作为协议的一部分，对N/2-1同时故障具有鲁棒性 对尾延迟不太敏感 Further readingPrimary-backup and 2PC Replication techniques for availability - Robbert van Renesse &amp; Rachid Guerraoui, 2010 Concurrency Control and Recovery in Database Systems Paxos The Part-Time Parliament - Leslie Lamport Paxos Made Simple - Leslie Lamport, 2001 Paxos Made Live - An Engineering Perspective - Chandra et al Paxos Made Practical - Mazieres, 2007 Revisiting the Paxos Algorithm - Lynch et al How to build a highly available system with consensus - Butler Lampson Reconfiguring a State Machine - Lamport et al - changing cluster membership Implementing Fault-Tolerant Services Using the State Machine Approach: a Tutorial - Fred Schneider Raft and ZAB In Search of an Understandable Consensus Algorithm, Diego Ongaro, John Ousterhout, 2013 Raft Lecture - User Study A simple totally ordered broadcast protocol - Junqueira, Reed, 2008 ZooKeeper Atomic Broadcast - Reed, 2011]]></content>
      <categories>
        <category>微服务理论文章阅读学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]Distributed systems for fun and profit_3时间和顺序]]></title>
    <url>%2F2019%2F04%2F23%2F%5B%E8%AF%91%5DDistributed%20systems%20for%20fun%20and%20profit_3%E6%97%B6%E9%97%B4%E5%92%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[三.Time and order 时间和顺序What is order and why is it important? What do you mean “what is order”? I mean, why are we so obsessed with order in the first place? Why do we care whether A happened before B? Why don’t we care about some other property, like “color”? Well, my crazy friend, let’s go back to the definition of distributed systems to answer that. As you may remember, I described distributed programming as the art of solving the same problem that you can solve on a single computer using multiple computers. This is, in fact, at the core of the obsession with order. Any system that can only do one thing at a time will create a total order of operations. Like people passing through a single door, every operation will have a well-defined predecessor and successor. That’s basically the programming model that we’ve worked very hard to preserve. 分布式系统的目标是能够让多台机器像单机一样处理问题。在这里面需要关注的核心是顺序。任何系统在同一时刻都只能做一件事情，这样就会产生一系列有顺序的操作。 The traditional model is: a single program, one process, one memory space running on one CPU. The operating system abstracts away the fact that there might be multiple CPUs and multiple programs, and that the memory on the computer is actually shared among many programs. I’m not saying that threaded programming and event-oriented programming don’t exist; it’s just that they are special abstractions on top of the “one/one/one” model. Programs are written to be executed in an ordered fashion: you start from the top, and then go down towards the bottom. 传统的模型是：单一程序利用一个CPU运行在一个进程和一个内存空间中。而分布式模型通常处理多个CPU和多个程序以及程序之间的内存通常需要共享。这个时候即使程序运行在多台机器之间，我们希望它也能像单机一样，同样顺序执行程序 Order as a property has received so much attention because the easiest way to define “correctness” is to say “it works like it would on a single machine”. And that usually means that a) we run the same operations and b) that we run them in the same order - even if there are multiple machines. The nice thing about distributed systems that preserve order (as defined for a single system) is that they are generic. You don’t need to care about what the operations are, because they will be executed exactly like on a single machine. This is great because you know that you can use the same system no matter what the operations are. In reality, a distributed program runs on multiple nodes; with multiple CPUs and multiple streams of operations coming in. You can still assign a total order, but it requires either accurate clocks or some form of communication. You could timestamp each operation using a completely accurate clock then use that to figure out the total order. Or you might have some kind of communication system that makes it possible to assign sequential numbers as in a total order. 分布式系统的好处在于一个定义好的顺序是通用的。你不需要担心它是如何操作的，无论你进行什么样的操作，你都可以使用这个系统。 分布式系统程序运行在多个节点之间。你需要分配一个全局顺序，需要精确的时钟与一些通信方式。你可以对每一个操作贴上一个时间戳，来保证它们全局有序，或者也可以采用某种通信方式（例如计数器），来确保它们全局有序 操作顺序包括：全局有序、偏序 Total and partial order 全局有序和偏序The natural state in a distributed system is partial order. Neither the network nor independent nodes make any guarantees about relative order; but at each node, you can observe a local order. A total order is a binary relation that defines an order for every element in some set. 全局有序：数据集中每一个元素顺序的一种二进制关系 偏序：分布式系统中最自然的状态就是偏序。独立节点与网络不能保证相关顺序，但在每个节点中，自身有本地顺序 Two distinct elements are comparable when one of them is greater than the other. In a partially ordered set, some pairs of elements are not comparable and hence a partial order doesn’t specify the exact order of every item. 两个明确的元素之间能够进行相互大小比较，但是在一个分区有序数据集中，不同区的元素之间是没法进行比较的，因为它们只在自己的分区中有序 Both total order and partial order are transitive and antisymmetric. The following statements hold in both a total order and a partial order for all a, b and c in X: 无论是全局有序还是偏序，都遵从传递性（transitive）和反对称性（antisymmetric）。下面的描述表达了顺序具有的性质： 12If a ≤ b and b ≤ a then a = b (antisymmetry);If a ≤ b and b ≤ c then a ≤ c (transitivity); However, a total order is total: 1a ≤ b or b ≤ a (totality) for all a, b in X while a partial order is only reflexive: 1a ≤ a (reflexivity) for all a in X Note that totality implies reflexivity; so a partial order is a weaker variant of total order.For some elements in a partial order, the totality property does not hold - in other words, some of the elements are not comparable. 偏序（分区有序？可以这样理解吗）的性质比全局顺序的性质要弱，因为有些元素他们是没法进行比较的 Git branches are an example of a partial order. As you probably know, the git revision control system allows you to create multiple branches from a single base branch - e.g. from a master branch. Each branch represents a history of source code changes derived based on a common ancestor: Git分支就是偏序的一个实例。我们都知道git的版本控制能够让你从一个单一的基本分支中创造出多个分支。例如，从主分支中进行后续创建。每一个分支代表原始代码从最初到后面的变化历程： 123[ branch A (1,2,0)] [ master (3,0,0) ] [ branch B (1,0,2) ][ branch A (1,1,0)] [ master (2,0,0) ] [ branch B (1,0,1) ] \ [ master (1,0,0) ] / The branches A and B were derived from a common ancestor, but there is no definite order between them: they represent different histories and cannot be reduced to a single linear history without additional work (merging). You could, of course, put all the commits in some arbitrary order (say, sorting them first by ancestry and then breaking ties by sorting A before B or B before A) - but that would lose information by forcing a total order where none existed. 这里分支A和B都来自于同一个祖先，但是它们两者之间的顺序是没有定义的：两者表示的是不同的历史版本，并且如若不经过其它额外操作，类似于merging，是无法将它们两者归到同一线性改变的版本中。当然你可以自己进行一些操作，定义提交的顺序，但如果自定义AB之间的顺序，会强制出现一个本来就不存在的total order In a system consisting of one node, a total order emerges by necessity: instructions are executed and messages are processed in a specific, observable order in a single program. We’ve come to rely on this total order - it makes executions of programs predictable. This order can be maintained on a distributed system, but at a cost: communication is expensive, and time synchronization is difficult and fragile. 在一个有单一节点构成的系统中，全局有序是必要的，这使得程序的执行结果具有可预测性。这样的顺序也能在分布式系统中维持，但昂贵的通信成本以及时间同步的困难和脆弱性使得其代价十分昂贵 What is time? 时间Time is a source of order - it allows us to define the order of operations - which coincidentally also has an interpretation that people can understand (a second, a minute, a day and so on). 没有时间就没有顺序。时间能让我们确定操作的顺序，同时也能被更好的理解 In some sense, time is just like any other integer counter. It just happens to be important enough that most computers have a dedicated time sensor, also known as a clock. It’s so important that we’ve figured out how to synthesize an approximation of the same counter using some imperfect physical system (from wax candles to cesium atoms). By “synthesize”, I mean that we can approximate the value of the integer counter in physically distant places via some physical property without communicating it directly. 从某种角度而言，时间像计数器一样，能够让大多数的运算有自己专用的时间传感器。对于同步系统而言，我们能有一个不用互相通信就存在的精确的计数器 Timestamps really are a shorthand value for representing the state of the world from the start of the universe to the current moment - if something occurred at a particular timestamp, then it was potentially influenced by everything that happened before it. This idea can be generalized into a causal clock that explicitly tracks causes (dependencies) rather than simply assuming that everything that preceded a timestamp was relevant. Of course, the usual assumption is that we should only worry about the state of the specific system rather than the whole world. 时间戳是表达事物所处状态的一个简单的速记值。如果一件事发生在某一时刻，那么它受到发生在它之前的事件的影响。这个想法可以概括为一个因果时钟，它明确地跟踪原因（依赖性），而不是简单地假设时间戳之前的所有内容都是相关的。当然，通常的假设是，我们只应该担心特定系统的状态，而不是整个世界。 Assuming that time progresses at the same rate everywhere - and that is a big assumption which I’ll return to in a moment - time and timestamps have several useful interpretations when used in a program. The three interpretations are: Order Duration Interpretation 假设时间在任何地方都以相同的速度进行——这是一个很大的假设，我稍后将回到这个假设——时间和时间戳在程序中使用时有几个有用的解释。这三种解释是： 顺序 持续时间 表现形式 Order. When I say that time is a source of order, what I mean is that: we can attach timestamps to unordered events to order them we can use timestamps to enforce a specific ordering of operations or the delivery of messages (for example, by delaying an operation if it arrives out of order) we can use the value of a timestamp to determine whether something happened chronologically before something else 顺序：一系列事件发生的顺序 我们能通过事件发生的时间戳来确定事件发生的顺序 我们能够利用时间戳来定义一系列操作的顺序，或者传递信息（例如，如果一个操作发生故障，则延迟它） 通过时间戳能够确定一件事是否发生在另一件事前 Interpretation - time as a universally comparable value. The absolute value of a timestamp can be interpreted as a date, which is useful for people. Given a timestamp of when a downtime started from a log file, you can tell that it was last Saturday, when there was a thunderstorm. 表现形式：时间是一个可以进行全局比较的值。时间的表现形式可以有多种，比如日期、星期几等等 Duration - durations measured in time have some relation to the real world. Algorithms generally don’t care about the absolute value of a clock or its interpretation as a date, but they might use durations to make some judgment calls. In particular, the amount of time spent waiting can provide clues about whether a system is partitioned or merely experiencing high latency. 持续时间：通过时间段长短可以来判断系统是发生分区还是发生了高延迟 By their nature, the components of distributed systems do not behave in a predictable manner. They do not guarantee any specific order, rate of advance, or lack of delay. Each node does have some local order - as execution is (roughly) sequential - but these local orders are independent of each other. Imposing (or assuming) order is one way to reduce the space of possible executions and possible occurrences. Humans have a hard time reasoning about things when things can happen in any order - there just are too many permutations to consider. 强制（或假定）顺序是减少可能执行和发生的空间的一种方法。当事情可以以任何顺序发生时，有太多的排列需要考虑，因而很难对事情进行推理。 Does time progress at the same rate everywhere?各个分布式节点中时间相同吗？We all have an intuitive concept of time based on our own experience as individuals. Unfortunately, that intuitive notion of time makes it easier to picture total order rather than partial order. It’s easier to picture a sequence in which things happen one after another, rather than concurrently. It is easier to reason about a single order of messages than to reason about messages arriving in different orders and with different delays. 依据个人经验，我们都有一个直观的时间概念。但直观的时间概念使我们更容易描绘出总顺序而不是部分顺序。更容易想象事情发生的顺序，一个接一个，而不是同时发生。对一个消息顺序进行推理要比对以不同顺序和不同延迟到达的消息进行推理容易得多。（通常一串连续发生的事件比同时发生要更容易理解） However, when implementing distributing systems we want to avoid making strong assumptions about time and order, because the stronger the assumptions, the more fragile a system is to issues with the “time sensor” - or the onboard clock. Furthermore, imposing an order carries a cost. The more temporal nondeterminism that we can tolerate, the more we can take advantage of distributed computation. 然而，在实施分布式系统时，我们希望避免对时间和顺序做出强有力的假设，因为假设越强，系统就越容易受到“时间传感器”或时钟的问题的影响。此外，执行命令也会带来成本。我们越能容忍时间上的不确定性，就越能利用分布式计算。（分布式系统中关于“时间传感器”的定义和假设相较而言没那么刻板。对时间的不确定性容忍度约稿，对于系统的分布式计算更有利） There are three common answers to the question “does time progress at the same rate everywhere?”. These are: “Global clock”: yes “Local clock”: no, but “No clock”: no! 对于分布式节点中的时间是否同步，使用不同的时钟假设是不同的： 全局时钟：相同 （同步系统模型） 本地时钟：不同，但是本地有序 （部分同步系统模型） 不使用时钟：不同 （异步系统模型，用逻辑时钟来确定顺序） These correspond roughly to the three timing assumptions that I mentioned in the second chapter: the synchronous system model has a global clock, the partially synchronous model has a local clock, and in the asynchronous system model one cannot use clocks at all. Let’s look at these in more detail. Time with a “global-clock” assumption 全局时钟The global clock assumption is that there is a global clock of perfect accuracy, and that everyone has access to that clock. This is the way we tend to think about time, because in human interactions small differences in time don’t really matter. 全局时钟：有一个全局精确的时钟，并且每个节点都能接触到 The global clock is basically a source of total order (exact order of every operation on all nodes even if those nodes have never communicated). However, this is an idealized view of the world: in reality, clock synchronization is only possible to a limited degree of accuracy. This is limited by the lack of accuracy of clocks in commodity computers, by latency if a clock synchronization protocol such as NTP is used and fundamentally by the nature of spacetime. Assuming that clocks on distributed nodes are perfectly synchronized means assuming that clocks start at the same value and never drift apart. It’s a nice assumption because you can use timestamps freely to determine a global total order - bound by clock drift rather than latency - but this is a nontrivial operational challenge and a potential source of anomalies. There are many different scenarios where a simple failure - such as a user accidentally changing the local time on a machine, or an out-of-date machine joining a cluster, or synchronized clocks drifting at slightly different rates and so on that can cause hard-to-trace anomalies. Nevertheless, there are some real-world systems that make this assumption. Facebook’s Cassandra is an example of a system that assumes clocks are synchronized. It uses timestamps to resolve conflicts between writes - the write with the newer timestamp wins. This means that if clocks drift, new data may be ignored or overwritten by old data; again, this is an operational challenge (and from what I’ve heard, one that people are acutely aware of). Another interesting example is Google’s Spanner: the paper describes their TrueTime API, which synchronizes time but also estimates worst-case clock drift. 全局时钟的存在使在任何节点上的操作都按照一定的顺序执行，即便这些节点之间不发生通信交互 但在现实世界中，时钟同步只能存在于能容忍一定程度上的不精确的系统中。因为由于空间分布的原因而存在的延迟。 假设时钟在分布式节点中完美同步的话，说明时间都是从同一个值开始计时，并且永远不会不一样。这样一来的话使用时间戳的话就能完美的保证全局顺序。但是通常会有异常现象存在，但针对这些异常，也会有相应的处理方案 然而，现实中有一些系统使用的是全局时钟假设。比如Facebook的Cassandra系统。它使用时间戳来解决写的冲突，最新的版本胜出。另一个例子Google的Spanner，它的时钟也是同步的，但同时考虑了最坏情况下的时钟漂移 Time with a “Local-clock” assumption 本地时钟The second, and perhaps more plausible assumption is that each machine has its own clock, but there is no global clock. It means that you cannot use the local clock in order to determine whether a remote timestamp occurred before or after a local timestamp; in other words, you cannot meaningfully compare timestamps from two different machines. 更合理的情况是，每一个机器上有自己的时钟，但是不存在全局时钟。即你能通过本地时间戳来确定本地事件发生顺序，但是不能对不同机器间的时间戳进行比较 The local clock assumption corresponds more closely to the real world. It assigns a partial order: events on each system are ordered but events cannot be ordered across systems by only using a clock. 本地时钟的假设最符合真实世界的情况。它表明时间在每一个独立的分区上能够有序，但是在所有的系统中仅仅靠时钟来并不能保证事件的顺序 However, you can use timestamps to order events on a single machine; and you can use timeouts on a single machine as long as you are careful not to allow the clock to jump around. Of course, on a machine controlled by an end-user this is probably assuming too much: for example, a user might accidentally change their date to a different value while looking up a date using the operating system’s date control. Time with a “No-clock” assumption 无时钟Finally, there is the notion of logical time. Here, we don’t use clocks at all and instead track causality in some other way. Remember, a timestamp is simply a shorthand for the state of the world up to that point - so we can use counters and communication to determine whether something happened before, after or concurrently with something else. 这里存在一个逻辑时间的概念。不在使用时钟来追寻时间发生的因果顺序，而是通过另一种其它的方式-计数器和通信 This way, we can determine the order of events between different machines, but cannot say anything about intervals and cannot use timeouts (since we assume that there is no “time sensor”). This is a partial order: events can be ordered on a single system using a counter and no communication, but ordering events across systems requires a message exchange. 通过这种方式，我们能对不同机器上发生的事件的顺序进行比较，但是没法使用关于时间间隔以及超时设置的变量了（即不存在时间传感器了）。这从某种角度而言也是一种局部有序的情况：时间在单系统中能够使用计数器并且不用进行通信来保证事件顺序，但是在多系统之间就需要进行信息交换 One of the most cited papers in distributed systems is Lamport’s paper on time, clocks and the ordering of events. Vector clocks, a generalization of that concept (which I will cover in more detail), are a way to track causality without using clocks. Cassandra’s cousins Riak (Basho) and Voldemort (Linkedin) use vector clocks rather than assuming that nodes have access to a global clock of perfect accuracy. This allows those systems to avoid the clock accuracy issues mentioned earlier. When clocks are not used, the maximum precision at which events can be ordered across distant machines is bound by communication latency. 矢量钟就是一种不利用真正的时间来进行时间顺序因果追寻的方式。后面会进行详细介绍 How is time used in a distributed system?分布式系统中时间的作用What is the benefit of time? Time can define order across a system (without communication) Time can define boundary conditions for algorithms 时间的作用： 能够定义系统中事件的顺序（不需要通信） 能够定义算法的边界条件（故障检测器） The order of events is important in distributed systems, because many properties of distributed systems are defined in terms of the order of operations/events: where correctness depends on (agreement on) correct event ordering, for example serializability in a distributed database order can be used as a tie breaker when resource contention occurs, for example if there are two orders for a widget, fulfill the first and cancel the second one 在分布式系统中，确定事件发生的顺序是很重要的，因为许多操作是需要顺序进行的 系统的正确性取决于正确的时间顺序 当资源发生竞争时，顺序能够作为评判标准 A global clock would allow operations on two different machines to be ordered without the two machines communicating directly. Without a global clock, we need to communicate in order to determine order. 一个全局时钟能够允许两个机器上的操作有序进行并且不需要通信。但一旦没有全局时钟，我们就需要进行通信来确定不同机器上操作的顺序 Time can also be used to define boundary conditions for algorithms - specifically, to distinguish between “high latency” and “server or network link is down”. This is a very important use case; in most real-world systems timeouts are used to determine whether a remote machine has failed, or whether it is simply experiencing high network latency. Algorithms that make this determination are called failure detectors; and I will discuss them fairly soon. 时间也可以用来定义算法的边界条件，比如，判定系统到底是发生了“高延迟”还是出现了“服务或者网络的宕机”。用来做这个判断的算法被称为是故障检测器，后面会进行讨论 Vector clocks (time for causal order) 矢量时钟Earlier, we discussed the different assumptions about the rate of progress of time across a distributed system. Assuming that we cannot achieve accurate clock synchronization - or starting with the goal that our system should not be sensitive to issues with time synchronization, how can we order things? 前面我们讨论了分布式系统中不同的时间假设。如果我们没法获得精确的时钟同步的话，那么如何保证分布式系统中事件有序呢？ Lamport clocks and vector clocks are replacements for physical clocks which rely on counters and communication to determine the order of events across a distributed system. These clocks provide a counter that is comparable across different nodes. Lamport clocks和矢量时钟能够代替物理时钟来进行保证系统有序。通过计数器+通信来决定事件顺序 A Lamport clock is simple. Each process maintains a counter using the following rules: Whenever a process does work, increment the counter Whenever a process sends a message, include the counter When a message is received, set the counter to max(local_counter, received_counter) + 1 Lamport clockLamport clock很简单，每一个进程都有一个计数器，服从下面的规则： 一旦一个进程开始工作，计数器递增 任何进程发送的消息中，包含计数器的值 当一个消息被接收时，更新计数器的值为max（本地，接收）+1 Expressed as code: 用代码表示的话: 123456789101112131415function LamportClock() &#123; this.value = 1;&#125;LamportClock.prototype.get = function() &#123; return this.value;&#125;LamportClock.prototype.increment = function() &#123; this.value++;&#125;LamportClock.prototype.merge = function(other) &#123; this.value = Math.max(this.value, other.value) + 1;&#125; A Lamport clock allows counters to be compared across systems, with a caveat: Lamport clocks define a partial order. If timestamp(a) &lt; timestamp(b): a may have happened before b or a may be incomparable with b Lamport clock允许使用计数器来比较事件发生的顺序，比如如果 timestamp(a) &lt; timestamp(b)： a可能在b之前发生 a可能b无法和b比较 This is known as clock consistency condition: if one event comes before another, then that event’s logical clock comes before the others. If a and b are from the same causal history, e.g. either both timestamp values were produced on the same process; or b is a response to the message sent in a then we know that a happened before b. 这和时钟的一致性一样，如果一件事发生在另一件事之前，那么它的逻辑时钟也发生在这件事之前。如果事件a和事件b都是从同一个历史中演化而来的，那么如果a&lt;b，a一定发生在b之前 Intuitively, this is because a Lamport clock can only carry information about one timeline / history; hence, comparing Lamport timestamps from systems that never communicate with each other may cause concurrent events to appear to be ordered when they are not. 使用Lamport时钟也有一个缺点，因为它只包含了一个时间线的计数信息，那么同步发生的事情在这个时钟下仍然可比，即表现出有序性，但本质上，他们是同步的，不应该表现出有序性 Imagine a system that after an initial period divides into two independent subsystems which never communicate with each other. For all events in each independent system, if a happened before b, then ts(a) &lt; ts(b); but if you take two events from the different independent systems (e.g. events that are not causally related) then you cannot say anything meaningful about their relative order. While each part of the system has assigned timestamps to events, those timestamps have no relation to each other. Two events may appear to be ordered even though they are unrelated. However - and this is still a useful property - from the perspective of a single machine, any message sent with ts(a) will receive a response with ts(b) which is &gt; ts(a). A vector clock is an extension of Lamport clock, which maintains an array [ t1, t2, ... ] of N logical clocks - one per each node. Rather than incrementing a common counter, each node increments its own logical clock in the vector by one on each internal event. Hence the update rules are: Whenever a process does work, increment the logical clock value of the node in the vector Whenever a process sends a message, include the full vector of logical clocks When a message is received: update each element in the vector to be max(local, received) increment the logical clock value representing the current node in the vector vector clock矢量时钟矢量时钟是Lamport clock的一个衍生，它包含了一个含有N个节点计数器值的计数器列表 [t1,t2,…]，每一个节点递增他们自己的逻辑时钟（计数器的值），规则如下： 一旦一个进程开始工作，矢量钟中的关于该进程节点上的计数器值递增 任何进程发送的消息中，包含矢量计数器列表 当一个消息被接收时： 更新矢量 递增矢量中代表当前节点的逻辑时钟的计数器值 Again, expressed as code: 12345678910111213141516171819202122232425262728293031323334function VectorClock(value) &#123; // expressed as a hash keyed by node id: e.g. &#123; node1: 1, node2: 3 &#125; this.value = value || &#123;&#125;;&#125;VectorClock.prototype.get = function() &#123; return this.value;&#125;;VectorClock.prototype.increment = function(nodeId) &#123; if(typeof this.value[nodeId] == 'undefined') &#123; this.value[nodeId] = 1; &#125; else &#123; this.value[nodeId]++; &#125;&#125;;VectorClock.prototype.merge = function(other) &#123; var result = &#123;&#125;, last, a = this.value, b = other.value; // This filters out duplicate keys in the hash (Object.keys(a) .concat(b)) .sort() .filter(function(key) &#123; var isDuplicate = (key == last); last = key; return !isDuplicate; &#125;).forEach(function(key) &#123; result[key] = Math.max(a[key] || 0, b[key] || 0); &#125;); this.value = result;&#125;; This illustration (source) shows a vector clock : 下图也能表示矢量钟： Each of the three nodes (A, B, C) keeps track of the vector clock. As events occur, they are timestamped with the current value of the vector clock. Examining a vector clock such as { A: 2, B: 4, C: 1 } lets us accurately identify the messages that (potentially) influenced that event. The issue with vector clocks is mainly that they require one entry per node, which means that they can potentially become very large for large systems. A variety of techniques have been applied to reduce the size of vector clocks (either by performing periodic garbage collection, or by reducing accuracy by limiting the size). We’ve looked at how order and causality can be tracked without physical clocks. Now, let’s look at how time durations can be used for cutoff. 上图对ABC三个节点的矢量钟进行了一个追踪。可以发现当一个事件发生后，矢量钟对每个节点目前的情况打上了一个时间戳的标记，随着事件发生，计数器的值进行改变。从矢量值中可以对事件发生的顺序进行判断 总结： Lamport clock每个节点上维护一个计数器值，每次通信对这个值进行更新 vector clock每个节点上维护一个计数器列表，每次通信对这个列表进行更新 Failure detectors (time for cutoff) 故障检测器As I stated earlier, the amount of time spent waiting can provide clues about whether a system is partitioned or merely experiencing high latency. In this case, we don’t need to assume a global clock of perfect accuracy - it is simply enough that there is a reliable-enough local clock. 前面说过，等待花费的时长能够用来作为一个判断系统到底是发生了分区故障，还是高延迟的一种线索。在这里，我们不需要假设有一个精确的全局时钟，仅仅有一个可信赖的本地时钟就足够了 Given a program running on one node, how can it tell that a remote node has failed? In the absence of accurate information, we can infer that an unresponsive remote node has failed after some reasonable amount of time has passed. But what is a “reasonable amount”? This depends on the latency between the local and remote nodes. Rather than explicitly specifying algorithms with specific values (which would inevitably be wrong in some cases), it would be nicer to deal with a suitable abstraction. 一个节点上运行一个程序，如果运行的信息延迟的时间到达一定的时长的话，我们就认为这个节点发生了故障。但是这个时长改如何定义呢？ A failure detector is a way to abstract away the exact timing assumptions. Failure detectors are implemented using heartbeat messages and timers. Processes exchange heartbeat messages. If a message response is not received before the timeout occurs, then the process suspects the other process. A failure detector based on a timeout will carry the risk of being either overly aggressive (declaring a node to have failed) or being overly conservative (taking a long time to detect a crash). How accurate do failure detectors need to be for them to be usable? 故障检测器是一种抽象的方法：心跳信息+定时器 进程间交换心跳信息，如果一个进程在超时前没有收到响应信息，那么这个进程会怀疑其它进程 一个基于超时而言的故障检测器通常要么会过度检测（轻易断言一个节点发生故障），要么检测会过于保守（花费很长的等待时间来判断故障）。那么一个如何使用故障检测器使得它发挥出好的作用呢？ Chandra et al. (1996) discuss failure detectors in the context of solving consensus - a problem that is particularly relevant since it underlies most replication problems where the replicas need to agree in environments with latency and network partitions. They characterize failure detectors using two properties, completeness and accuracy: Strong completeness.Every crashed process is eventually suspected by every correct process. Weak completeness.Every crashed process is eventually suspected by some correct process. Strong accuracy.No correct process is suspected ever. Weak accuracy.Some correct process is never suspected. 有人用两个属性（完整性、精确性）将故障检测器进行了描述： 强完整性：每个故障的进程都会被任何正确的进程怀疑弱完整性：每个故障的进程会被一部分正确的进程怀疑强精确性：没有正确的进程会被怀疑弱精确性：一些正确的进程也会被怀疑 Completeness is easier to achieve than accuracy; indeed, all failure detectors of importance achieve it - all you need to do is not to wait forever to suspect someone. Chandra et al. note that a failure detector with weak completeness can be transformed to one with strong completeness (by broadcasting information about suspected processes), allowing us to concentrate on the spectrum of accuracy properties. 完整性比精确性更容易实现。并且一个弱完整性的故障检测器能够转换成强完整性的故障检测器（通过广播被怀疑的进程的消息） Avoiding incorrectly suspecting non-faulty processes is hard unless you are able to assume that there is a hard maximum on the message delay. That assumption can be made in a synchronous system model - and hence failure detectors can be strongly accurate in such a system. Under system models that do not impose hard bounds on message delay, failure detection can at best be eventually accurate. 通常对一个没有发生故障的进程进行错误的怀疑是无法避免的，因为你不知道消息延迟的上限是多少。但是在同步系统中，这个消息延迟的上限是固定的，因此在同步系统中使用故障检测器是非常精确的。 Chandra et al. show that even a very weak failure detector - the eventually weak failure detector ⋄W (eventually weak accuracy + weak completeness) - can be used to solve the consensus problem. The diagram below (from the paper) illustrates the relationship between system models and problem solvability: 研究表明即使是一个弱故障检测器（最终故障检测器），也能用来解决一致性问题。下图阐述了系统模型和问题可解性之间的关系： As you can see above, certain problems are not solvable without a failure detector in asynchronous systems. This is because without a failure detector (or strong assumptions about time bounds e.g. the synchronous system model), it is not possible to tell whether a remote node has crashed, or is simply experiencing high latency. That distinction is important for any system that aims for single-copy consistency: failed nodes can be ignored because they cannot cause divergence, but partitioned nodes cannot be safely ignored. 从上图可以看到，异步系统中，不使用故障检测器是无法对明确的问题进行解决的。因为，如果没有故障检测器，你无法得知一个远方的节点是否发生故障，或是因为高延迟的存在。这个判断对于单拷贝一致性的系统来说非常重要：故障节点会被忽略，因为它们不会带来分歧，但是分区节点不能被忽略，因为可能会造成数据分歧 How can one implement a failure detector? Conceptually, there isn’t much to a simple failure detector, which simply detects failure when a timeout expires. The most interesting part relates to how the judgments are made about whether a remote node has failed. 怎样实施一个故障检测器呢？事实上，不存在一个很简单的故障检测器，因为判断一个节点是否发生故障时很难的 Ideally, we’d prefer the failure detector to be able to adjust to changing network conditions and to avoid hardcoding timeout values into it. For example, Cassandra uses an accrual failure detector, which is a failure detector that outputs a suspicion level (a value between 0 and 1) rather than a binary “up” or “down” judgment. This allows the application using the failure detector to make its own decisions about the tradeoff between accurate detection and early detection. 依赖超时设置的值来判断是否发生故障。Cassandra它使用的是一个精确的故障检测器，它给出的故障判断是一个猜测值（0-1间，概率值）而不是一个二进制的数（0、1），这样一来，系统应用能够根据自己的定义来判断节点是否发生故障，进行一个无误检测和超前检测的权衡 Time, order and performance 时间、顺序和性能Earlier, I alluded to having to pay the cost for order. What did I mean? If you’re writing a distributed system, you presumably own more than one computer. The natural (and realistic) view of the world is a partial order, not a total order. You can transform a partial order into a total order, but this requires communication, waiting and imposes restrictions that limit how many computers can do work at any particular point in time. 如果你在设计一个分布式系统，你肯定拥有不止一台的计算机。那么从直观的角度来看，顺序是分区有序的而并非全局有序。你能够通过通信的方式，使得分区有序转变成全局有序，但是这通常还需要等待以及受到任意同一时刻能够有多少节点进行同时工作的限制 All clocks are mere approximations bound by either network latency (logical time) or by physics. Even keeping a simple integer counter in sync across multiple nodes is a challenge. 就最简单的保持一个简单的整数计数器在分布节点上同步都很有挑战性 While time and order are often discussed together, time itself is not such a useful property. Algorithms don’t really care about time as much as they care about more abstract properties: the causal ordering of events failure detection (e.g. approximations of upper bounds on message delivery) consistent snapshots (e.g. the ability to examine the state of a system at some point in time; not discussed here) 事实上，算法通常不在乎时间，而是在乎顺序： 事件发生的原因顺序 故障检测器 一致快照 Imposing a total order is possible, but expensive. It requires you to proceed at the common (lowest) speed. Often the easiest way to ensure that events are delivered in some defined order is to nominate a single (bottleneck) node through which all operations are passed. 全局一致是可以实现的，但是代价很大，它要求所有的处理在相同的速度条件下。一个最简单的方法是：投票，选出一个经过了所有操作的节点出来 Is time / order / synchronicity really necessary? It depends. In some use cases, we want each intermediate operation to move the system from one consistent state to another. For example, in many cases we want the responses from a database to represent all of the available information, and we want to avoid dealing with the issues that might occur if the system could return an inconsistent result. 时间/顺序/同步真的有必要吗？这取决于你的案例。比如在一些用户案例中，我们想要每一次的操作都能让系统从一个一致性的转态转到另一个一致性的状态。举个例子：在数据库中，我们想要从数据库中找到能代表所有可用的信息，同时我们想避免处理系统返回不一致的结果所带来的问题。 But in other cases, we might not need that much time / order / synchronization. For example, if you are running a long running computation, and don’t really care about what the system does until the very end - then you don’t really need much synchronization as long as you can guarantee that the answer is correct. 但是在其他的一些例子中，我们可能不需要时间/顺序/同步。比如，我们想进行一个很长的计算操作，只要能保证最后的结果是正确的，我们并不关心这些运算是否同步发生 Synchronization is often applied as a blunt tool across all operations, when only a subset of cases actually matter for the final outcome. When is order needed to guarantee correctness? The CALM theorem - which I will discuss in the last chapter - provides one answer. 同步性通常是用来做为操作的限制工具的，仅仅当我们的结果只是收到一部分数据集的影响的时候才需要。顺序什么时候保证系统的可用性-这涉及到我们之后讨论的CALM理论 In other cases, it is acceptable to give an answer that only represents the best known estimate - that is, is based on only a subset of the total information contained in the system. In particular, during a network partition one may need to answer queries with only a part of the system being accessible. In other use cases, the end user cannot really distinguish between a relatively recent answer that can be obtained cheaply and one that is guaranteed to be correct and is expensive to calculate. For example, is the Twitter follower count for some user X, or X+1? Or are movies A, B and C the absolutely best answers for some query? Doing a cheaper, mostly correct “best effort” can be acceptable. In the next two chapters we’ll examine replication for fault-tolerant strongly consistent systems - systems which provide strong guarantees while being increasingly resilient to failures. These systems provide solutions for the first case: when you need to guarantee correctness and are willing to pay for it. Then, we’ll discuss systems with weak consistency guarantees, which can remain available in the face of partitions, but that can only give you a “best effort” answer. 还有另一些例子中，我们能够接受那些尽力而为的答案作为我们系统的最后结果。这会涉及到一致性问题 强一致性：保证准确性但付出可用性低的代价 弱一致性：保证系统可用性，但要只能告诉你“best effort”(尽力了) Further readingLamport clocks, vector clocks Time, Clocks and Ordering of Events in a Distributed System - Leslie Lamport, 1978 Failure detection Unreliable failure detectors and reliable distributed systems - Chandra and Toueg Latency- and Bandwidth-Minimizing Optimal Failure Detectors - So &amp; Sirer, 2007 The failure detector abstraction, Freiling, Guerraoui &amp; Kuznetsov, 2011 Snapshots Consistent global states of distributed systems: Fundamental concepts and mechanisms, Ozalp Babaogly and Keith Marzullo, 1993 Distributed snapshots: Determining global states of distributed systems, K. Mani Chandy and Leslie Lamport, 1985 Causality Detecting Causal Relationships in Distributed Computations: In Search of the Holy Grail - Schwarz &amp; Mattern, 1994 Understanding the Limitations of Causally and Totally Ordered Communication - Cheriton &amp; Skeen, 1993]]></content>
      <categories>
        <category>微服务理论文章阅读学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]Distributed systems for fun and profit_2抽象描述系统的特征]]></title>
    <url>%2F2019%2F04%2F22%2F%5B%E8%AF%91%5DDistributed%20systems%20for%20fun%20and%20profit_2%E6%8A%BD%E8%B1%A1%E6%8F%8F%E8%BF%B0%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%89%B9%E5%BE%81%2F</url>
    <content type="text"><![CDATA[二. Up and down the level of abstraction 抽象描述系统的特征In this chapter, we’ll travel up and down the level of abstraction, look at some impossibility results (CAP and FLP), and then travel back down for the sake of performance. 这一节将关注一些不可能的结果（CAP和FLP），并且会继续讨论性能 If you’ve done any programming, the idea of levels of abstraction is probably familiar to you. You’ll always work at some level of abstraction, interface with a lower level layer through some API, and probably provide some higher-level API or user interface to your users. The seven-layer OSI model of computer networking is a good example of this. 通常编程会在一些抽象层次上工作，比如API接口，7层计算机网络OSI模型（开放系统互连模型，将通信系统分为7层：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层） Distributed programming is, I’d assert, in large part dealing with consequences of distribution (duh!). That is, there is a tension between the reality that there are many nodes and with our desire for systems that “work like a single system”. That means finding a good abstraction that balances what is possible with what is understandable and performant. 分布式编程在很大程度上都在解决由于分布带来的一系列问题。怎样让分布式系统工作起来像单机一样有一定的困难。我们希望找到一个能够平衡“可理解性”和“高效性”的好的抽象 What do we mean when say X is more abstract than Y? First, that X does not introduce anything new or fundamentally different from Y. In fact, X may remove some aspects of Y or present them in a way that makes them more manageable.Second, that X is in some sense easier to grasp than Y, assuming that the things that X removed from Y are not important to the matter at hand. 通常我们说X是一个比Y更好的抽象，代表我们认为X在Y的基础上不会给人带来额外的信息，X只是比Y更好理解而已 As Nietzsche wrote: Every concept originates through our equating what is unequal. No leaf ever wholly equals another, and the concept “leaf” is formed through an arbitrary abstraction from these individual differences, through forgetting the distinctions; and now it gives rise to the idea that in nature there might be something besides the leaves which would be “leaf” - some kind of original form after which all leaves have been woven, marked, copied, colored, curled, and painted, but by unskilled hands, so that no copy turned out to be a correct, reliable, and faithful image of the original form. 正如尼采所言： 我们将不同事物的共性抽象成为一个概念。例如没有一片叶子和其余的叶子完全相同。。。 Abstractions, fundamentally, are fake. Every situation is unique, as is every node. But abstractions make the world manageable: simpler problem statements - free of reality - are much more analytically tractable and provided that we did not ignore anything essential, the solutions are widely applicable. 事实上，在分布式系统中，每一个节点、每一种情况都是不一样的，抽象其实在根本上是不存在的。但是一个抽象能够让一些问题能够被陈述出来，更利于理解和处理，并且由于关注的是问题的本质，所以解决方案往往应用会更广泛 Indeed, if the things that we kept around are essential, then the results we can derive will be widely applicable. This is why impossibility results are so important: they take the simplest possible formulation of a problem, and demonstrate that it is impossible to solve within some set of constraints or assumptions. All abstractions ignore something in favor of equating things that are in reality unique. The trick is to get rid of everything that is not essential. How do you know what is essential? Well, you probably won’t know a priori. Every time we exclude some aspect of a system from our specification of the system, we risk introducing a source of error and/or a performance issue. That’s why sometimes we need to go in the other direction, and selectively introduce some aspects of real hardware and the real-world problem back. It may be sufficient to reintroduce some specific hardware characteristics (e.g. physical sequentiality) or other physical characteristics to get a system that performs well enough. With this in mind, what is the least amount of reality we can keep around while still working with something that is still recognizable as a distributed system? A system model is a specification of the characteristics we consider important; having specified one, we can then take a look at some impossibility results and challenges. 例如不可能的结果就是一种好的抽象：由于对于问题采用最简单的公式来描述，并且证明不可能存在一组约束或假设能解决这个问题。 A system model 系统模型A key property of distributed systems is distribution. More specifically, programs in a distributed system: run concurrently on independent nodes … are connected by a network that may introduce nondeterminism and message loss … and have no shared memory or shared clock. 分布式系统最重要的一个属性就是：分布，程序在分布式系统上需要满足： 在独立的节点上同时运行 网络连接可能会带来不确定性和丢失信息 不共享内存和时钟 There are many implications: each node executes a program concurrently knowledge is local: nodes have fast access only to their local state, and any information about global state is potentially out of date nodes can fail and recover from failure independently messages can be delayed or lost (independent of node failure; it is not easy to distinguish network failure and node failure) and clocks are not synchronized across nodes (local timestamps do not correspond to the global real time order, which cannot be easily observed) 具体而言： 每个节点会同时执行程序 信息局部存在：局部节点之间相互访问快，但是关于全局的信息可能会过期 节点之间的失败和恢复能够相互独立 信息可能会出现延迟或丢失（网络失败与节点失败难以辨别） 节点之间的时钟不同步（局部时间戳和全局时间顺序不吻合，并且难以被观察到） A system model enumerates the many assumptions associated with a particular system design. System model 系统模型a set of assumptions about the environment and facilities on which a distributed system is implemented分布式系统实施过程中的一系列关于环境与设置的假设 System models vary in their assumptions about the environment and facilities. These assumptions include: what capabilities the nodes have and how they may fail how communication links operate and how they may fail and properties of the overall system, such as assumptions about time and order 这些假设包括： 节点容量及怎样算失败 通信交互操作如何进行及怎样算失败 总体系统的属性，如：时间和顺序 A robust system model is one that makes the weakest assumptions: any algorithm written for such a system is very tolerant of different environments, since it makes very few and very weak assumptions. On the other hand, we can create a system model that is easy to reason about by making strong assumptions. For example, assuming that nodes do not fail means that our algorithm does not need to handle node failures. However, such a system model is unrealistic and hence hard to apply into practice. 弱假设会使得系统具有更高的鲁棒性，强假设会使系统具有更高的理解性和可推理性* Let’s look at the properties of nodes, links and time and order in more detail. Nodes in our system model 分布式系统中的节点Nodes serve as hosts for computation and storage. They have: the ability to execute a program the ability to store data into volatile memory (which can be lost upon failure) and into stable state (which can be read after a failure) a clock (which may or may not be assumed to be accurate) 节点用来计算和存储，它拥有： 执行程序的能力 存储大量数据的能力（数据可能会丢失，但也可以恢复） 时钟（可能不是很精确） Nodes execute deterministic algorithms: the local computation, the local state after the computation, and the messages sent are determined uniquely by the message received and local state when the message was received. 节点执行时满足：当消息被接收到时，本地状态与消息决定了本地运算、本地运算后的状态以及消息发送的唯一性 There are many possible failure models which describe the ways in which nodes can fail. In practice, most systems assume a crash-recovery failure model: that is, nodes can only fail by crashing, and can (possibly) recover after crashing at some later point. 有许多故障模型会描述能够容许节点发生什么样类型的失败。实际上，大部分系统都是一个崩溃恢复失效模型：节点只能由于崩溃而失败,并且在接下来的时间节点能够被恢复 Another alternative is to assume that nodes can fail by misbehaving in any arbitrary way. This is known as Byzantine fault tolerance. Byzantine faults are rarely handled in real world commercial systems, because algorithms resilient to arbitrary faults are more expensive to run and more complex to implement. I will not discuss them here. 另一种故障模型是拜占庭容错：节点可以能以任意方式发生故障，其容错过于复杂昂贵，现实中很少处理。 Communication links in our system model 系统模型中的通信链路Communication links connect individual nodes to each other, and allow messages to be sent in either direction. Many books that discuss distributed algorithms assume that there are individual links between each pair of nodes, that the links provide FIFO (first in, first out) order for messages, that they can only deliver messages that were sent, and that sent messages can be lost. 通信链路使得各个分布的节点之间能相互连接，并传输信息。 Some algorithms assume that the network is reliable: that messages are never lost and never delayed indefinitely. This may be a reasonable assumption for some real-world settings, but in general it is preferable to consider the network to be unreliable and subject to message loss and delays. A network partition occurs when the network fails while the nodes themselves remain operational. When this occurs, messages may be lost or delayed until the network partition is repaired. Partitioned nodes may be accessible by some clients, and so must be treated differently from crashed nodes. The diagram below illustrates a node failure vs. a network partition: 当节点仍在操作时，网络发生失败会导致网络分区，信息将会丢失或者发生延迟，直到网络修复。但此时用户仍然能够访问节点。所以网络失败必须要和节点失败区分开来。下同阐明了两种情况： It is rare to make further assumptions about communication links. We could assume that links only work in one direction, or we could introduce different communication costs (e.g. latency due to physical distance) for different links. However, these are rarely concerns in commercial environments except for long-distance links (WAN latency) and so I will not discuss them here; a more detailed model of costs and topology allows for better optimization at the cost of complexity. Timing / ordering assumptions 时间/顺序假设One of the consequences of physical distribution is that each node experiences the world in a unique manner. This is inescapable, because information can only travel at the speed of light. If nodes are at different distances from each other, then any messages sent from one node to the others will arrive at a different time and potentially in a different order at the other nodes. 分布式中，节点之间存在距离，信息传递一定存在通信时间，并且每个节点接收到信息的时刻不一致 Timing assumptions are a convenient shorthand for capturing assumptions about the extent to which we take this reality into account. The two main alternatives are: Synchronous system model Processes execute in lock-step; there is a known upper bound on message transmission delay; each process has an accurate clock Asynchronous system model No timing assumptions - e.g. processes execute at independent rates; there is no bound on message transmission delay; useful clocks do not exist 两种关于时间假设选择的模型： 同步系统模型程序锁步执行，消息传播延迟有明确的上限，每一步都有精确的时钟 异步系统模型没有时间假设，每个进程执行在相互独立的比率下，消息传播没有延迟上限，时钟不存在 The synchronous system model imposes many constraints on time and order. It essentially assumes that the nodes have the same experience: that messages that are sent are always received within a particular maximum transmission delay, and that processes execute in lock-step. This is convenient, because it allows you as the system designer to make assumptions about time and order, while the asynchronous system model doesn’t. 同步系统有许多时间与顺序的限制，默认节点有相同的体验，消息传播延迟在一定的范围内，程序执行严格按照锁步。但在异步系统中，时间的依赖不存在 Asynchronicity is a non-assumption: it just assumes that you can’t rely on timing (or a “time sensor”). It is easier to solve problems in the synchronous system model, because assumptions about execution speeds, maximum message transmission delays and clock accuracy all help in solving problems since you can make inferences based on those assumptions and rule out inconvenient failure scenarios by assuming they never occur. Of course, assuming the synchronous system model is not particularly realistic. Real-world networks are subject to failures and there are no hard bounds on message delay. Real world systems are at best partially synchronous: they may occasionally work correctly and provide some upper bounds, but there will be times where messages are delayed indefinitely and clocks are out of sync. I won’t really discuss algorithms for synchronous systems here, but you will probably run into them in many other introductory books because they are analytically easier (but unrealistic). 因此对于同步系统来说，解决问题更容易一些，因为你能通过对于时间和顺序的假设，来推测和排除失败场景发生在哪些地方。 但通常而言，同步系统模型能更好解释和理解，但不现实 The consensus problem 一致性问题During the rest of this text, we’ll vary the parameters of the system model. Next, we’ll look at how varying two system properties: whether or not network partitions are included in the failure model, and synchronous vs. asynchronous timing assumptions influence the system design choices by discussing two impossibility results (FLP and CAP). 接下来讨论两个系统中的属性差异： 失败模型中是否包含网络分区 同步异步时间假设 会对系统设计造成什么样的影响，以及会带来的两种结果（FLP与CAP） Of course, in order to have a discussion, we also need to introduce a problem to solve. The problem I’m going to discuss is the consensus problem. Several computers (or nodes) achieve consensus if they all agree on some value. More formally: Agreement: Every correct process must agree on the same value. Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process. Termination: All processes eventually reach a decision. Validity: If all correct processes propose the same value V, then all correct processes decide V. The consensus problem is at the core of many commercial distributed systems. After all, we want the reliability and performance of a distributed system without having to deal with the consequences of distribution (e.g. disagreements / divergence between nodes), and solving the consensus problem makes it possible to solve several related, more advanced problems such as atomic broadcast and atomic commit. 首先解释什么是一致性问题： 一致性（Agreement]）：每个正确的进程对某个值达成共识 完整性（Integrity）：每个正确的进程最多决定一个值，一旦决定了某个值，那么这个值一定被其它进程接受 终止性（Termination）：所有进程最终达成一致同意某个值 合法性（Validity）：如果所有正确的节点进程提出相同的值V，那么所有正确的节点进程达成一致，承认值V 共识问题是许多商业分布式系统的核心，解决共识问题可以解决几个相关的、更高级的问题，如原子广播和原子提交 Two impossibility results 两个不可能结果The first impossibility result, known as the FLP impossibility result, is an impossibility result that is particularly relevant to people who design distributed algorithms. The second - the CAP theorem - is a related result that is more relevant to practitioners; people who need to choose between different system designs but who are not directly concerned with the design of algorithms. FLP定理分布式算法需要关注 CAP定理实践者需要关注（不关心算法，关心选择什么样的系统设计） The FLP impossibility resultI will only briefly summarize the FLP impossibility result, though it is considered to be more important in academic circles. The FLP impossibility result (named after the authors, Fischer, Lynch and Patterson) examines the consensus problem under the asynchronous system model (technically, the agreement problem, which is a very weak form of the consensus problem). It is assumed that nodes can only fail by crashing; that the network is reliable, and that the typical timing assumptions of the asynchronous system model hold: e.g. there are no bounds on message delay. Under these assumptions, the FLP result states that “there does not exist a (deterministic) algorithm for the consensus problem in an asynchronous system subject to failures, even if messages can never be lost, at most one process may fail, and it can only fail by crashing (stopping executing)”. This result means that there is no way to solve the consensus problem under a very minimal system model in a way that cannot be delayed forever. The argument is that if such an algorithm existed, then one could devise an execution of that algorithm in which it would remain undecided (“bivalent”) for an arbitrary amount of time by delaying message delivery - which is allowed in the asynchronous system model. Thus, such an algorithm cannot exist. This impossibility result is important because it highlights that assuming the asynchronous system model leads to a tradeoff: algorithms that solve the consensus problem must either give up safety or liveness when the guarantees regarding bounds on message delivery do not hold. This insight is particularly relevant to people who design algorithms, because it imposes a hard constraint on the problems that we know are solvable in the asynchronous system model. The CAP theorem is a related theorem that is more relevant to practitioners: it makes slightly different assumptions (network failures rather than node failures), and has more clear implications for practitioners choosing between system designs. FLPFLP定理：在异步通信场景，即使只有一个进程失败了，没有任何算法能保证非失败进程能够达到一致性（在假设网络可靠、节点只会因崩溃而失效的最小化异步模型系统中，仍然不存在一个可以解决一致性问题的确定性算法。FLP只是证明了异步通信的最坏情况，实际上根据FLP定理，异步网络中是无法完全同时保证 safety 和 liveness 的一致性算法，但如果我们 safety 或 liveness 要求，这个算法进入无法表决通过的无限死循环的概率是非常低的。） 因为异步系统中的消息传播具有延迟性，那么如果存在这样一个算法，这个算法会在任何的时间内保持着不确定信，无法保证达成一致性，与假设矛盾。因此通常异步系统模型需要进行折中处理：放弃一定的安全性当消息传播延迟上限不能确定时 The CAP theoremThe CAP theorem was initially a conjecture made by computer scientist Eric Brewer. It’s a popular and fairly useful way to think about tradeoffs in the guarantees that a system design makes. It even has a formal proof by Gilbert and Lynch and no, Nathan Marz didn’t debunk it, in spite of what a particular discussion site thinks. The theorem states that of these three properties: Consistency: all nodes see the same data at the same time. Availability: node failures do not prevent survivors from continuing to operate. Partition tolerance: the system continues to operate despite message loss due to network and/or node failure CAPCAP定理：一个分布式系统不可能同时满足consistency、availability、partition tolerance这三个基本需求，最多同时满足两个 consistency一致性：所有节点同一时刻看到相同数据 availability可用性：节点失败不阻止其他正在运行的节点的工作 partition tolerance分区容错：即使出现信息丢失或网络、节点失败，系统也能继续运行（通过复制） only two can be satisfied simultaneously. We can even draw this as a pretty diagram, picking two properties out of three gives us three types of systems that correspond to different intersections: 这三种性质进行俩俩组合，得到下面三种情况 Note that the theorem states that the middle piece (having all three properties) is not achievable. Then we get three different system types: CA (consistency + availability). Examples include full strict quorum protocols, such as two-phase commit. CP (consistency + partition tolerance). Examples include majority quorum protocols in which minority partitions are unavailable such as Paxos. AP (availability + partition tolerance). Examples include protocols using conflict resolution, such as Dynamo. CA：完全严格的仲裁协议例如2PC（两阶段提交协议，第一阶段投票，第二阶段事物提交） CP：不完全（多数）仲裁协议，例如Paxos、Raft AP：使用冲突解决的协议，例如Dynamo、Gossip The CA and CP system designs both offer the same consistency model: strong consistency. The only difference is that a CA system cannot tolerate any node failures; a CP system can tolerate up to f faults given 2f+1 nodes in a non-Byzantine failure model (in other words, it can tolerate the failure of a minority f of the nodes as long as majority f+1 stays up). The reason is simple: A CA system does not distinguish between node failures and network failures, and hence must stop accepting writes everywhere to avoid introducing divergence (multiple copies). It cannot tell whether a remote node is down, or whether just the network connection is down: so the only safe thing is to stop accepting writes. A CP system prevents divergence (e.g. maintains single-copy consistency) by forcing asymmetric behavior on the two sides of the partition. It only keeps the majority partition around, and requires the minority partition to become unavailable (e.g. stop accepting writes), which retains a degree of availability (the majority partition) and still ensures single-copy consistency. CA和CP系统设计遵循的都是强一致性理论。不同的是CA系统不能容忍节点发生故障。CP系统能够容忍2f+1个节点中有f个节点发生失败。原因如下： CA系统无法辨别是网络故障还是节点故障，因此一旦发生失败，系统为了避免带来数据分歧，会立马阻止写操作（不能判别，所以安全的办法就是stop） CP系统通过强制性分开两侧分区的不对称行为来阻止发生分歧。仅仅保证主要的分区工作，并且要求最少的分区是不可用的。最终能够使得主要的分区能够运行工作，保证一定程度上的可用性，确保单拷贝一致性 I’ll discuss this in more detail in the chapter on replication when I discuss Paxos. The important thing is that CP systems incorporate network partitions into their failure model and distinguish between a majority partition and a minority partition using an algorithm like Paxos, Raft or viewstamped replication. CA systems are not partition-aware, and are historically more common: they often use the two-phase commit algorithm and are common in traditional distributed relational databases. 重要的一点是，CP系统中将网络分区考虑到了它的故障模型中，并且用算法识别主要分区和小分区。CA系统不关注分区 Assuming that a partition occurs, the theorem reduces to a binary choice between availability and consistency. 当我们假设分区一定发生时，那么在可用性和一致性中怎样进行一个选择呢 I think there are four conclusions that should be drawn from the CAP theorem: First, that many system designs used in early distributed relational database systems did not take into account partition tolerance (e.g. they were CA designs). Partition tolerance is an important property for modern systems, since network partitions become much more likely if the system is geographically distributed (as many large systems are). 第一,早期的分布式系统没有考虑分区容错（CA设计）。但是分区容错是一个很重要的性质，因为一旦系统规模很大，网络分区是不可避免的 Second, that there is a tension between strong consistency and high availability during network partitions. The CAP theorem is an illustration of the tradeoffs that occur between strong guarantees and distributed computation. 第二,P存在时 强一致性和高可用性之间存在矛盾.CAP理论说明了在P存在时权衡A和C In some sense, it is quite crazy to promise that a distributed system consisting of independent nodes connected by an unpredictable network “behaves in a way that is indistinguishable from a non-distributed system”. Strong consistency guarantees require us to give up availability during a partition. This is because one cannot prevent divergence between two replicas that cannot communicate with each other while continuing to accept writes on both sides of the partition. 强一致性会导致当存在分区时系统不可用。因为当两个复制集之间不能相互通信时，无法将写操作告诉对方，因此会带来分歧 How can we work around this? By strengthening the assumptions (assume no partitions) or by weakening the guarantees. Consistency can be traded off against availability (and the related capabilities of offline accessibility and low latency). If “consistency” is defined as something less than “all nodes see the same data at the same time” then we can have both availability and some (weaker) consistency guarantee. 那么该怎样避免这种情况发生呢。一致性和可用性之间应该怎样进行折中处理？这个时候就需要将强一致性进行弱化，转成弱一致性，来使得我们再保证弱一致性的情况下，也能保证系统的可用性 Third, that there is a tension between strong consistency and performance in normal operation. 第三,在一般操作中,强一致性和性能之间有矛盾 Strong consistency / single-copy consistency requires that nodes communicate and agree on every operation. This results in high latency during normal operation. 强一致/单拷贝一致性 性需要节点之间的通讯时间,所以会有高延迟 If you can live with a consistency model other than the classic one, a consistency model that allows replicas to lag or to diverge, then you can reduce latency during normal operation and maintain availability in the presence of partitions. 如果舍弃传统的一致性模型,可以通过复制来减少延迟 When fewer messages and fewer nodes are involved, an operation can complete faster. But the only way to accomplish that is to relax the guarantees: let some of the nodes be contacted less frequently, which means that nodes can contain old data. 允许某些节点存在旧数据 This also makes it possible for anomalies to occur. You are no longer guaranteed to get the most recent value. Depending on what kinds of guarantees are made, you might read a value that is older than expected, or even lose some updates. 可能会导致异常,可能会读到旧数据,甚至丢失某些更新 Fourth - and somewhat indirectly - that if we do not want to give up availability during a network partition, then we need to explore whether consistency models other than strong consistency are workable for our purposes. 第四,不直接的一点,如果不想降低可用性（在网络分区的情况下），就应该找到另一个一致性模型而不是继续锁定强一致性 For example, even if user data is georeplicated to multiple datacenters, and the link between those two datacenters is temporarily out of order, in many cases we’ll still want to allow the user to use the website / service. This means reconciling two divergent sets of data later on, which is both a technical challenge and a business risk. But often both the technical challenge and the business risk are manageable, and so it is preferable to provide high availability. 例如：当数据复制到不同的机器上，并且节点间的联系发生故障时，需要协调两方的数据。这个存在一定的技术挑战和商业风险，当两者都能解决时，此时系统还是高可用性的 Consistency and availability are not really binary choices, unless you limit yourself to strong consistency. But strong consistency is just one consistency model: the one where you, by necessity, need to give up availability in order to prevent more than a single copy of the data from being active. As Brewer himself points out, the “2 out of 3” interpretation is misleading. If you take away just one idea from this discussion, let it be this: “consistency” is not a singular, unambiguous property. Remember: ACID consistency != CAP consistency != Oatmeal consistency Instead, a consistency model is a guarantee - any guarantee - that a data store gives to programs that use it. Consistency modela contract between programmer and system, wherein the system guarantees that if the programmer follows some specific rules, the results of operations on the data store will be predictable The “C” in CAP is “strong consistency”, but “consistency” is not a synonym for “strong consistency”. CAP中的一致性指的是强一致性，但一致性并不代表强一致性**，一致性没有单一、明确的属性 Let’s take a look at some alternative consistency models. Strong consistency vs. other consistency models一致性模型（强一致性和其它一致性）Consistency models can be categorized into two types: strong and weak consistency models: Strong consistency models (capable of maintaining a single copy) Linearizable consistency Sequential consistency Weak consistency models (not strong) Client-centric consistency models Causal consistency: strongest model available Eventual consistency models Strong consistency models guarantee that the apparent order and visibility of updates is equivalent to a non-replicated system. Weak consistency models, on the other hand, do not make such guarantees. Note that this is by no means an exhaustive list. Again, consistency models are just arbitrary contracts between the programmer and system, so they can be almost anything. Strong consistency modelsStrong consistency models can further be divided into two similar, but slightly different consistency models: Linearizable consistency: Under linearizable consistency, all operations appear to have executed atomically in an order that is consistent with the global real-time ordering of operations. (Herlihy &amp; Wing, 1991) Sequential consistency: Under sequential consistency, all operations appear to have executed atomically in some order that is consistent with the order seen at individual nodes and that is equal at all nodes. (Lamport, 1979) 一致性模型能被分为两类：强一致性模型和弱一致性模型 强一致性模型（数据维持一份） 线性一致 顺序一致 弱一致性模型 客户为中心的一致性模型 因果一致性可用最强模型 最终一致性模型 The key difference is that linearizable consistency requires that the order in which operations take effect is equal to the actual real-time ordering of operations. Sequential consistency allows for operations to be reordered as long as the order observed on each node remains consistent. The only way someone can distinguish between the two is if they can observe all the inputs and timings going into the system; from the perspective of a client interacting with a node, the two are equivalent. The difference seems immaterial, but it is worth noting that sequential consistency does not compose. Strong consistency models allow you as a programmer to replace a single server with a cluster of distributed nodes and not run into any problems. All the other consistency models have anomalies (compared to a system that guarantees strong consistency), because they behave in a way that is distinguishable from a non-replicated system. But often these anomalies are acceptable, either because we don’t care about occasional issues or because we’ve written code that deals with inconsistencies after they have occurred in some way. Note that there really aren’t any universal typologies for weak consistency models, because “not a strong consistency model” (e.g. “is distinguishable from a non-replicated system in some way”) can be almost anything. 强一致性模型保证数据更新的表现和顺序像无复制系统一样，弱一致性模型则不保证 强一致性模型两种有轻微差别的形式： 线性一致性：所有操作都是有序进行的，按照全局真实时间操作顺序 顺序一致性：所有操作也是有序进行的，任一单独节点操作顺序与其它节点一致 不同的是，线性一致性模型要求操作按照全局真实时间顺序来，而顺序一致性模型只要节点操作被观察到的顺序是一致的就行 强一致性模型能够允许你的单服务程序一直到分布式节点集群上并且不会发生任何错误 相比于强一致性模型，其它的一致性模型都会存在一些异常，但是它们允许异常发生，或者写了处理异常的代码 Client-centric consistency models 客户为中心的一致性模型Client-centric consistency models are consistency models that involve the notion of a client or session in some way. For example, a client-centric consistency model might guarantee that a client will never see older versions of a data item. This is often implemented by building additional caching into the client library, so that if a client moves to a replica node that contains old data, then the client library returns its cached value rather than the old value from the replica. Clients may still see older versions of the data, if the replica node they are on does not contain the latest version, but they will never see anomalies where an older version of a value resurfaces (e.g. because they connected to a different replica). Note that there are many kinds of consistency models that are client-centric. 客户为中心的一致性模型是一种以某种方式涉及客户或会话概念的模型。举例来说，一个以客户为中心的一致性模型可以保证单个用户看到的数据版本永远是最新的版本。通常通过建立缓存到客户端库，使得如果一个用户移动到一个包含了老版本数据的复制的节点时，通过客户端库能够返回数据的缓存值而不是复制集的老版本。 如果复制节点没有包含最新的数据版本，客户仍然会看到数据的老版本，但是客户永远不会看到旧版本的值重现的异常情况。 Eventual consistency 最终一致性The eventual consistency model says that if you stop changing values, then after some undefined amount of time all replicas will agree on the same value. It is implied that before that time results between replicas are inconsistent in some undefined manner. Since it is trivially satisfiable (liveness property only), it is useless without supplemental information. 最终一致性模型中，当停止改变数值的一段不确定的时间后，所有的复制集将会最终保持一致。这表明，在这段时间之前，复制集在某种情形下是不一致的。由于这个条件非常容易满足（只用保证liveness），因此没有补充信息它是没有用的。经过一定的时间，各个复制集最终保持一致 Saying something is merely eventually consistent is like saying “people are eventually dead”. It’s a very weak constraint, and we’d probably want to have at least some more specific characterization of two things: 说一个事情最终能够保证一致就像是说”人总有一死”,这是一个很弱的约束条件。这里至少需要加一些比较明确的特征来形容它： First, how long is “eventually”? It would be useful to have a strict lower bound, or at least some idea of how long it typically takes for the system to converge to the same value. Second, how do the replicas agree on a value? A system that always returns “42” is eventually consistent: all replicas agree on the same value. It just doesn’t converge to a useful value since it just keeps returning the same fixed value. Instead, we’d like to have a better idea of the method. For example, one way to decide is to have the value with the largest timestamp always win. have a strict lower bound 首先，“最终保持一致”的最终到底是多久？这个需要有一个明确严格的上限，或者至少要知道通常系统达到一致结果时所需要的时间 replicas agree on a value 其次，复制集最终是如何到达一致的？一个总是返回固定值“42”的系统能保证最终一致性：所有的复制集都为同一值。但这个系统仅仅是范围一个固定的数值，并非收敛于同一个有用值。这不是我们想要的。我们希望能有一个使得复制集最终保持一致的好方法，例如，使用最大时间限的值作为最终值。这里系统需要达到一致的目标，而不是固定的返回某一个值 So when vendors say “eventual consistency”, what they mean is some more precise term, such as “eventually last-writer-wins, and read-the-latest-observed-value in the meantime” consistency. The “how?” matters, because a bad method can lead to writes being lost - for example, if the clock on one node is set incorrectly and timestamps are used. 所以，当提到”最终一致“时，它实际上想表达的是更精确的术语，类似于想表达”最后的写操作为最终值，与此同时，读操作读取最新的数据“这样的一致性。这里我们最需要关注的是”如何到达最终一致“中的”如何“，因为一个坏的方法可能会使得写操作的数据丢失，例如：如果某个节点上的时钟是不正确的，但是它的时间戳却仍然被使用，这个时候可能会造成写失误。 I will look into these two questions in more detail in the chapter on replication methods for weak consistency models. 关于这两点，在复制方法章节会进行更加详细地介绍。 Further reading Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services - Gilbert &amp; Lynch, 2002 Impossibility of distributed consensus with one faulty process - Fischer, Lynch and Patterson, 1985 Perspectives on the CAP Theorem - Gilbert &amp; Lynch, 2012 CAP Twelve Years Later: How the “Rules” Have Changed - Brewer, 2012 Uniform consensus is harder than consensus - Charron-Bost &amp; Schiper, 2000 Replicated Data Consistency Explained Through Baseball - Terry, 2011 Life Beyond Distributed Transactions: an Apostate’s Opinion - Helland, 2007 If you have too much data, then ‘good enough’ is good enough - Helland, 2011 Building on Quicksand - Helland &amp; Campbell, 2009]]></content>
      <categories>
        <category>微服务理论文章阅读学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]Distributed systems for fun and profit_0引言&1基本概念]]></title>
    <url>%2F2019%2F04%2F21%2F%5B%E8%AF%91%5DDistributed%20systems%20for%20fun%20and%20profit_0%E5%BC%95%E8%A8%80%261%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[零.Introduction 引言I wanted a text that would bring together the ideas behind many of the more recent distributed systems - systems such as Amazon’s Dynamo, Google’s BigTable and MapReduce, Apache’s Hadoop and so on. 我想写一篇能够说明目前一些分布式系统，例如亚马逊的Dynamo，谷歌的Big Table和Map Reduce，Apache的Hadoop背后的一些原理的文章 In this text I’ve tried to provide a more accessible introduction to distributed systems. To me, that means two things: introducing the key concepts that you will need in order to have a good time reading more serious texts, and providing a narrative that covers things in enough detail that you get a gist of what’s going on without getting stuck on details. It’s 2013, you’ve got the Internet, and you can selectively read more about the topics you find most interesting. 在这篇文章中我会尽力更简单易懂的介绍分布式系统，不关注概念背后的具体细节 In my view, much of distributed programming is about dealing with the implications of two consequences of distribution: that information travels at the speed of light that independent things fail independently* 在我看来，多数的分布式程序都是为了解决分布式导致的两个问题： 信息光速传播 独立事务独立失败 具体怎么理解呢？信息的传输速度是光速，虽然已经很快了，但是还是需要时间的，另一个系统要处理的错误不仅一类，而是会很多，而且这些错误之间没有联系，彼此独立。 In other words, that the core of distributed programming is dealing with distance (duh!) and having more than one thing (duh!). These constraints define a space of possible system designs, and my hope is that after reading this you’ll have a better sense of how distance, time and consistency models interact. 换句话而言，分布式程序的核心就是处理距离带来的问题和处理多个问题。这两方面的约束定义了一系列设计规则。希望通过这篇文章，你能更好的理解距离、时间和一致性模型是怎样相互影响的 This text is focused on distributed programming and systems concepts you’ll need to understand commercial systems in the data center. It would be madness to attempt to cover everything. You’ll learn many key protocols and algorithms (covering, for example, many of the most cited papers in the discipline), including some new exciting ways to look at eventual consistency that haven’t still made it into college textbooks - such as CRDTs and the CALM theorem. 文章会介绍一些主要的协议和算法，包括一些新的怎么去保证最终一致性的方法，比如CRDTs和CALM理论 I hope you like it! If you want to say thanks, follow me on Github (or Twitter). And if you spot an error, file a pull request on Github. 1. Basics 基本概念The first chapter covers distributed systems at a high level by introducing a number of important terms and concepts. It covers high level goals, such as scalability, availability, performance, latency and fault tolerance; how those are hard to achieve, and how abstractions and models as well as partitioning and replication come into play. 第一节会介绍分布式系统的一些重要术语和概念。包括可扩展性、实用性、性能、延迟和容错性，及这些实施起来的难度，并引入抽象和模型比如分区和复制的设计规则 2. Up and down the level of abstraction一系列的抽象来描述系统的特征The second chapter dives deeper into abstractions and impossibility results. It starts with a Nietzsche quote, and then introduces system models and the many assumptions that are made in a typical system model. It then discusses the CAP theorem and summarizes the FLP impossibility result. It then turns to the implications of the CAP theorem, one of which is that one ought to explore other consistency models. A number of consistency models are then discussed. 第二节会关注抽象和不可能的结果。首先会介绍尼采的引言，然后会从各种典型的系统模型的假设来介绍分布式系统模型，接着会讨论CAP原理以及FLP不可能结果。最后会介绍CAP原理怎样实施。还会讨论许多一致性的模型 3. Time and order 时间和顺序A big part of understanding distributed systems is about understanding time and order. To the extent that we fail to understand and model time, our systems will fail. The third chapter discusses time and order, and clocks as well as the various uses of time, order and clocks (such as vector clocks and failure detectors). 理解分布式系统的很重要的一点，需要了解时间和顺序。错误理解模型的时间，系统也不能很好的理解。第三节将讨论时间、顺序和时钟及他们的应用 4. Replication: preventing divergence 复制：强一致性The fourth chapter introduces the replication problem, and the two basic ways in which it can be performed. It turns out that most of the relevant characteristics can be discussed with just this simple characterization. Then, replication methods for maintaining single-copy consistency are discussed from the least fault tolerant (2PC) to Paxos. 第四节将讨论分布式系统中的复制问题，以及介绍两种主要的复制方式。将从最小容错到Paxos来论述保持单拷贝一致性的复制方法 5. Replication: accepting divergence 复制：弱一致性The fifth chapter discussed replication with weak consistency guarantees. It introduces a basic reconciliation scenario, where partitioned replicas attempt to reach agreement. It then discusses Amazon’s Dynamo as an example of a system design with weak consistency guarantees. Finally, two perspectives on disorderly programming are discussed: CRDTs and the CALM theorem. 第五节将讨论保持弱一致性的复制。首先介绍分散复制集如何达到最终一致，然后以亚马逊的Dynamo系统作为一个例子，来讨论怎样设计一个保证弱一致性的分布式系统。最后，介绍了CRDTs和CALM理论 6. AppendixThe appendix covers recommendations for further reading. *: This is a lie. This post by Jay Kreps elaborates. 一.Distributed systems at a high level 分布式系统概览 Distributed programming is the art of solving the same problem that you can solve on a single computer using multiple computers.分布式编程就是使用多台计算机解决单机问题 There are two basic tasks that any computer system needs to accomplish: storage and computation 分布式系统主要是为了提高两个方面的能力 存储能力 计算能力 Distributed programming is the art of solving the same problem that you can solve on a single computer using multiple computers - usually, because the problem no longer fits on a single computer. 分布式编程就是使用多台计算机解决单机问题-通常是因为问题已经不适合使用单机环境解决 Nothing really demands that you use distributed systems. Given infinite money and infinite R&amp;D time, we wouldn’t need distributed systems. All computation and storage could be done on a magic box - a single, incredibly fast and incredibly reliable system that you pay someone else to design for you. 如果有足够的钱和无穷的等待回复时间，那分布式系统就没有存在的必要。所有的计算和存储都能在一个单机上实现 However, few people have infinite resources. Hence, they have to find the right place on some real-world cost-benefit curve. At a small scale, upgrading hardware is a viable strategy. However, as problem sizes increase you will reach a point where either the hardware upgrade that allows you to solve the problem on a single node does not exist, or becomes cost-prohibitive. At that point, I welcome you to the world of distributed systems. 但实际上不会存在无尽的资源。因此，我们必须了解真实存在的花费-收益曲线。在小范围内，升级硬件资源能够带来更多的收益。但是随着规模的扩大，光靠升级一个单节点上的硬件来解决问题的成本花费是及其高的。基于此，分布式系统的存在就很有必要性 It is a current reality that the best value is in mid-range, commodity hardware - as long as the maintenance costs can be kept down through fault-tolerant software. 在现实情况中，最好的结果是拥有中等程度的硬件，同时维护成本能够通过具有容错性的软件来降低 Computations primarily benefit from high-end hardware to the extent to which they can replace slow network accesses with internal memory accesses. The performance advantage of high-end hardware is limited in tasks that require large amounts of communication between nodes. 高端硬件带来计算的主要好处在于，从内存直接访问数据速度会比网络获取快得多，能够降低访问速度。但是当节点之间需要进行大量的通信时，高端硬件的优点会受到限制 As the figure above from Barroso, Clidaras &amp; Hölzle shows, the performance gap between high-end and commodity hardware decreases with cluster size assuming a uniform memory access pattern across all nodes. 图：不同粒度的集群下，高端硬件（128-core SMP）和低端硬件（4-core SMP）构建同样的处理核的性能对比 所有节点都采用统一的内存访问模式时，随着集群大小的增加，高端硬件和普通硬件之间的性能差距会减小。 Ideally, adding a new machine would increase the performance and capacity of the system linearly. But of course this is not possible, because there is some overhead that arises due to having separate computers. Data needs to be copied around, computation tasks have to be coordinated and so on. This is why it’s worthwhile to study distributed algorithms - they provide efficient solutions to specific problems, as well as guidance about what is possible, what the minimum cost of a correct implementation is, and what is impossible. 理想情况下，增加一个新的机器会使得系统的性能线性增长。但是这显然是不现实的，因为由于计算机的分离会带来一些难以克服的问题：数据需要被复制到各个节点上，需要协作节点上的计算任务等。这也是为什么分布式算法值得花时间学习-分布式算法能为这些特定问题提供有效的解决方案，同时指引我们什么是可能的，正确的实现解决方案需要的最小代价是什么,以及什么是不可能的 The focus of this text is on distributed programming and systems in a mundane, but commercially relevant setting: the data center. For example, I will not discuss specialized problems that arise from having an exotic network configuration, or that arise in a shared-memory setting. Additionally, the focus is on exploring the system design space rather than on optimizing any specific design - the latter is a topic for a much more specialized text. 本文关注系统的设计，不进行细节优化的探讨 What we want to achieve: Scalability and other good things我们的目标：扩展性和其他好的效果The way I see it, everything starts with the need to deal with size. 在我看来,分布式系统的一切都是和系统规模做斗争 Most things are trivial at a small scale - and the same problem becomes much harder once you surpass a certain size, volume or other physically constrained thing. It’s easy to lift a piece of chocolate, it’s hard to lift a mountain. It’s easy to count how many people are in a room, and hard to count how many people are in a country. 任何问题在小范围内都很容易,当超过了一定规模，问题将变得难以解决 So everything starts with size - scalability. Informally speaking, in a scalable system as we move from small to large, things should not get incrementally worse. Here’s another definition: Scalability is the ability of a system, network, or process, to handle a growing amount of work in a capable manner or its ability to be enlarged to accommodate that growth. 扩展性：一个系统、网络或进程适应任务量增长的能力，增长后性能不会受很大的影响 What is it that is growing? Well, you can measure growth in almost any terms (number of people, electricity usage etc.). But there are three particularly interesting things to look at: Size scalability: adding more nodes should make the system linearly faster; growing the dataset should not increase latency Geographic scalability: it should be possible to use multiple data centers to reduce the time it takes to respond to user queries, while dealing with cross-data center latency in some sensible manner. Administrative scalability: adding more nodes should not increase the administrative costs of the system (e.g. the administrators-to-machines ratio). 增长关注的三个维度： 尺寸可扩展：增加节点使系统线性增长，数据增大不会加大延迟 地理可扩展：多数据中心能够被用来减少反馈用户查询命任务的响应时间，同时能够处理因多中心带来的延迟 管理可扩展：增加节点的同时，不应该增加系统在管理节点上的开销 Of course, in a real system growth occurs on multiple different axes simultaneously; each metric captures just some aspect of growth. 在真实的系统中，增长往往同时发生在多个不同的维度上；每个度量标准仅仅表述的是某个方面的增长 A scalable system is one that continues to meet the needs of its users as scale increases. There are two particularly relevant aspects - performance and availability - which can be measured in various ways. 一个可扩展性的系统随着尺度的增长，仍然能够满足用户的需求。性能和可用性通常用来衡量系统是否能够满足用户需求。 Performance (and latency) 性能（和延迟） Performance is characterized by the amount of useful work accomplished by a computer system compared to the time and resources used.性能：任务所花费的时间和资源 Depending on the context, this may involve achieving one or more of the following: Short response time/low latency for a given piece of work High throughput (rate of processing work) Low utilization of computing resource(s) 好性能的系统通常需要满足以下三点： 低响应时间 高吞吐量（生产力，即工作进程速率） 低计算资源利用率 There are tradeoffs involved in optimizing for any of these outcomes. For example, a system may achieve a higher throughput by processing larger batches of work thereby reducing operation overhead. The tradeoff would be longer response times for individual pieces of work due to batching. 想要优化这些结果，系统需要进行权衡和折中处理。比如：一个系统通过处理大批次的工作可能拥有很高的吞吐量，但是由于个体间的独立工作会导致响应时间变长。 I find that low latency - achieving a short response time - is the most interesting aspect of performance, because it has a strong connection with physical (rather than financial) limitations. It is harder to address latency using financial resources than the other aspects of performance. 同时，低延迟（短的响应时间）是性能表现指标中最有意思的，它很大程度上是受物理分布的限制，而与经济条件限制无关。你很难通过利用更好的硬件资源来减少分布式系统的延迟 There are a lot of really specific definitions for latency, but I really like the idea that the etymology of the word evokes: LatencyThe state of being latent; delay, a period between the initiation of something and the occurrence.延迟性：事务从发生开始到产生具象的时长 And what does it mean to be “latent”? LatentFrom Latin latens, latentis, present participle of lateo (“lie hidden”). Existing or present but concealed or inactive. This definition is pretty cool, because it highlights how latency is really the time between when something happened and the time it has an impact or becomes visible. For example, imagine that you are infected with an airborne virus that turns people into zombies. The latent period is the time between when you became infected, and when you turn into a zombie. That’s latency: the time during which something that has already happened is concealed from view. 假设一个分布式系统在处理一个高级别的任务：给定一个查询，需要读取系统的各个节点的数据最终计算返回一个值 Let’s assume for a moment that our distributed system does just one high-level task: given a query, it takes all of the data in the system and calculates a single result. In other words, think of a distributed system as a data store with the ability to run a single deterministic computation (function) over its current content: result = query(all data in the system) 结果=查询（系统的所有数据） Then, what matters for latency is not the amount of old data, but rather the speed at which new data “takes effect” in the system. For example, latency could be measured in terms of how long it takes for a write to become visible to readers. 影响系统延迟性的并不在于数据的多少，而在于系统处理数据到返回结果的速度。具体而言，延迟性应该是系统能够返回一个直观结果所需要花费的时长 The other key point based on this definition is that if nothing happens, there is no “latent period”. A system in which data doesn’t change doesn’t (or shouldn’t) have a latency problem. In a distributed system, there is a minimum latency that cannot be overcome: the speed of light limits how fast information can travel, and hardware components have a minimum latency cost incurred per operation (think RAM and hard drives but also CPUs). 在分布式系统中，有一个最小延迟无法避免：信息传播速度(光速)限制，硬件操作的时间最小延迟时长的大小取决于查询语句本身，以及这些信息之间传输的物理距离 How much that minimum latency impacts your queries depends on the nature of those queries and the physical distance the information needs to travel. Availability (and fault tolerance) 可用性（和容错性）The second aspect of a scalable system is availability. Availability the proportion of time a system is in a functioning condition. If a user cannot access the system, it is said to be unavailable.可用性：系统所能处于可用状态的时间比例 Availability = uptime /（uptime + downtime） Distributed systems allow us to achieve desirable characteristics that would be hard to accomplish on a single system. For example, a single machine cannot tolerate any failures since it either fails or doesn’t. 分布式系统建立冗余（组件、服务、数据等方面）来允许部分失败的发生，从而提升它的可用性。 Distributed systems can take a bunch of unreliable components, and build a reliable system on top of them. Systems that have no redundancy can only be as available as their underlying components. Systems built with redundancy can be tolerant of partial failures and thus be more available. It is worth noting that “redundant” can mean different things depending on what you look at - components, servers, datacenters and so on. Formulaically, availability is: Availability = uptime / (uptime + downtime). Availability from a technical perspective is mostly about being fault tolerant. Because the probability of a failure occurring increases with the number of components, the system should be able to compensate so as to not become less reliable as the number of components increases. 可用性从技术角度而言，与系统的容错性相关。例如：当系统的组件数量增多时，系统发生错误的可能性会上升，但一个高可用性的系统应该保证系统的可靠性不会随着组件数量的增多而下降。 For example: Availability % How much downtime is allowed per year? 90% (“one nine”) More than a month 99% (“two nines”) Less than 4 days 99.9% (“three nines”) Less than 9 hours 99.99% (“four nines”) Less than an hour 99.999% (“five nines”) ~ 5 minutes 99.9999% (“six nines”) ~ 31 seconds Availability is in some sense a much wider concept than uptime, since the availability of a service can also be affected by, say, a network outage or the company owning the service going out of business (which would be a factor which is not really relevant to fault tolerance but would still influence the availability of the system). But without knowing every single specific aspect of the system, the best we can do is design for fault tolerance. 可用性的概念比正常运行时间概念更广。例如一个系统如果遭遇断电、或者服务发生中断，这个时候与系统的容错性无关，但是这些情况仍然会影响系统的可用性。但通常而言，系统的容错性越强，可用性越高。提高系统的容错性是我们最需要关注的 What does it mean to be fault tolerant? Fault toleranceability of a system to behave in a well-defined manner once faults occurFault tolerance boils down to this: define what faults you expect and then design a system or an algorithm that is tolerant of them. You can’t tolerate faults you haven’t considered.容错性：系统在错误发生后有明确的处理方式 系统定义发生的错误，并定义相应的处理方法。系统的容错性设计是考虑已想到的故障，没有考虑到的故障，系统是没法容错的 What prevents us from achieving good things?什么阻止我们取得好的效果?Distributed systems are constrained by two physical factors: the number of nodes (which increases with the required storage and computation capacity) the distance between nodes (information travels, at best, at the speed of light) 分布式系统受两个物理因素的限制： 节点数量（当需要更大的存储、计算能力时，节点数会增多） 节点间的距离（信息传播） Working within those constraints: an increase in the number of independent nodes increases the probability of failure in a system (reducing availability and increasing administrative costs) an increase in the number of independent nodes may increase the need for communication between nodes (reducing performance as scale increases) an increase in geographic distance increases the minimum latency for communication between distant nodes (reducing performance for certain operations) 限制： 节点增加，系统发生失败的可能性增加（降低可用性，增加管理的花费） 节点增加，独立节点之间的通信增多（随着尺度增大降低性能） 地理距离增加，最小延迟时长增加（降低特定的操作的性能） Beyond these tendencies - which are a result of the physical constraints - is the world of system design options. Both performance and availability are defined by the external guarantees the system makes. On a high level, you can think of the guarantees as the SLA (service level agreement) for the system: if I write data, how quickly can I access it elsewhere? After the data is written, what guarantees do I have of durability? If I ask the system to run a computation, how quickly will it return results? When components fail, or are taken out of operation, what impact will this have on the system? 性能和可用性都由系统的保证确定的。例如：在SLA中，系统保证如果写数据，那么多长时间能从其它地方访问它？如果让系统进行一个运算，多长时间能返回结果？当组件发生失败时，系统会遭受什么样的影响？ There is another criterion, which is not explicitly mentioned but implied: intelligibility. How understandable are the guarantees that are made? Of course, there are no simple metrics for what is intelligible. 这里有一个不明确但是需要实施的标准：可理解性。也就是这些保证需要能够被理解和明白的。 I was kind of tempted to put “intelligibility” under physical limitations. After all, it is a hardware limitation in people that we have a hard time understanding anything that involves more moving things than we have fingers. That’s the difference between an error and an anomaly - an error is incorrect behavior, while an anomaly is unexpected behavior. If you were smarter, you’d expect the anomalies to occur. Abstractions and models 分布式系统中的抽象和模型This is where abstractions and models come into play. Abstractions make things more manageable by removing real-world aspects that are not relevant to solving a problem. Models describe the key properties of a distributed system in a precise manner. I’ll discuss many kinds of models in the next chapter, such as: System model (asynchronous / synchronous) Failure model (crash-fail, partitions, Byzantine) Consistency model (strong, eventual) 抽象：将现实层面的事务抽象出来，便于更好的管理和处理 模型：精确描述分布式系统中的关键属性 本文主要讨论下面三种类型的模型： 系统模型（异步/同步） 故障模型（crash-fail，分区，Byzantine） 一致性模型（强一致性、最终一致性） A good abstraction makes working with a system easier to understand, while capturing the factors that are relevant for a particular purpose. 好的抽象能使系统的运行更方便理解，同时能捕获与特定目标相关的因素 There is a tension between the reality that there are many nodes and with our desire for systems that “work like a single system”. Often, the most familiar model (for example, implementing a shared memory abstraction on a distributed system) is too expensive. 我们希望多节点上运行的分布式系统能像单系统一样运作。但是通常最熟悉的模型需要花费的代价是很大的，比如在一个分布式系统上实施内存共享 A system that makes weaker guarantees has more freedom of action, and hence potentially greater performance - but it is also potentially hard to reason about. People are better at reasoning about systems that work like a single system, rather than a collection of nodes. 一个实施若保证的分布式系统通常能获得更自由以及更好的操作性能，但同时它会难以推理验证。相比于多节点，我们往往能够很好的理解单系统的工作原理 One can often gain performance by exposing more details about the internals of the system. For example, in columnar storage, the user can (to some extent) reason about the locality of the key-value pairs within the system and hence make decisions that influence the performance of typical queries. Systems which hide these kinds of details are easier to understand (since they act more like single unit, with fewer details to think about), while systems that expose more real-world details may be more performant (because they correspond more closely to reality). 当对系统内部的了解更深时，系统的性能将会越高。举个例子，在列存储技术中，典型的查询语句，用户能够推断键值对所在的位置从而做出影响性能的决定。系统隐藏更多的技术细节，用户更容易理解，但性能会更低。 Several types of failures make writing distributed systems that act like a single system difficult. Network latency and network partitions (e.g. total network failure between some nodes) mean that a system needs to sometimes make hard choices about whether it is better to stay available but lose some crucial guarantees that cannot be enforced, or to play it safe and refuse clients when these types of failures occur. 分布式系统中可能出现的各种问题，使得它像单系统一样运行变得困难。当发生网络延迟和网络分区时，系统将要面临是保证可用性还是保证系统的安全性的取舍 The CAP theorem - which I will discuss in the next chapter - captures some of these tensions. In the end, the ideal system meets both programmer needs (clean semantics) and business needs (availability/consistency/latency). CAP定理会讨论分布式系统中这些问题的取舍关系 一个理想的分布式系统需要满足编程人员的需求（明确的语义）和业务需求（可用性/一致性/延迟性） Design techniques: partition and replicate分布式系统设计的技术：分区和复制The manner in which a data set is distributed between multiple nodes is very important. In order for any computation to happen, we need to locate the data and then act on it. 数据集在多节点中的分配方式非常重要。对于系统的任何计算，我们需要定位数据然后再对其进行操作 There are two basic techniques that can be applied to a data set. It can be split over multiple nodes (partitioning) to allow for more parallel processing. It can also be copied or cached on different nodes to reduce the distance between the client and the server and for greater fault tolerance (replication). 对数据集的操作设计到两个基本的技术： 分区：为了进行并行处理，数据将被分割到多个节点中 复制：为了减少客户端和服务器之间的距离，同时为了提高容错性，数据会被备份或缓存到不同的节点中 Divide and conquer - I mean, partition and replicate. 分而治之-分区和复制 The picture below illustrates the difference between these two: partitioned data (A and B below) is divided into independent sets, while replicated data (C below) is copied to multiple locations. 下图阐述了两者之间的差异：分区数据（A和B）被划分成两个独立的数据集，于此同时复制数据（C）被拷贝到不同的节点上 This is the one-two punch for solving any problem where distributed computing plays a role. Of course, the trick is in picking the right technique for your concrete implementation; there are many algorithms that implement replication and partitioning, each with different limitations and advantages which need to be assessed against your design objectives. 许多分布式的算法都用到了分区和复制。不同的限制条件取决于你设计的目的 Partitioning 分区Partitioning is dividing the dataset into smaller distinct independent sets; this is used to reduce the impact of dataset growth since each partition is a subset of the data. Partitioning improves performance by limiting the amount of data to be examined and by locating related data in the same partition Partitioning improves availability by allowing partitions to fail independently, increasing the number of nodes that need to fail before availability is sacrificed 将数据集分割成相互独立的小数据集，减少因数据集增长而带来对单个节点的压力 提高性能：限制分区中数据量的大小，降低数据压力 提高可用性：数据之间相互独立，不同分区之间失败互不影响，允许失败节点的存在 Partitioning is also very much application-specific, so it is hard to say much about it without knowing the specifics. That’s why the focus is on replication in most texts, including this one. Partitioning is mostly about defining your partitions based on what you think the primary access pattern will be, and dealing with the limitations that come from having independent partitions (e.g. inefficient access across partitions, different rate of growth etc.). 分区需要考虑： 基于将要进行的主要访问模式如何定义分区 怎样处理独立分区带来的局限性（跨分区低效率访问） Replication 复制Replication is making copies of the same data on multiple machines; this allows more servers to take part in the computation. 将同样的数据备份到多个机器当中，这样能够使得更多的服务器参与到计算当中 Let me inaccurately quote Homer J. Simpson: To replication! The cause of, and solution to all of life’s problems. Replication - copying or reproducing something - is the primary way in which we can fight latency. Replication improves performance by making additional computing power and bandwidth applicable to a new copy of the data Replication improves availability by creating additional copies of the data, increasing the number of nodes that need to fail before availability is sacrificed 复制是解决延迟的主要方法之一 提高性能：复制使额外的计算能力和带宽适用于数据的新副本，从而提高了性能 提高可用性：备份数据，允许更多的节点出错，提高容错性 Replication is about providing extra bandwidth, and caching where it counts. It is also about maintaining consistency in some way according to some consistency model. Replication allows us to achieve scalability, performance and fault tolerance. Afraid of loss of availability or reduced performance? Replicate the data to avoid a bottleneck or single point of failure. Slow computation? Replicate the computation on multiple systems. Slow I/O? Replicate the data to a local cache to reduce latency or onto multiple machines to increase throughput. Replication is also the source of many of the problems, since there are now independent copies of the data that has to be kept in sync on multiple machines - this means ensuring that the replication follows a consistency model. 复制能够让我们实现系统的可扩展性和容错性。 避免单节点故障和瓶颈 多系统上复制计算能够加快计算速度 复制数据到本地缓存中能够减少在多个机器上进行运算的延迟时长，同时提高吞吐量 The choice of a consistency model is crucial: a good consistency model provides clean semantics for programmers (in other words, the properties it guarantees are easy to reason about) and meets business/design goals such as high availability or strong consistency. Only one consistency model for replication - strong consistency - allows you to program as-if the underlying data was not replicated. Other consistency models expose some internals of the replication to the programmer. However, weaker consistency models can provide lower latency and higher availability - and are not necessarily harder to understand, just different. 复制来到好处的同时，也带来了很多问题，最大的就是数据一致性问题，只有当模型是 strong consistency 的时候，我们才会得到一个简单的编程模型（和单机系统一致），其他模型我们都好去理解系统内部是怎么做的，这样子才能很好的满足我们的需求。 Further reading The Datacenter as a Computer - An Introduction to the Design of Warehouse-Scale Machines - Barroso &amp; Hölzle, 2008 Fallacies of Distributed Computing Notes on Distributed Systems for Young Bloods - Hodges, 2013]]></content>
      <categories>
        <category>微服务理论文章阅读学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[译]可扩展的网站架构和分布式系统]]></title>
    <url>%2F2019%2F03%2F02%2F%5B%E8%AF%91%5D%E5%8F%AF%E4%BC%B8%E7%BC%A9%E7%9A%84%E7%BD%91%E7%AB%99%E6%9E%B6%E6%9E%84%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[原文地址 参考翻译 Scalable Web Architecture and Distributed Systems可扩展Web架构与分布式系统Open source software has become a fundamental building block for some of the biggest websites. And as those websites have grown, best practices and guiding principles around their architectures have emerged. This chapter seeks to cover some of the key issues to consider when designing large websites, as well as some of the building blocks used to achieve these goals. 开源软件已 经成为构建最大的一些网站的基石.随着这些网站规模的增长,围绕它们的架构出现了许多最佳实践与指导原则.这篇文章旨在涉及一些在设计大型网站时需要考虑的关键问题和一些为达到这些目标所使用的组件 This chapter is largely focused on web systems, although some of the material is applicable to other distributed systems as well. 本章主要讲Web系统，不过一些内容也适用于其他分布式系统 1.1. Principles of Web Distributed Systems Design Web分布式系统设计原则What exactly does it mean to build and operate a scalable web site or application? At a primitive level it’s just connecting users with remote resources via the Internet—the part that makes it scalable is that the resources, or access to those resources, are distributed across multiple servers. 构建和运作可扩展网站或者Web应用，到底意味着什么?说到底这些系统只是通过互联网将用户与远程资源连接而已，它们之所以变成可扩展的，是因为资源或者对资源的访问是跨多个服务器分布的 Like most things in life, taking the time to plan ahead when building a web service can help in the long run; understanding some of the considerations and tradeoffs behind big websites can result in smarter decisions at the creation of smaller web sites. Below are some of the key principles that influence the design of large-scale web systems: 与现实生活中的大多数事情一样，构建Web服务的过程中花些时间预先计划从长远来看是有帮助的。理解了大型网站背后的考虑因素和取舍，开发较小的网站时你就能够做出更明智的决策。影响大规模Web系统设计的一些关键原则如下 Availability: The uptime of a website is absolutely critical to the reputation and functionality of many companies. For some of the larger online retail sites, being unavailable for even minutes can result in thousands or millions of dollars in lost revenue, so designing their systems to be constantly available and resilient to failure is both a fundamental business and a technology requirement. High availability in distributed systems requires the careful consideration of redundancy for key components, rapid recovery in the event of partial system failures, and graceful degradation when problems occur. 可用性 网站的正常运行时间对许多公司的声誉和功能至关重要。对于一些规模较大的在线零售网站，即使几分钟都不可用，也可能导致数千或数百万美元的收入损失，因此，将其系统设计为持续可用并能够抵御失败既是一项基本业务，也是一项技术要求。分布式系统中的高可用性需要仔细考虑关键组件的冗余、在部分系统故障时快速恢复以及出现问题时的优雅降级。 Performance: Website performance has become an important consideration for most sites. The speed of a website affects usage and user satisfaction, as well as search engine rankings, a factor that directly correlates to revenue and retention. As a result, creating a system that is optimized for fast responses and low latency is key. 性能 网站性能已成为大多数网站的重要考虑因素。网站的速度会影响使用率和用户满意度，以及搜索引擎排名，这是一个与收入和留存率直接相关的因素。因此，创建一个针对快速响应和低延迟进行优化的系统是关键。 Reliability: A system needs to be reliable, such that a request for data will consistently return the same data. In the event the data changes or is updated, then that same request should return the new data. Users need to know that if something is written to the system, or stored, it will persist and can be relied on to be in place for future retrieval. 可靠性 一个系统需要可靠，这样对数据的请求将一致地返回相同的数据。如果数据发生更改或更新，则同一请求应返回新数据。用户需要知道，如果某个东西被写入或存储到系统中，它将持续存在，并且将来可以依赖于它的存在。 Scalability: When it comes to any large distributed system, size is just one aspect of scale that needs to be considered. Just as important is the effort required to increase capacity to handle greater amounts of load, commonly referred to as the scalability of the system. Scalability can refer to many different parameters of the system: how much additional traffic can it handle, how easy is it to add more storage capacity, or even how many more transactions can be processed. 可扩展性 提到任何大型分布式系统时，规模只是需要考虑的一个方面。同样重要的是，为了处理更大的负载（通常称为系统的可扩展性），需要增加容量。可扩展性可能是指系统的各个的参数：它可以处理多少额外的流量，增加更多的存储容量有多容易，甚至可以处理多少事务 Manageability: Designing a system that is easy to operate is another important consideration. The manageability of the system equates to the scalability of operations: maintenance and updates. Things to consider for manageability are the ease of diagnosing and understanding problems when they occur, ease of making updates or modifications, and how simple the system is to operate. (I.e., does it routinely operate without failure or exceptions?) 易管理性 设计一个易于操作的系统是另一个重要考虑因素。系统的可管理性等同于操作的可伸缩性：维护和更新。对于可管理性，需要考虑的是在问题发生时诊断和理解问题的容易程度，更新或修改的容易程度，以及系统操作的简单程度。（即，它是否正常运行而无故障或异常？） Cost: Cost is an important factor. This obviously can include hardware and software costs, but it is also important to consider other facets needed to deploy and maintain the system. The amount of developer time the system takes to build, the amount of operational effort required to run the system, and even the amount of training required should all be considered. Cost is the total cost of ownership. 成本 成本是一个重要的因素.很明显它包括硬件和软件成本,但是考虑部署和维护系统所需的其他方面也很重要。系统构建所需的开发人员时间、运行系统所需的操作工作量，甚至所需的培训量都应该考虑在内。成本是总拥有成本。 Each of these principles provides the basis for decisions in designing a distributed web architecture. However, they also can be at odds with one another, such that achieving one objective comes at the cost of another. A basic example: choosing to address capacity by simply adding more servers (scalability) can come at the price of manageability (you have to operate an additional server) and cost (the price of the servers). 这些原则中的每一条都为设计分布式Web体系结构的决策提供了基础。然而，它们之间也可能存在分歧，导致实现一个目标是以另一个目标为代价的。一个基本的例子：选择通过简单地添加更多的服务器来解决容量问题（可伸缩性）可能要以可管理性（必须操作一个额外的服务器）和成本（服务器的价格）为代价。 1.2. The Basics 基本原理When it comes to system architecture there are a few things to consider: what are the right pieces, how these pieces fit together, and what are the right tradeoffs. Investing in scaling before it is needed is generally not a smart business proposition; however, some forethought into the design can save substantial time and resources in the future. 当提到系统体系结构时，有几个问题需要考虑：什么是正确的组件，这些组件如何协作，需要做哪些正确的权衡。在真正需要可扩展性之前就花时间和精力投入其中从商业上来看不是一个明智的建议；但是，一些设计中的远见卓识将会在未来节省大量的时间和资源。 This section is focused on some of the core factors that are central to almost all large web applications: services, redundancy, partitions, and handling failure. Each of these factors involves choices and compromises, particularly in the context of the principles described in the previous section. In order to explain these in detail it is best to start with an example. 本节着眼于一些对于几乎所有大型Web应用都非常核心的要素：服务，冗余，分区和失败处理。每个要素均牵涉到选择和妥协，特别是在上一节提到的准则上下文中。为了更好的说明我们从一个例子开始。 Example: Image Hosting Application 举例:图片托管应用At some point you have probably posted an image online. For big sites that host and deliver lots of images, there are challenges in building an architecture that is cost-effective, highly available, and has low latency (fast retrieval). 你很可能已经在网上上传过一张图片。对于托管和传输大量图片的大型网站来说，构建一个低成本、高可用性和低延迟（能够快速检索）的体系结构存在挑战。 Imagine a system where users are able to upload their images to a central server, and the images can be requested via a web link or API, just like Flickr or Picasa. For the sake of simplicity, let’s assume that this application has two key parts: the ability to upload (write) an image to the server, and the ability to query for an image. While we certainly want the upload to be efficient, we care most about having very fast delivery when someone requests an image (for example, images could be requested for a web page or other application). This is very similar functionality to what a web server or Content Delivery Network (CDN) edge server (a server CDN uses to store content in many locations so content is geographically/physically closer to users, resulting in faster performance) might provide. 设想一个这样的系统：用户可以将他们的图片上传到一个中央服务器，并且图片可以通过web链接或者API（应用程序接口）进行请求，就像Flickr或者Picasa一样。为了简单起见，我们假定这个应用有两个关键部分：能够上传（写入）一张图片到服务器，能够查询一张图片。虽然我们希望上传能够更快速，但我们最关心的是系统能够快速分发用户请求的图片（比如图片可以被请求用于一张网页或是其他应用）。这些跟一个web服务器或者CDN（内容分发网络） edge server（CDN所使用的服务器，用于在很多位置存放内容，这样内容在地理/物理上更接近用户，起到更高性能的作用）所提供的功能非常类似。 Other important aspects of the system are: There is no limit to the number of images that will be stored, so storage scalability, in terms of image count needs to be considered. There needs to be low latency for image downloads/requests. If a user uploads an image, the image should always be there (data reliability for images). The system should be easy to maintain (manageability). Since image hosting doesn’t have high profit margins, the system needs to be cost-effective Figure 1.1 is a simplified diagram of the functionality. 系统的其他重要方面包括： 对于存储的图片数量没有设限，所以就图片数量而言，需要考虑存储的可扩展性。 图片下载/请求需要做到低延迟。 如果一个用户上传了一张图片，那该图片应该总是存在的（图像的数据可靠性）。 系统应该易于维护（可管理性）。 由于图片托管没有很高的利润率，系统需要低成本。 图1.1是功能的简化图。 Figure 1.1: Simplified architecture diagram for image hosting application 图1.1：图像托管应用程序的简化架构图 In this image hosting example, the system must be perceivably fast, its data stored reliably and all of these attributes highly scalable. Building a small version of this application would be trivial and easily hosted on a single server; however, that would not be interesting for this chapter. Let’s assume that we want to build something that could grow as big as Flickr. 在这个图片托管示例中，系统必须做到（让用户）可感知到快速，存储数据的可靠性和那些所有高可扩展性的特征。构建一个小型的托管于单台服务器上的应用过于简单，也没有意义，对于本章来说也没有乐趣所在。来假设下，我们想要构建出可以成长为像Flickr一样的庞然大物。 Services 服务When considering scalable system design, it helps to decouple functionality and think about each part of the system as its own service with a clearly defined interface. In practice, systems designed in this way are said to have a Service-Oriented Architecture (SOA). For these types of systems, each service has its own distinct functional context, and interaction with anything outside of that context takes place through an abstract interface, typically the public-facing API of another service. 在考虑可伸缩系统设计时，使用明确定义的接口有助于将功能解耦，并将系统的每个部分视为一个服务。在实践中，以这种方式设计的系统被称为面向服务的体系结构（SOA）。对于这些类型的系统，每个服务都有它们各自确切的功能上下文，并且和该上下文以外的任何交互均是与一个抽象的接口（通常是另一个服务暴露的API）进行的。 Deconstructing a system into a set of complementary services decouples the operation of those pieces from one another. This abstraction helps establish clear relationships between the service, its underlying environment, and the consumers of that service. Creating these clear delineations can help isolate problems, but also allows each piece to scale independently of one another. This sort of service-oriented design for systems is very similar to object-oriented design for programming. 将一个系统分解成一组互补的服务，可以使服务彼此解耦。这种抽象有助于在服务、服务的底层(运行)环境和该服务的消费者之间建立清晰的关系。创建这些清晰的描述有助于隔离问题，并且也允许每个部分独立地进行扩展。这种面向服务的系统设计与面向对象的程序设计非常相似。 In our example, all requests to upload and retrieve images are processed by the same server; however, as the system needs to scale it makes sense to break out these two functions into their own services. 在我们的示例中，上传和检索图像的所有请求都是由同一服务器处理的；但是，由于系统需要进行扩展，有必要将这两个功能分解为独立的服务。 Fast-forward and assume that the service is in heavy use; such a scenario makes it easy to see how longer writes will impact the time it takes to read the images (since they two functions will be competing for shared resources). Depending on the architecture this effect can be substantial. Even if the upload and download speeds are the same (which is not true of most IP networks, since most are designed for at least a 3:1 download-speed:upload-speed ratio), read files will typically be read from cache, and writes will have to go to disk eventually (and perhaps be written several times in eventually consistent situations). Even if everything is in memory or read from disks (like SSDs), database writes will almost always be slower than reads. (Pole Position, an open source tool for DB benchmarking, http://polepos.org/ and results http://polepos.sourceforge.net/results/PolePositionClientServer.pdf.). 快进下，假设服务正在大量使用；这样的场景很容易观测到写入时间多长时将影响读取图片所需的时间（因为这两个功能将争夺共享资源）。在这样的架构下，这种影响是真实存在的。即使上传和下载速度相同（大多数IP网络并非如此，因为大多数都是设计成下载速度与上传速度3:1的比例），文件通常直接从缓存中读取，而写入则最终必须到达磁盘（在最终一致的场景中可能会被写入多次）。即使所有东西都是从内存或者磁盘（比如SSD固态硬盘）读取，数据库的写入操作几乎总是比读取要慢。（Pole Position，一个用于数据库基准测试的开源工具，http://polepos.org/和结果http://polepos.sourceforge.net/results/polepositionclientserver.pdf.）。 Another potential problem with this design is that a web server like Apache or lighttpd typically has an upper limit on the number of simultaneous connections it can maintain (defaults are around 500, but can go much higher) and in high traffic, writes can quickly consume all of those. Since reads can be asynchronous, or take advantage of other performance optimizations like gzip compression or chunked transfer encoding, the web server can switch serve reads faster and switch between clients quickly serving many more requests per second than the max number of connections (with Apache and max connections set to 500, it is not uncommon to serve several thousand read requests per second). Writes, on the other hand, tend to maintain an open connection for the duration for the upload, so uploading a 1MB file could take more than 1 second on most home networks, so that web server could only handle 500 such simultaneous writes. 这种设计的另一个潜在问题是，像Apache 或lighttpd这样的Web服务器,通常有一个它可以维持并发连接数的上线（默认大约在500左右，但可以调得更高），并且在高流量下，写操作将很快消耗完所有（连接资源）。不过由于读操作可以异步进行，或者利用其它性能优化手段如gzip压缩或者分块传输编码，Web服务器可以更快地切换服务读操作、更快切换客户端，从而比最大连接数每秒服务更多的请求（Apache和最大连接数设置为500，但一般都能每秒服务数千个请求）。另一方面，上传时写入操作往往会保持一个打开状态的连接，所以上传一个1M大小的文件在大多数家庭网络上将花费超过1秒的时间，因此Web服务器只能同时处理500个写入操作。 Figure 1.2: Splitting out reads and writes 图1.2：读写分离 Planning for this sort of bottleneck makes a good case to split out reads and writes of images into their own services, shown in Figure 1.2. This allows us to scale each of them independently (since it is likely we will always do more reading than writing), but also helps clarify what is going on at each point. Finally, this separates future concerns, which would make it easier to troubleshoot and scale a problem like slow reads. 将图片的读、写操作拆分成各自的服务是一个应对这种瓶颈很好的解决方案，如图1.2所示。我们能够独立的扩展它们（一般读总是多余写），同时也有助于理清每一个服务中发生的事情。最后，这样做分离了未来的隐患，可以更简单地解决像读操作缓慢的问题，并做到可伸缩。。 The advantage of this approach is that we are able to solve problems independently of one another—we don’t have to worry about writing and retrieving new images in the same context. Both of these services still leverage the global corpus of images, but they are free to optimize their own performance with service-appropriate methods (for example, queuing up requests, or caching popular images—more on this below). And from a maintenance and cost perspective each service can scale independently as needed, which is great because if they were combined and intermingled, one could inadvertently impact the performance of the other as in the scenario discussed above. 这种方法的优点在于我们能够独立（不影响其他）解决问题——我们不用担心在同一上下文中写入、读取新的图片。这两者（服务）仍然影响着全部的图片，但均能通过service-appropriate方法优化它们的性能（比如让请求排队，或者缓存热点图片——更多种方式请见下文）。从一个维护和成本的角度来看，每个服务均能独立、按需扩展是非常好的，因为如上面讨论的场景中所讨论的那样,如果它们被组合、混合在一起，可能某一（服务）不经意间就会影响到其他（服务）的性能。 Of course, the above example can work well when you have two different endpoints (in fact this is very similar to several cloud storage providers’ implementations and Content Delivery Networks). There are lots of ways to address these types of bottlenecks though, and each has different tradeoffs. 当然，当你考虑着两个不同点时，上面的例子能够工作得很好（事实上，这跟一些云存储提供商的实现方案和CDN很类似）。尽管还有很多方法来处理这些类型的瓶颈，但每个都有不同方面的权衡。 For example, Flickr solves this read/write issue by distributing users across different shards such that each shard can only handle a set number of users, and as users increase more shards are added to the cluster (see the presentation on Flickr’s scaling,http://mysqldba.blogspot.com/2008/04/mysql-uc-2007-presentation-file.html). In the first example it is easier to scale hardware based on actual usage (the number of reads and writes across the whole system), whereas Flickr scales with their user base (but forces the assumption of equal usage across users so there can be extra capacity). In the former an outage or issue with one of the services brings down functionality across the whole system (no-one can write files, for example), whereas an outage with one of Flickr’s shards will only affect those users. In the first example it is easier to perform operations across the whole dataset—for example, updating the write service to include new metadata or searching across all image metadata—whereas with the Flickr architecture each shard would need to be updated or searched (or a search service would need to be created to collate that metadata—which is in fact what they do). 例如，Flickr通过将用户分布在不同的分片中的方法来解决这个读/写问题，这样每个分片只能处理一组用户，并且随着用户数量的增加，更多的分片会添加到集群中（请参见关于Flickr系统扩展的演示文稿，http://mysqldba.blogspot.com/2008/04/mysql-uc-2007-presentation-file.html）。在第一个例子中，根据实际使用情况（整个系统的读写次数）扩展硬件更加容易，而Flickr则根据用户基数来扩展（但强制假定用户之间使用相同，所以可能会有额外的容量）。在前一种情况下，某个服务中断或发生问题会降低整个系统的功能性（例如，没有人可以写入文件），然而Flickr的一个分片的中断只会影响该分片的用户。。第一个例子易于操作整个数据集，比如升级写入服务来包含新的元数据或者搜索所有的图片元数据，然而在Flickr的架构下，每个分片均需要被更新或搜索（或者需要创建搜索服务以对元数据进行排——事实上他们确实这么做） When it comes to these systems there is no right answer, but it helps to go back to the principles at the start of this chapter, determine the system needs (heavy reads or writes or both, level of concurrency, queries across the data set, ranges, sorts, etc.), benchmark different alternatives, understand how the system will fail, and have a solid plan for when failure happens. 对于这些系统来说没有孰对孰错，而是帮助我们回到本章开头所说的准则，判断系统需求（读多还是写多还是两者都多，并发程度，跨数据集查询，搜索，排序等），检测不同的取舍，理解系统为什么会失败并且有可靠的计划来应对失败的发生。 Redundancy 冗余In order to handle failure gracefully a web architecture must have redundancy of its services and data. For example, if there is only one copy of a file stored on a single server, then losing that server means losing that file. Losing data is seldom a good thing, and a common way of handling it is to create multiple, or redundant, copies. 为了优雅地处理故障，Web体系结构必须具有服务和数据的冗余。例如，如果一台服务器上只存储了一份文件，那么丢失该服务器就意味着丢失该文件。丢失数据很糟糕，处理它的常见方法是创建多个或冗余的副本。 This same principle also applies to services. If there is a core piece of functionality for an application, ensuring that multiple copies or versions are running simultaneously can secure against the failure of a single node. 同样的原则也适用于服务。如果应用程序有一个核心功能，确保同时运行多个副本或版本可以防止单个节点的故障。 Creating redundancy in a system can remove single points of failure and provide a backup or spare functionality if needed in a crisis. For example, if there are two instances of the same service running in production, and one fails or degrades, the system can failoverto the healthy copy. Failover can happen automatically or require manual intervention. 在系统中创建冗余可以消除单点故障，并提供一个备份或在必要的紧急时刻替换功能。例如，如果在生产环境有同一服务的两个实例在运行，其中一个发生故障或降级了，系统可以（启动）failover到那个健康状态的服务。Failover可以自动发生或者需要人工干预。 Another key part of service redundancy is creating a shared-nothing architecture. With this architecture, each node is able to operate independently of one another and there is no central “brain” managing state or coordinating activities for the other nodes. This helps a lot with scalability since new nodes can be added without special conditions or knowledge. However, and most importantly, there is no single point of failure in these systems, so they are much more resilient to failure. 服务冗余的另一个关键点在于创建一个非共享的架构(即无状态架构)。通过这种架构，每个节点都能够独立操作，并且没有中央“大脑”来管理状态或者协调其他节点的活动。这对于可扩展性非常有帮助，因为新的节点不需要特殊的条件或知识就能加入（到集群）。但是，最重要的是在这些系统中不会存在单点失败问题，所以它们能够更加弹性地面对失败。 For example, in our image server application, all images would have redundant copies on another piece of hardware somewhere (ideally in a different geographic location in the event of a catastrophe like an earthquake or fire in the data center), and the services to access the images would be redundant, all potentially servicing requests. (See Figure 1.3.) (Load balancers are a great way to make this possible, but there is more on that below). 例如，在我们的图片服务应用中，所有的图片会在另一个地方的硬件中有冗余的备份（理想情况是在一个不同的地理位置，以防地震或者数据中心火灾这类的灾难发生），而访问图片的服务同样是冗余的，，所有可能的服务请求都是冗余的。（见图 1.3）（负载均衡器可以将其变为现实，详情请见下文。） Figure 1.3: Image hosting application with redundancy ​ 图1.3 具有冗余的图像托管应用程序 Partitions 分区There may be very large data sets that are unable to fit on a single server. It may also be the case that an operation requires too many computing resources, diminishing performance and making it necessary to add capacity. In either case you have two choices: scale vertically or horizontally. 单台服务器可能没法放下海量数据集。也可能是一个操作需要太多计算资源，消耗性能，使得有必要增加（系统）容量。无论是哪种情况，你都有两种选择：垂直扩展（scale vertically）或者水平扩展（scale horizontally）。 Scaling vertically means adding more resources to an individual server. So for a very large data set, this might mean adding more (or bigger) hard drives so a single server can contain the entire data set. In the case of the compute operation, this could mean moving the computation to a bigger server with a faster CPU or more memory. In each case, vertical scaling is accomplished by making the individual resource capable of handling more on its own. 垂直扩展意味着在单台服务器上增加更多的资源。所以对于大数据来说，这意味着增加更多（更大容量）的硬盘以便让单台服务器能够容纳整个数据集。对于计算操作的场景，这意味着将计算任务交给一台拥有更快CPU或者更多内存的大型服务器。对于每种场景，垂直扩展是通过自身（个体）能够处理更多的方式来达到目标的。 To scale horizontally, on the other hand, is to add more nodes. In the case of the large data set, this might be a second server to store parts of the data set, and for the computing resource it would mean splitting the operation or load across some additional nodes. To take full advantage of horizontal scaling, it should be included as an intrinsic design principle of the system architecture, otherwise it can be quite cumbersome to modify and separate out the context to make this possible. 另一方面，水平扩展就是添加更多的节点。对于大数据集，可能是用另一台服务器来存储部分数据集；而对于计算资源来说，则意味着将操作进行分解或者加载在一些额外的节点上。为了充分利用水平扩展的优势，这（此处指代的是系统支持水平扩展。垂直扩展对于应用来说无需修改，通常升级机器即可达到目的。而水平扩展就要求应用架构能够支持这种方式的扩展，因为数据、服务都是分布式的，需要从软件层面来支持这一特性，从而做到数据、服务的水平可扩展。）应该被天然地包含在系统架构设计准则里，否则想要通过修改、隔离上下文来达到这一点将会相当麻烦。 When it comes to horizontal scaling, one of the more common techniques is to break up your services into partitions, or shards. The partitions can be distributed such that each logical set of functionality is separate; this could be done by geographic boundaries, or by another criteria like non-paying versus paying users. The advantage of these schemes is that they provide a service or data store with added capacity. 对于水平扩展来说，通常方法之一就是将你的服务打散、分区。分区可以是分布式的，这样每个逻辑功能集都是分离的；分区可通过地理边界来划分，或者其他标准如付费/未付费用户。这些设计的好处在于它们能够使得服务或数据存储易于增加容量。 In our image server example, it is possible that the single file server used to store images could be replaced by multiple file servers, each containing its own unique set of images. (See Figure 1.4.) Such an architecture would allow the system to fill each file server with images, adding additional servers as the disks become full. The design would require a naming scheme that tied an image’s filename to the server containing it. An image’s name could be formed from a consistent hashing scheme mapped across the servers. Or alternatively, each image could be assigned an incremental ID, so that when a client makes a request for an image, the image retrieval service only needs to maintain the range of IDs that are mapped to each of the servers (like an index). 在我们的图片服务器例子中，可以将单台存储图片的服务器替换为多台文件服务器，每台保存各自单独的图片集。（见图1.4)这样的架构使得系统能够往各台文件服务器中存入图片，当磁盘快满时再增加额外的服务器。这种设计将需要一种命名机制，将图片的文件名与所在服务器关联起来。一个图片的名字可以通过服务器间一致性Hash机制来生成。或者另一种选择是，可以分配给每张图片一个增量ID，当一个客户端请求一张图片时，图片检索服务只需要维护每台服务器对应的ID区间即可（类似索引）。 Figure 1.4: Image hosting application with redundancy and partitioning 图1.4：具有冗余和分区特性的图片托管应用程序 Of course there are challenges distributing data or functionality across multiple servers. One of the key issues is data locality; in distributed systems the closer the data to the operation or point of computation, the better the performance of the system. Therefore it is potentially problematic to have data spread across multiple servers, as any time it is needed it may not be local, forcing the servers to perform a costly fetch of the required information across the network. 当然，将数据或功能分布在多台服务器上会带来很多挑战。关键问题之一是数据局部性（data locality）；在分布式系统里，数据离操作或者计算点越近，系统性能就越高。因此将数据分布在多台服务器可能是有问题的，任何需要（数据）的时候都可能不在本地，使得服务器必须通过网络来获取所需的信息。 Another potential issue comes in the form of inconsistency. When there are different services reading and writing from a shared resource, potentially another service or data store, there is the chance for race conditions—where some data is supposed to be updated, but the read happens prior to the update—and in those cases the data is inconsistent. For example, in the image hosting scenario, a race condition could occur if one client sent a request to update the dog image with a new title, changing it from “Dog” to “Gizmo”, but at the same time another client was reading the image. In that circumstance it is unclear which title, “Dog” or “Gizmo”, would be the one received by the second client. 另一个潜在问题是不一致性。当不同的服务在对同一块共享资源进行读、写时，可能是另一个服务或者数据，就会存在竞态条件的可能——当一些数据将被更新，但读操作先于更新发生——这类场景下数据就会发生不一致。例如，在图片托管这个场景下，竞争条件会发生在一个客户端发出将小狗图片标题由“Dog”更新为“Gizmo”的请求，但同时另一个客户端正在读取该图片这样的情况下。在这样的情况下，不清楚第二个客户端接收到的标题会是“Dog”还是“Gizmo”。 There are certainly some obstacles associated with partitioning data, but partitioning allows each problem to be split—by data, load, usage patterns, etc.—into manageable chunks. This can help with scalability and manageability, but is not without risk. There are lots of ways to mitigate risk and handle failures; however, in the interest of brevity they are not covered in this chapter. If you are interested in reading more, you can check out my blog post on fault tolerance and monitoring. 诚然，关于数据分区还存在很多阻碍，但分区通过数据、负载、用户使用模式等使得每个问题分解成易处理的部分。这样有助于可扩展性和可管理型，但也不是没有风险的。有很多方法能够用来降低风险和处理故障；但为了简化篇幅，本章就不深入(这些方法）了。如果你有兴趣想了解更多，可以查看我博客上发表的关于容错性和监控的博客文章。 1.3. The Building Blocks of Fast and Scalable Data Access 构建快速和可扩展的数据访问组件Having covered some of the core considerations in designing distributed systems, let’s now talk about the hard part: scaling access to the data. 在讨论了设计分布式系统时的一些核心考虑因素之后，现在让我们来谈谈比较困难的部分：数据访问的可扩展性。 Most simple web applications, for example, LAMP stack applications, look something like Figure 1.5. 大多数简单的web应用程序，例如LAMP技术栈应用程序，看起来像图1.5。 Figure 1.5: Simple web applications As they grow, there are two main challenges: scaling access to the app server and to the database. In a highly scalable application design, the app (or web) server is typically minimized and often embodies a shared-nothing architecture. This makes the app server layer of the system horizontally scalable. As a result of this design, the heavy lifting is pushed down the stack to the database server and supporting services; it’s at this layer where the real scaling and performance challenges come into play. 随着系统的成长，会有两个主要挑战：对应用服务器访问的可扩展性和对数据库访问的可扩展性。在一个高度可扩展的应用程序设计中，应用程序（或Web）服务器通常最小化，并且通常体现为非共享（无状态）架构。这使得系统的应用服务器层可以水平扩展。这种设计的结果是，压力被向下推到了数据库服务器和相关（底层）支持服务；真正的扩展和性能挑战就在这一层起到作用。 The rest of this chapter is devoted to some of the more common strategies and methods for making these types of services fast and scalable by providing fast access to data. 本章余下部分致力于介绍）一些更加通用的策略和方法，通过更快的数据访问使得这些类型的服务更加快速和可扩展。 Figure 1.6: Oversimplified web application图1.6: 极简的web应用程序 Most systems can be oversimplified to Figure 1.6. This is a great place to start. If you have a lot of data, you want fast and easy access, like keeping a stash of candy in the top drawer of your desk. Though overly simplified, the previous statement hints at two hard problems: scalability of storage and fast access of data. 大多数系统可以极度简化为像图1.6这样的。这是一个很好的开始。如果你有大量的数据且希望快速、简单地访问，就像你把糖果藏在你桌子第一个抽屉里。虽然被极度简化，前面的观点仍暗示着两个难题：存储的可扩展性和数据的快速访问。 For the sake of this section, let’s assume you have many terabytes (TB) of data and you want to allow users to access small portions of that data at random. (See Figure 1.7.) This is similar to locating an image file somewhere on the file server in the image application example. 为了本节的目的，让我们假设你有数TB的数据，并希望用户能够随机访问该数据的一小部分。（请参见图1.7。）这就类似于在图片应用例子里定位文件服务器上一个图片文件的位置。。 Figure 1.7: Accessing specific data图1.7：访问特定数据 This is particularly challenging because it can be very costly to load TBs of data into memory; this directly translates to disk IO. Reading from disk is many times slower than from memory—memory access is as fast as Chuck Norris, whereas disk access is slower than the line at the DMV. This speed difference really adds up for large data sets; in real numbers memory access is as little as 6 times faster for sequential reads, or 100,000 times faster for random reads, than reading from disk (see “The Pathologies of Big Data”, http://queue.acm.org/detail.cfm?id=1563874). Moreover, even with unique IDs, solving the problem of knowing where to find that little bit of data can be an arduous task. It’s like trying to get that last Jolly Rancher from your candy stash without looking. 由于很难将TB级的数据加载到内存，所以这会使事情变得非常有挑战性；这（种访问）将直接变为磁盘IO操作。从磁盘读取比从内存读取慢很多倍，访问速度和Chuck Norris(美国武术家,电影明星)一样快，而磁盘访问比DMV线还慢。这样的速度差异对于大数据来说比较客观（This speed difference really adds up for large data sets）；顺序读方面访问内存的速度是访问磁盘的6倍，而在随机读方面，前者是后者的十万倍（参见”The Pathologies of Big Data”, http://queue.acm.org/detail.cfm?id=1563874）。而且，即使有唯一ID，从哪里能够找到这样一小块数据仍然是一项艰巨的任务。这就好比从你藏糖果的地方不看一眼地想拿到最后一块Jolly Rancher。 Thankfully there are many options that you can employ to make this easier; four of the more important ones are caches, proxies, indexes and load balancers. The rest of this section discusses how each of these concepts can be used to make data access a lot faster. 幸运的是，你有很多能把事情变得更加容易的选择；其中重要的有如下4个：缓存、代理、索引、负载均衡。本节的其余部分将讨论如何使用这些概念使数据访问更快。 Caches 缓存Caches take advantage of the locality of reference principle: recently requested data is likely to be requested again. They are used in almost every layer of computing: hardware, operating systems, web browsers, web applications and more. A cache is like short-term memory: it has a limited amount of space, but is typically faster than the original data source and contains the most recently accessed items. Caches can exist at all levels in architecture, but are often found at the level nearest to the front end, where they are implemented to return data quickly without taxing downstream levels. 缓存利用了本地引用原则的好处：最近访问的数据可能被再次访问。缓存几乎被用在计算机运行的各层：硬件，操作系统，web浏览器，web应用等等。缓存就像短期的内存：有着限定大小的空间，但通常比访问原始数据源更快，并且包含有最近最多被访问过的（数据）项。缓存可以存在于架构的各个层次，但会发现到经常更靠近前端（非web前端界面，架构上层），这样就可尽快返回数据而不用经过繁重的下层（处理）了。 How can a cache be used to make your data access faster in our API example? In this case, there are a couple of places you can insert a cache. One option is to insert a cache on your request layer node, as in Figure 1.8. 在我们的API例子中，如何使用一个缓存来加速你的数据访问速度呢？在这个场景下，你可以在很多地方插入一个缓存。选择之一是在你的请求层节点中插入一个缓存，如图 1.8.. ​ Figure 1.8: Inserting a cache on your request layer node​ 图1.8: 在请求层节点中插入缓存 Placing a cache directly on a request layer node enables the local storage of response data. Each time a request is made to the service, the node will quickly return local, cached data if it exists. If it is not in the cache, the request node will query the data from disk. The cache on one request layer node could also be located both in memory (which is very fast) and on the node’s local disk (faster than going to network storage). 将缓存直接放置在请求层节点中让本地存储响应数据变为可能。每次对于一个服务的请求，节点将立即返回存在的本地、缓存的数据。如果（对应的）缓存不存在，请求节点将会从磁盘中查询数据。请求层节点的缓存既可以放置在内存（更快）也可以在节点本地磁盘（比通过网络快）上。 ​ Figure 1.9: Multiple caches​ 图1.9：多个缓存 What happens when you expand this to many nodes? As you can see in Figure 1.9, if the request layer is expanded to multiple nodes, it’s still quite possible to have each node host its own cache. However, if your load balancer randomly distributes requests across the nodes, the same request will go to different nodes, thus increasing cache misses. Two choices for overcoming this hurdle are global caches and distributed caches. 当你扩展到多个节点时，会发生什么呢？如图 1.9所示,如果请求层扩展到多个节点，那么每个节点都可以拥有它自身的缓存。但是，如果你的负载均衡器将请求随机分发到这些节点上，同样的请求会到达不同的节点，就会提高缓存miss率。两种克服这种困难的方法是：全局缓存和分布式缓存。 Global Cache 全局缓存A global cache is just as it sounds: all the nodes use the same single cache space. This involves adding a server, or file store of some sort, faster than your original store and accessible by all the request layer nodes. Each of the request nodes queries the cache in the same way it would a local one. This kind of caching scheme can get a bit complicated because it is very easy to overwhelm a single cache as the number of clients and requests increase, but is very effective in some architectures (particularly ones with specialized hardware that make this global cache very fast, or that have a fixed dataset that needs to be cached). 正如听起来的一样，全局缓存是指：所有节点使用同一缓存空间。这包括增加一台服务器或是某种类型的文件存储，比从你原始存储地方（访问）更快，并且所有请求层的节点均可以访问（全局缓存）。所有请求节点统一像访问其本地缓存般访问（全局）缓存。这种类型的缓存机制可能会变得比较复杂，因为随着客户端和请求数量的增加，单个缓存（服务器）很容易被压垮，但是在一些架构中非常有效（特别是有专门定制的硬件使得访问全局缓存非常快速，或者需要缓存的数据集是固定的）。 There are two common forms of global caches depicted in the diagrams. In Figure 1.10, when a cached response is not found in the cache, the cache itself becomes responsible for retrieving the missing piece of data from the underlying store. In Figure 1.11 it is the responsibility of request nodes to retrieve any data that is not found in the cache. 通常有两种形式的全局缓存，如下图。图 1.10,中，如果缓存中找不到对应的响应，那缓存自身会去从下层存储中获取丢失的数据。在图1.11中，当缓存中找不到相应数据时，需要请求节点自己去获取数据。 Figure 1.10: Global cache where cache is responsible for retrieval 图1.10：全局缓存自身负责存取 Figure 1.11: Global cache where request nodes are responsible for retrieval ​ 图1.11 全局缓存，请求节点负责存取 第一种方式相当于是全局缓存将查询缓存、底层获取数据、填充缓存这些操作一并做掉，理想情况下对于上层应用应该只需要提供一个获取数据的API，上层应用无需关心所请求的数据是已存在于缓存中的还是从底层存储中获取的，能够更专注于上层业务逻辑，但这就可能需要这种全局缓存设计成能够根据传入API接口的参数去获取底层存储的数据，译者认为接口签名可以简化为Object getData(String uniqueId, DataRetrieveCallback callback)，第一个参数代表与缓存约定的唯一标示一个数据的ID，第二个是一个获取数据回调接口，具体实现由调用该接口的业务端来实现，即当全局缓存中未找到uniqueId对应的缓存数据时，那就会以该callback去获取数据，并以uniqueId为key、callback获取数据为value放入全局缓存中。第二种方式相对来说自由一些。请求节点自行根据业务场景需求来决定查询数据的方式，以及查数据后的处理（比如缓存回收策略），全局缓存只作为一个基础组件让请求节点能够在其中存取数据 The majority of applications leveraging global caches tend to use the first type, where the cache itself manages eviction and fetching data to prevent a flood of requests for the same data from the clients. However, there are some cases where the second implementation makes more sense. For example, if the cache is being used for very large files, a low cache hit percentage would cause the cache buffer to become overwhelmed with cache misses; in this situation it helps to have a large percentage of the total data set (or hot data set) in the cache. Another example is an architecture where the files stored in the cache are static and shouldn’t be evicted. (This could be because of application requirements around that data latency—certain pieces of data might need to be very fast for large data sets—where the application logic understands the eviction strategy or hot spots better than the cache.) 大多数应用倾向于通过第一种方式使用全局缓存，由缓存自身来管理回收、获取数据，来应对从客户端发起的对同一数据的众多请求。但是，对于一些场景来说，第二种实现就比较有意义。比如，如果是用来缓存大型文件，那缓存低命中率将会导致缓存缓冲区被缓存miss给压垮；在这种情况下，缓存中缓存大部分数据集（或热门数据）将会有助解决这个问题。另一个例子是，一个架构中缓存的文件是静态、不应回收的。（这可能跟应用对于数据延迟的需求有关——对于大数据集来说，某些数据段需要被快速访问——这时应用的业务逻辑会比缓存更懂得回收策略或热点处理。） Distributed Cache 分布式缓存In a distributed cache (Figure 1.12), each of its nodes own part of the cached data, so if a refrigerator acts as a cache to the grocery store, a distributed cache is like putting your food in several locations—your fridge, cupboards, and lunch box—convenient locations for retrieving snacks from, without a trip to the store. Typically the cache is divided up using a consistent hashing function, such that if a request node is looking for a certain piece of data it can quickly know where to look within the distributed cache to determine if that data is available. In this case, each node has a small piece of the cache, and will then send a request to another node for the data before going to the origin. Therefore, one of the advantages of a distributed cache is the increased cache space that can be had just by adding nodes to the request pool. 在一个分布式缓存中（如图1.12），没个节点拥有部分缓存的数据，如果将杂货店里的冰箱比作一个缓存，那么一个分布式缓存好比是将你的食物放在几个不同的地方——你的冰箱、食物柜、午餐饭盒里——非常便于取到快餐的地方而无需跑一趟商店。通常这类缓存使用一致性Hash算法进行切分，这样一个请求节点在查询指定数据时，可以很快知道去哪里查询，并通过分布式缓存来判断数据可用性。这种场景下，每个节点都会拥有一部分缓存，并且会将请求传递到其他节点来获取数据，最后才到原始地方查询数据。因此，分布式缓存的一个优势就是通过往请求池里增加节点来扩大缓存空间。 A disadvantage of distributed caching is remedying a missing node. Some distributed caches get around this by storing multiple copies of the data on different nodes; however, you can imagine how this logic can get complicated quickly, especially when you add or remove nodes from the request layer. Although even if a node disappears and part of the cache is lost, the requests will just pull from the origin—so it isn’t necessarily catastrophic! 分布式缓存的一个缺点在于节点丢失纠正问题。一些分布式缓存通过将复制数据多份存放在不同的节点来解决这个问题；但是，你可以想象到这样做会让逻辑迅速变得复杂，特别是当你向请求层增加或减少节点的时候。虽然一个节点丢失并且缓存失效，但请求仍然可以从源头来获取（数据）——所以这不一定是最悲剧的。 Figure 1.12: Distributed cache The great thing about caches is that they usually make things much faster (implemented correctly, of course!) The methodology you choose just allows you to make it faster for even more requests. However, all this caching comes at the cost of having to maintain additional storage space, typically in the form of expensive memory; nothing is free. Caches are wonderful for making things generally faster, and moreover provide system functionality under high load conditions when otherwise there would be complete service degradation. 缓存的伟大之处在于它们让事情进行的更快（当然需要执行正确）。你所选择的方法只是让你能够更快处理更多的请求。但是，这些缓存是以需要维护更多存储空间为代价的，特别是昂贵的内存方式；天下没有免费的午餐。缓存让事情变得更快，同时还保证了高负载条件下系统的功能，否则（系统）服务可能早已降级。 One example of a popular open source cache is Memcached (http://memcached.org/) (which can work both as a local cache and distributed cache); however, there are many other options (including many language- or framework-specific options). 一个非常受欢迎的开源缓存叫做Memcached(http://memcached.org/)（既可以是本地又可以是分布式缓存）；但是，还有很多其他选择（包括许多语言/框架特定选择）。 Memcached is used in many large web sites, and even though it can be very powerful, it is simply an in-memory key value store, optimized for arbitrary data storage and fast lookups (O(1)). Memcached被应用于许多大型web网站，纵然它功能强大，但它简单来说就是一个内存key-value存储，对任意数据存储和快速查找做了优化（时间复杂度O(1)）。 Facebook uses several different types of caching to obtain their site performance (see “Facebook caching and performance”). They use $GLOBALS and APC caching at the language level (provided in PHP at the cost of a function call) which helps make intermediate function calls and results much faster. (Most languages have these types of libraries to improve web page performance and they should almost always be used.) Facebook then use a global cache that is distributed across many servers (see “Scaling memcached at Facebook”), such that one function call accessing the cache could make many requests in parallel for data stored on different Memcached servers. This allows them to get much higher performance and throughput for their user profile data, and have one central place to update data (which is important, since cache invalidation and maintaining consistency can be challenging when you are running thousands of servers). Facebook使用了若干种不同类型的缓存以达到他们网站的性能（参见“Facebook caching and performance“）。他们在语言层面使用$GLOBALS和APC缓存（在PHP中提供的函数调用）使得中间功能调用和（得到）结果更加快速。（大多数语言都有这种类型的类库来提高web性能，应该经常去使用。）Facebook使用一种全局缓存，分布在多台服务器上（参见”Scaling memcached at Facebook“），这样一个访问缓存的函数调用就会产生很多并行请求来从Memcached服务器（集群）获取数据。这使得他们能够在用户概况数据上获得更高的性能和吞吐量，并且有一个集中的地方去更新数据（当你运行着数以千计的服务器时，缓存失效、管理一致性都将变得很有挑战，所以这是很重要的）。 Now let’s talk about what to do when the data isn’t in the cache… 现在让我们来聊聊缓存失效的时候应该做什么。 Proxies 代理At a basic level, a proxy server is an intermediate piece of hardware/software that receives requests from clients and relays them to the backend origin servers. Typically, proxies are used to filter requests, log requests, or sometimes transform requests (by adding/removing headers, encrypting/decrypting, or compression). 从基本层面来看，代理服务器是硬件/软件的一个中间层，用于接收从客户端发起的请求并传递到后端服务器。通常来说，代理是用来过滤请求、记录请求日志或者有时对请求进行转换（增加/去除头文件，加密/解密或者进行压缩）。 Figure 1.13: Proxy server 图1.13：代理服务器 Proxies are also immensely helpful when coordinating requests from multiple servers, providing opportunities to optimize request traffic from a system-wide perspective. One way to use a proxy to speed up data access is to collapse the same (or similar) requests together into one request, and then return the single result to the requesting clients. This is known as collapsed forwarding. 代理同样能够极大帮助协调多个服务器的请求，有机会从系统的角度来优化请求流量。使用代理来加快数据访问速度的方式之一是将多个同种请求集中放到一个请求中，然后将单个结果返回到请求客户端。这就叫做压缩转发。 Imagine there is a request for the same data (let’s call it littleB) across several nodes, and that piece of data is not in the cache. If that request is routed thought the proxy, then all of those requests can be collapsed into one, which means we only have to read littleB off disk once. (See Figure 1.14.) There is some cost associated with this design, since each request can have slightly higher latency, and some requests may be slightly delayed to be grouped with similar ones. But it will improve performance in high load situations, particularly when that same data is requested over and over. This is similar to a cache, but instead of storing the data/document like a cache, it is optimizing the requests or calls for those documents and acting as a proxy for those clients. 假设在几个节点上存在对同样数据的请求（我们叫它littleB），并且这份数据不在缓存里。如果请求通过代理路由，那么这些请求可以被压缩为一个，就意味着我们只需要从磁盘读取一次littleB即可。（见图1.14）这种设计是会带来一定的开销，因为每个请求都会产生更高的延迟（跟不用代理相比），并且一些请求会因为要与相同请求合并而产生一些延迟。但这种做法在高负载的情况下提高系统性能，特别是当相同的数据重复被请求。这很像缓存，但不用像缓存那样存储数据/文件，而是优化了对那些文件的请求或调用，并且充当那些客户端的代理。 In a LAN proxy, for example, the clients do not need their own IPs to connect to the Internet, and the LAN will collapse calls from the clients for the same content. It is easy to get confused here though, since many proxies are also caches (as it is a very logical place to put a cache), but not all caches act as proxies. 例如，在局域网（LAN）代理中，客户端不需有自己的IP来连接互联网，而局域网会将对同样内容的客户端请求进行压缩。这里可能很容易产生困惑，因为许多代理同样也是缓存（因为在这里放一个缓存很合理），但不是所有缓存都能充当代理。 Figure 1.14: Using a proxy server to collapse requests ​ 图1.14：使用一个代理服务器来压缩请求 Another great way to use the proxy is to not just collapse requests for the same data, but also to collapse requests for data that is spatially close together in the origin store (consecutively on disk). Employing such a strategy maximizes data locality for the requests, which can result in decreased request latency. For example, let’s say a bunch of nodes request parts of B: partB1, partB2, etc. We can set up our proxy to recognize the spatial locality of the individual requests, collapsing them into a single request and returning only bigB, greatly minimizing the reads from the data origin. (See Figure 1.15.) This can make a really big difference in request time when you are randomly accessing across TBs of data! Proxies are especially helpful under high load situations, or when you have limited caching, since they can essentially batch several requests into one. 另一个使用代理的好方法是，不单把代理用来压缩对同样数据的请求，还可以用来压缩对那些在原始存储中空间上紧密联系的数据（磁盘连续块）的请求。使用这一策略最大化（利用）所请求数据的本地性，可以减少请求延迟。例如，我们假设一群节点请求B的部分（数据）：B1， B2，等。我们可以对代理进行设置使其能够识别出不同请求的空间局部性，将它们压缩为单个请求并且只返回bigB，最小化对原始数据的读取操作。（见图1.15）当你随机访问TB级的数据时，这样会大幅改变（降低）请求时间。在高负载情况下或者当你只有有限的缓存，代理是非常有帮助的，因为代理可以从根本上将若干个请求合并为一个。 Figure 1.15: Using a proxy to collapse requests for data that is spatially close together ​ 图1.15：使用代理压缩空间上邻近的数据请求 It is worth noting that you can use proxies and caches together, but generally it is best to put the cache in front of the proxy, for the same reason that it is best to let the faster runners start first in a crowded marathon race. This is because the cache is serving data from memory, it is very fast, and it doesn’t mind multiple requests for the same result. But if the cache was located on the other side of the proxy server, then there would be additional latency with every request before the cache, and this could hinder performance. 你完全可以将代理和缓存一起使用，但通常最好将缓存放在代理之前使用，正如在马拉松赛跑中最好让跑得快的选手跑在前面。这是因为缓存通过内存来提供数据非常快速，并且它也不关心多个对同样结果的请求。但如果缓存被放在代理服务器的另一边（后面），那在每个请求访问缓存前就会有额外的延迟，这会阻碍系统性能。 If you are looking at adding a proxy to your systems, there are many options to consider; Squid and Varnish have both been road tested and are widely used in many production web sites. These proxy solutions offer many optimizations to make the most of client-server communication. Installing one of these as a reverse proxy (explained in the load balancer section below) at the web server layer can improve web server performance considerably, reducing the amount of work required to handle incoming client requests. 如果你在寻找一款代理想要加入到你的系统中，那有很多选择可供考虑；Squid和Varnish都是经过路演并广泛应用于很多网站的生产环境中。这些代理方案做了很多优化来充分使用客户端与服务端的通信。安装其中之一并在web服务器层将其作为一个反向代理（将在下面的负载均衡小节解释）可以提高web服务器相当大的性能，降低处理来自客户端的请求所消耗的工作量。 Indexes 索引Using an index to access your data quickly is a well-known strategy for optimizing data access performance; probably the most well known when it comes to databases. An index makes the trade-offs of increased storage overhead and slower writes (since you must both write the data and update the index) for the benefit of faster reads. 使用索引来加快访问数据已经是优化数据访问性能众所周知的策略；可能更多来自数据库。索引是以增加存储开销和减慢写入速度（因为你必须同时写入数据并更新索引）的代价来得到更快读取的好处。 Just as to a traditional relational data store, you can also apply this concept to larger data sets. The trick with indexes is you must carefully consider how users will access your data. In the case of data sets that are many TBs in size, but with very small payloads (e.g., 1 KB), indexes are a necessity for optimizing data access. Finding a small payload in such a large data set can be a real challenge since you can’t possibly iterate over that much data in any reasonable time. Furthermore, it is very likely that such a large data set is spread over several (or many!) physical devices—this means you need some way to find the correct physical location of the desired data. Indexes are the best way to do this. 就像对于传统的关系数据库，你同样可以将这种概念应用到大数据集上。索引的诀窍在于你必须仔细考虑你的用户会如何使用你的数据。对于TB级但单项数据比较小（比如1KB）的数据集，索引是优化数据访问非常必要的方式。在一个大数据集中寻找一个小单元是非常困难的，因为你不可能在一个可接受的时间里遍历这么大的数据。并且，像这么一个大数据集很有可能是分布在几个（或更多）物理设备上——这就意味着你需要有方法能够找到所要数据正确的物理位置。索引是达到这个的最好方法。 Figure 1.16: Indexes 图1.16：索引 An index can be used like a table of contents that directs you to the location where your data lives. For example, let’s say you are looking for a piece of data, part 2 of B—how will you know where to find it? If you have an index that is sorted by data type—say data A, B, C—it would tell you the location of data B at the origin. Then you just have to seek to that location and read the part of B you want. (See Figure 1.16.) 索引可以像一张可以引导你至所要数据位置的表格来使用。例如，我们假设你在寻找B的part2数据——你将如何知道到哪去找到它？如果你有一个按照数据类型（如A,B,C）排序好的索引，它会告诉你数据B在哪里。然后你查找到位置，然后读取你所要的部分。（见图1.16） These indexes are often stored in memory, or somewhere very local to the incoming client request. Berkeley DBs (BDBs) and tree-like data structures are commonly used to store data in ordered lists, ideal for access with an index. 这些索引通常存放在内存中，或者在更靠近客户端请求的地方。伯克利数据库（BDBs）和树形数据结构经常用来有序地存储数据，非常适合通过索引来访问。 Often there are many layers of indexes that serve as a map, moving you from one location to the next, and so forth, until you get the specific piece of data you want. (See Figure 1.17.) 索引经常会有很多层，类似一个map，将你从一个地方引导至另一个，以此类推，直到你获取到你所要的那份数据。（见图1.17） Figure 1.17: Many layers of indexes ​ 图1.17：多层索引 Indexes can also be used to create several different views of the same data. For large data sets, this is a great way to define different filters and sorts without resorting to creating many additional copies of the data. 索引也可以用来对同样的数据创建出一些不同的视图。对于大数据集来说，通过定义不同的过滤器和排序是一个很好的方式，而不需要创建很多额外数据拷贝。 For example, imagine that the image hosting system from earlier is actually hosting images of book pages, and the service allows client queries across the text in those images, searching all the book content about a topic, in the same way search engines allow you to search HTML content. In this case, all those book images take many, many servers to store the files, and finding one page to render to the user can be a bit involved. First, inverse indexes to query for arbitrary words and word tuples need to be easily accessible; then there is the challenge of navigating to the exact page and location within that book, and retrieving the right image for the results. So in this case the inverted index would map to a location (such as book B), and then B may contain an index with all the words, locations and number of occurrences in each part. 例如，假设之前的图片托管系统就是在管理书页上的图片，并且服务能够允许客户端查询图片中的文字，按照标题搜索整本书的内容，就像搜索引擎允许你搜索HTML内容一样。这种场景下，所有书中的图片需要很多很多的服务器去存储文件，查找到其中一页渲染给用户将会是比较复杂的。首先，对需要易于查询的任意单词、词组进行倒排索引；然后挑战在于导航至那本书具体的页面、位置并获取到正确的图片。所以，在这一场景，倒排索引将会映射到一个位置（比如B书），然后B可能会包含每个部分的所有单词、位置、出现次数的索引。 An inverted index, which could represent Index1 in the diagram above, might look something like the following—each word or tuple of words provide an index of what books contain them. 倒排索引可能如同下图——每个单词或词组会提供一个哪些书包含它的索引。 Word(s) Book(s) being awesome Book B, Book C, Book D always Book C, Book F believe Book B The intermediate index would look similar but would contain just the words, location, and information for book B. This nested index architecture allows each of these indexes to take up less space than if all of that info had to be stored into one big inverted index. 这种中间索引看上去都类似，仅会包含单词、位置和B的一些信息。这种嵌套索引的架构允许每个索引占用更少的空间而非将所有的信息存放在一个巨大的倒排索引中。 And this is key in large-scale systems because even compressed, these indexes can get quite big and expensive to store. In this system if we assume we have a lot of the books in the world—100,000,000 (see Inside Google Books blog post)—and that each book is only 10 pages long (to make the math easier), with 250 words per page, that means there are 250 billion words. If we assume an average of 5 characters per word, and each character takes 8 bits (or 1 byte, even though some characters are 2 bytes), so 5 bytes per word, then an index containing only each word once is over a terabyte of storage. So you can see creating indexes that have a lot of other information like tuples of words, locations for the data, and counts of occurrences, can add up very quickly. 在大型可伸缩的系统中，即使索引已被压缩但仍会变得很大，不易存储。在这个系统里，我们假设世界上有很多书——100,000,000本——并且每本书仅有10页（为了便于计算），每页有250个单词，这就意味着一共有2500亿个单词。如果我们假设平均每个单词有5个字符，每个字符占用8个比特，每个单词5个字节，那么对于仅包含每个单词的索引的大小就达到TB级。所以你会发现创建像一些如词组、数据位置、出现次数之类的其他信息的索引将会增长得更快。 Creating these intermediate indexes and representing the data in smaller sections makes big data problems tractable. Data can be spread across many servers and still accessed quickly. Indexes are a cornerstone of information retrieval, and the basis for today’s modern search engines. Of course, this section only scratched the surface, and there is a lot of research being done on how to make indexes smaller, faster, contain more information (like relevancy), and update seamlessly. (There are some manageability challenges with race conditions, and with the sheer number of updates required to add new data or change existing data, particularly in the event where relevancy or scoring is involved). 创建这些中间索引并且以更小的方式表达数据，将大数据的问题变得易于处理。数据可以分布在多台服务器但仍可以快速访问。索引是信息获取的基石，也是当今现代搜索引擎的基础。当然，这一小节仅仅是揭开表面，为了把索引变得更小、更快、包含更多信息（比如关联）、无缝更新，还有大量的研究工作要做。（还有一些可管理性方面的挑战，比如竞争条件、增加或修改数据所带来的更新操作，特别是再加上关联、scoring） Being able to find your data quickly and easily is important; indexes are an effective and simple tool to achieve this. 能够快速、简单地找到你的数据非常重要；索引是达到这一目标非常有效、简单的工具。 Load Balancers 负载均衡器Finally, another critical piece of any distributed system is a load balancer. Load balancers are a principal part of any architecture, as their role is to distribute load across a set of nodes responsible for servicing requests. This allows multiple nodes to transparently service the same function in a system. (See Figure 1.18.) Their main purpose is to handle a lot of simultaneous connections and route those connections to one of the request nodes, allowing the system to scale to service more requests by just adding nodes. 另一个任何分布式系统的关键组件是负载均衡器。负载均衡器是任何架构的关键部分，用于将负载分摊在一些列负责服务请求的节点上。这使得一个系统的多个节点能够为相同功能提供服务。（见图1.18）它们主要目的是处理许多同时进行的连接并将这些连接路由到其中的一个请求节点上，使得系统能够可伸缩地通过增加节点来服务更多请求。 Figure 1.18: Load balancer ​ 图1.18 负载均衡器 There are many different algorithms that can be used to service requests, including picking a random node, round robin, or even selecting the node based on certain criteria, such as memory or CPU utilization. Load balancers can be implemented as software or hardware appliances. One open source software load balancer that has received wide adoption is HAProxy). 有很多不同的用于服务请求的算法，包括随机挑选一个节点、循环（round robin）或给予某些标准如内存/CPU使用率选取节点。一个广泛使用的开源软件级负载均衡器是HAProxy。 In a distributed system, load balancers are often found at the very front of the system, such that all incoming requests are routed accordingly. In a complex distributed system, it is not uncommon for a request to be routed to multiple load balancers as shown inFigure 1.19. 在一个分布式系统中，负责均衡器通常是放置在系统很前端的地方，这样就能路由所有进入（系统）的请求。在一个复杂的分布式系统中，一个请求被多个负载均衡器路由也不是不可能。（见图1.19） Figure 1.19: Multiple load balancers ​ 图1.19：多重负责均衡器 Like proxies, some load balancers can also route a request differently depending on the type of request it is. (Technically these are also known as reverse proxies.) 如同代理一般，一些负载均衡器也能根据不同类型的请求进行路由。（从技术上来说，就是所谓的反向代理。） One of the challenges with load balancers is managing user-session-specific data. In an e-commerce site, when you only have one client it is very easy to allow users to put things in their shopping cart and persist those contents between visits (which is important, because it is much more likely you will sell the product if it is still in the user’s cart when they return). However, if a user is routed to one node for a session, and then a different node on their next visit, there can be inconsistencies since the new node may be missing that user’s cart contents. (Wouldn’t you be upset if you put a 6 pack of Mountain Dew in your cart and then came back and it was empty?) One way around this can be to make sessions sticky so that the user is always routed to the same node, but then it is very hard to take advantage of some reliability features like automatic failover. In this case, the user’s shopping cart would always have the contents, but if their sticky node became unavailable there would need to be a special case and the assumption of the contents being there would no longer be valid (although hopefully this assumption wouldn’t be built into the application). Of course, this problem can be solved using other strategies and tools in this chapter, like services, and many not covered (like browser caches, cookies, and URL rewriting). 负载均衡器的挑战之一在于（如何）管理用户session数据。在一个电子商务网站，当你只有一个客户端时很容易让用户把东西放到他们的购物车并且在不同的访问间保存（这是很重要的，因为当用户回来时很有可能买放在购物车里的产品）。但是，如果一个用户先被路由到一个session节点，然后在他们下次访问时路由到另一个不同的节点，那将会因为新节点可能丢失用户购物车里的东西而产生不一致。（如果你精心挑选了6包Mountain Dew放到购物车，但当你回来的时候发现购物车清空了，你会不会很沮丧？）解决办法之一通过粘性session机制总是将用户路由到同一节点，但这样既很难享受到一些像自动failover的可靠机制了。在这一场景下，用户的购物车总是会有东西的，如果他们所对应的粘性节点不可用了，那么就会是一个特殊情况对于（保存）在那里的东西的假设就无效了（当然我们希望这种假设不会出现在应用里）。当然，这个问题可以通过本章中的一些其他策略或者工具来解决，比如服务，还有一些没有提到的（如浏览器缓存、cookie、URL地址重写）。 If a system only has a couple of a nodes, systems like round robin DNS may make more sense since load balancers can be expensive and add an unneeded layer of complexity. Of course in larger systems there are all sorts of different scheduling and load-balancing algorithms, including simple ones like random choice or round robin, and more sophisticated mechanisms that take things like utilization and capacity into consideration. All of these algorithms allow traffic and requests to be distributed, and can provide helpful reliability tools like automatic failover, or automatic removal of a bad node (such as when it becomes unresponsive). However, these advanced features can make problem diagnosis cumbersome. For example, when it comes to high load situations, load balancers will remove nodes that may be slow or timing out (because of too many requests), but that only exacerbates the situation for the other nodes. In these cases extensive monitoring is important, because overall system traffic and throughput may look like it is decreasing (since the nodes are serving less requests) but the individual nodes are becoming maxed out. 如果系统只是由少数节点构成的，那么像Round Robin DNS那样的系统就更加明智，因为负责均衡器很贵而且增加了一层不必要的复杂度。当然在大型系统里有各种各样的调度和负载均衡算法，包括简单的像随机选择或循环方式，还有更加复杂的机制如考虑（系统）使用率和容量的。所有这些算法都分布化了流量和请求，并且提供像自动failover或者自动去除坏节点（当该节点失去响应后）这类对可靠性非常有帮助的工具。但是，这些先进特性也会使得问题诊断变得复杂化。比如，在一个高负载情况下，负载均衡器会去除掉那些变慢或者超时（由于请求过多）的节点，但这样反而加重了其他节点的（恶劣）处境。在这些情况下，全面监控变得很重要，因为从全局来看系统的流量和吞吐量正在下降（由于各节点服务请求越来越少），但从节点个体来看正在达到极限。 Load balancers are an easy way to allow you to expand system capacity, and like the other techniques in this article, play an essential role in distributed system architecture. Load balancers also provide the critical function of being able to test the health of a node, such that if a node is unresponsive or over-loaded, it can be removed from the pool handling requests, taking advantage of the redundancy of different nodes in your system. 负载均衡器是一个非常简单能让你提高系统容量的方法，并且像本文其他的技术一样，在分布式系统架构中扮演者重要角色。负载均衡器还能用来判断一个节点的健康度，这样当一个节点失去响应或者过载时，得益于系统不同节点的冗余性，可以将其从请求处理池中去除。 Queues 队列So far we have covered a lot of ways to read data quickly, but another important part of scaling the data layer is effective management of writes. When systems are simple, with minimal processing loads and small databases, writes can be predictably fast; however, in more complex systems writes can take an almost non-deterministically long time. For example, data may have to be written several places on different servers or indexes, or the system could just be under high load. In the cases where writes, or any task for that matter, may take a long time, achieving performance and availability requires building asynchrony into the system; a common way to do that is with queues. 至此，我们已经覆盖了很多用于加快数据读取的方法，另一个扩展数据层的重要部分是有效管理写入操作。当系统比较简单，系统处理负载很低，数据库也很小，可以预见写入操作是很快的；但是，在更加复杂的系统中，写入操作的时间可能无法确定。例如，数据需要被写入到不同服务器或索引的多个地方，或者系统负载很高。这些情况下，由于上面的原因，写操作或者任何任务都会花费很长的时间，这时需要异步化系统才能提高系统的性能和可靠性；通常的方法之一是使用队列。 Figure 1.20: Synchronous request 图1.20：同步化请求 Imagine a system where each client is requesting a task to be remotely serviced. Each of these clients sends their request to the server, where the server completes the tasks as quickly as possible and returns the results to their respective clients. In small systems where one server (or logical service) can service incoming clients just as fast as they come, this sort of situation should work just fine. However, when the server receives more requests than it can handle, then each client is forced to wait for the other clients’ requests to complete before a response can be generated. This is an example of a synchronous request, depicted in Figure 1.20. 假设在一个系统中，每个客户端在请求远程服务来处理任务。每个客户端将其请求送至服务器，服务器尽可能快地完成这些任务并返回结果给相应的客户端。在小型系统中，当一台服务器（或者逻辑上的一个服务）可以尽快地服务到来的客户端（请求），这种情况下（系统）工作会比较好。但是，当服务器接收到超过其处理能力的请求时，那每个客户端都只能被迫等待其他客户端请求完成才能得到响应。图1.20描绘的就是一个同步请求的例子。 This kind of synchronous behavior can severely degrade client performance; the client is forced to wait, effectively performing zero work, until its request can be answered. Adding additional servers to address system load does not solve the problem either; even with effective load balancing in place it is extremely difficult to ensure the even and fair distribution of work required to maximize client performance. Further, if the server handling requests is unavailable, or fails, then the clients upstream will also fail. Solving this problem effectively requires abstraction between the client’s request and the actual work performed to service it. 这种同步的方式将会严重降低客户端性能；客户端被强制等待，在请求被响应前什么都做不了。增加额外的服务器并不能解决这个问题；即使通过有效的负载均衡，依然难以保证最大化客户端性能所需做的公平分配的工作。更进一步来说，当处理请求的服务器不可用或挂掉了，那么上游的客户端同样也会失败。有效解决这个问题需要抽象化客户端的请求和真正服务它所做的工作。 Figure 1.21: Using queues to manage requests 图1.21：使用队列来管理请求 When designing any sort of web application it is important to consider these key principles, even if it is to acknowledge that a design may sacrifice one or more of them.在设计任何类型的Web应用程序时，考虑这些关键原则很重要，即使承认设计可能会牺牲其中的一个或多个原则。 Enter queues. A queue is as simple as it sounds: a task comes in, is added to the queue and then workers pick up the next task as they have the capacity to process it. (See Figure 1.21.) These tasks could represent simple writes to a database, or something as complex as generating a thumbnail preview image for a document. When a client submits task requests to a queue they are no longer forced to wait for the results; instead they need only acknowledgement that the request was properly received. This acknowledgement can later serve as a reference for the results of the work when the client requires it.现在进入队列环节。一个队列，正如听上去的，简单来说就是当一个任务过来时，会被加入到队列中，然后会有当前有能力处理（任务）的worker去取下一个任务来做。（见图1.21。）这些任务可以是对数据库的写入操作，或是复杂一些的如生成文件的小型预览图。当一个客户端将任务的请求提交到队列后，它们不再需要被迫等待结果；取而代之的是，它们只需要确认请求被得到正确接收。当客户端需要的时候，这个确认此后可以当做是任务结果的引用。 Queues enable clients to work in an asynchronous manner, providing a strategic abstraction of a client’s request and its response. On the other hand, in a synchronous system, there is no differentiation between request and reply, and they therefore cannot be managed separately. In an asynchronous system the client requests a task, the service responds with a message lacknowledging the task was received, and then the client can periodically check the status of the task, only requesting the result once it has completed. While the client is waiting for an asynchronous request to be completed it is free to perform other work, even making asynchronous requests of other services. The latter is an example of how queues and messages are leveraged in distributed systems. 队列使得客户端能够以异步的方式进行工作，至关重要地抽象了一个客户端请求及其响应。另一方面，一个同步化系统不会区分请求和响应，因此就无法分开管理。在一个异步化系统里，客户端提交任务请求，后端服务反馈一个收到任务的确认信息，并且客户端可以定期地查看任务的状态，一旦完成即可取得任务结果。在客户端等待一个异步请求完成时，它可以自由地处理其他的工作，即使是发起对其他服务的异步请求。上面第二个就是分布式系统中采用队列和消息的例子。 Queues also provide some protection from service outages and failures. For instance, it is quite easy to create a highly robust queue that can retry service requests that have failed due to transient server failures. It is more preferable to use a queue to enforce quality-of-service guarantees than to expose clients directly to intermittent service outages, requiring complicated and often-inconsistent client-side error handling. 队列还能提供对服务断供/失败的保护措施。比如，很容易创建一个健壮的队列来重试那些由于服务器短暂失败的服务请求。更好的是通过使用队列来确保服务品质，而非将客户端直接面对断断续续的服务，因为那样会需要客户端复杂且经常不一致的错误处理。 Queues are fundamental in managing distributed communication between different parts of any large-scale distributed system, and there are lots of ways to implement them. There are quite a few open source queues like RabbitMQ, ActiveMQ, BeanstalkD, but some also use services like Zookeeper, or even data stores like Redis. 队列是管理大型可伸缩分布式应用不同部分间通信的基础，可以通过很多方式来实现。有一些开源的队列如RabbitMQ, ActiveMQ, BeanstalkD，也有一些使用像Zookeeper的服务，还有像Redis那样的数据存储。 1.4. Conclusion 总结Designing efficient systems with fast access to lots of data is exciting, and there are lots of great tools that enable all kinds of new applications. This chapter covered just a few examples, barely scratching the surface, but there are many more—and there will only continue to be more innovation in the space. 设计出能够快速访问大量数据的高效系统（的方法）是一件激动人心的事情，并且又很多非常棒的工具来帮助各种各样的新应用来达到这一点。本章只覆盖了少量例子，仅仅是掀开了面纱，但其实还有更多，并将继续保持创新。]]></content>
      <categories>
        <category>微服务理论文章阅读学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[迭代器模式]]></title>
    <url>%2F2018%2F11%2F25%2F%E8%BF%AD%E4%BB%A3%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[[TOC] 示例合并菜单(C#实现)需求说明某连锁餐厅有一家早餐店和一家晚餐店,现需要将早餐店和晚餐店合并,由于早餐和晚餐其数据结构不同,现在需要一个统一的菜单,即菜单项结构相同 数据结构说明合并后的菜单单项 123456789101112131415161718192021222324252627public class MenuItem &#123; public MenuItem(string name, string description, bool vegetarin, double price) &#123; Name = name; Description = description; IsVegetarian = vegetarin; Price = price; &#125; /// &lt;summary&gt; /// 名称 /// &lt;/summary&gt; public string Name &#123; get; set; &#125; /// &lt;summary&gt; /// 价格 /// &lt;/summary&gt; public double Price &#123; get; set; &#125; /// &lt;summary&gt; /// 是否素食 /// &lt;/summary&gt; public bool IsVegetarian &#123; get; set; &#125; /// &lt;summary&gt; /// 描述 /// &lt;/summary&gt; public string Description &#123; get; set; &#125; &#125; 早餐类,结构是ArrayList 123456789101112131415161718192021222324public class BreakfastMenu &#123; private readonly ArrayList _menuItems; public BreakfastMenu() &#123; _menuItems = new ArrayList(); AddItem("牛奶", "牛奶description", false, 3.0); AddItem("油条", "油条description", false, 1.0); AddItem("馒头", "馒头description", true, 1.0); AddItem("豆浆", "豆浆description", true, 1.5); &#125; private void AddItem(string name, string description, bool vegetarian, double price) &#123; var menuItem = new MenuItem(name, description, vegetarian, price); _menuItems.Add(menuItem); &#125; public ArrayList GetMenuItems() &#123; return _menuItems; &#125; &#125; 晚餐类,结构是数组 123456789101112131415161718192021222324252627282930313233public class DinnerMenu &#123; private const int MaxItems = 6; int _numberOfItems = 0; private readonly MenuItem[] _menuItems; public DinerMenu() &#123; menuItems = new MenuItem[Max_ITEMS]; AddItem("香菇豆腐饭", "香菇豆腐", false, 10.5); AddItem("蛋炒饭", "哈哈", false, 8.5); AddItem("鱼香肉丝", "你猜", true, 15.5); &#125; public void AddItem(string name, string description, bool vegetarian, double price) &#123; var menuItem = new MenuItem(name, description, vegetarian, price); if (_numberOfItems &gt; MaxItems) &#123; Console.WriteLine("菜单已满"); &#125; else &#123; _menuItems[_numberOfItems++] = menuItem; &#125; &#125; public MenuItem[] GetMenuItems() &#123; return menuItems; &#125;&#125; 原有的客户端实现 12345678910111213141516171819202122//不使用迭代器 var breakfastMenu = new BreakfastMenu(); ArrayList breakfastItems = breakfastMenu.GetMenuItems(); var dinnerMenu = new DinnerMenu(); MenuItem[] dinnerItems = dinnerMenu.GetMenuItems(); foreach (var breakfastItem in breakfastItems) &#123; if (breakfastItem is MenuItem menuItem) &#123; Console.WriteLine($"&#123;menuItem.Name&#125; &#123;menuItem.Price&#125; &#123;menuItem.Description&#125;"); &#125; &#125; foreach (var dinnerItem in dinnerItems) &#123; if (dinnerItem != null) &#123; Console.WriteLine($"&#123;dinnerItem.Name&#125; &#123;dinnerItem.Price&#125; &#123;dinnerItem.Description&#125;"); &#125; &#125; 上面的遍历的算法是一样的，因为早餐和晚餐的数据结构的不同导致了代码不能复用 如果以后还要将一个午餐厅餐单合并到菜单中(数据结构和早餐晚餐都不同),又要修改代码,可以使用泛型解决该问题。但是这里使用的是迭代器设计模式 使用迭代器模式实现定义一个迭代器接口 12345678910111213interface ITerator &#123; /// &lt;summary&gt; /// 用来判断下一个元素是否为空 /// &lt;/summary&gt; /// &lt;returns&gt;&lt;/returns&gt; bool HasNext(); /// &lt;summary&gt; /// 用来获取当前元素 /// &lt;/summary&gt; /// &lt;returns&gt;&lt;/returns&gt; object Next();&#125; 我们希望的是能通过迭代器实现下面的操作 12345while (iterator.HasNext())&#123; var menuItem = (MenuItem)iterator.Next(); Console.WriteLine($"&#123;menuItem.Name&#125; &#123;menuItem.Price&#125; &#123;menuItem.Description&#125;");&#125; 创建早晚餐菜单的迭代器 1234567891011121314151617181920public class BreakfastIterator : ITerator &#123; private readonly ArrayList _items; private int _position; public BreakfastIterator(ArrayList arrayList) &#123; _items = arrayList; &#125; public bool HasNext() &#123; return _position &lt; _items.Count &amp;&amp; _items[_position] != null; &#125; public object Next() &#123; return _items[_position++] as MenuItem; &#125;&#125; 1234567891011121314151617181920public class DinnerIterator : ITerator &#123; private readonly MenuItem[] _items; private int _position = 0; public DinnerIterator(MenuItem[] items) &#123; _items = items; &#125; public bool HasNext() &#123; return _position &lt; _items.Length &amp;&amp; _items[_position] != null; &#125; public object Next() &#123; return _items[_position++]; &#125;&#125; 定义一个菜单接口，来创建迭代器,返回迭代器接口 1234interface IMenu &#123; ITerator CreateIterator(); &#125; 在早餐和晚餐的菜单中实现这个菜单接口 12345678910public class BreakfastMenu : IMenu&#123; //其余部分省略 //... public ITerator CreateIterator() &#123; return new BreakfastIterator(menuItems); &#125;&#125; 12345678910public class DinnerMenu : IMenu&#123; //其余部分省略 //... public ITerator CreateIterator() &#123; return new DinnerIterator(menuItems); &#125;&#125; 注意在 BreakfastMenu和DinnerMenu 类中去掉了 GetMenuItems() 方法.因为在改造前的实现中我们需要GetMenuItems()提供聚集数据 ,但是正是因为两个类的GetMenuItems() 返回的数据结构不同,造成了代码不能复用 改造后BreakfastMenu和DinnerMenu 类只是单纯的存放聚集数据,我们用更抽象的ITerator的来返回单个数据,避免 数据的不一致,ITerator 类的Next()方法 接管了原来GetMenuItems()方法的 功能 客户端调用 123456789101112131415161718192021222324252627static void Main(string[] args) &#123; //使用迭代器 var breakfastMenu = new BreakfastMenu(); var dinnerMenu = new DinnerMenu(); var dinnerIterator = dinnerMenu.CreateIterator(); var breakfastIterator = breakfastMenu.CreateIterator(); Console.WriteLine("早餐:"); Print(breakfastIterator); Console.WriteLine(); Console.WriteLine("晚餐:"); Print(dinnerIterator); Console.ReadKey();&#125;static void Print(ITerator iterator) &#123; while (iterator.HasNext()) &#123; var menuItem = (MenuItem)iterator.Next(); Console.WriteLine($"&#123;menuItem.Name&#125; &#123;menuItem.Price&#125; &#123;menuItem.Description&#125;"); &#125;&#125; 迭代器模式主要是聚合对象创建迭代器，借助单一职责的原则，从而实现客户端可以对聚合的各种对象实现相同的操作，达到代码复用的效果。 类图对比上面代码的类图 迭代器模式的类图 .NET中的迭代器模式.NET1.1在上面的例子中,我们没有使用任何.NET 的特性 实际要在.NET内部已经定义了实现Iterator模式 需要的聚集接口和迭代器接口，其中IEnumerator 扮演的就是迭代器接口的角色,也就是上文中的ITerator： 123456789101112//System.Collections.IEnumeratorpublic interface IEnumerator&#123; object Current &#123; get; &#125; bool MoveNext(); void Reset();&#125; 属性Current 返回当前集合中的元素，Reset() 方法恢复初始化指向的位置，MoveNext() 方法返回值true 表示迭代器成功前进到集合中的下一个元素，返回值false 表示已经位于集合的末尾 IEnumerable 则扮演的就是抽象聚集接口的角色，相当于上文中的IMenu,只有一个GetEnumerator() 方法，如果集合对象需要具备跌代遍历的功能，就必须实现该接口。 1234public interface IEnumerable&#123; IEumerator GetEnumerator();&#125; 我们试着使用.NET1.1 的方式来改写合并菜单的例子,因为有了系统接口,我们可以少写两个接口,使用系统接口 迭代器: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class BreakfastIterator : IEnumerator &#123; private readonly ArrayList _items; private int _position = -1; public BreakfastIterator(ArrayList arrayList) &#123; _items = arrayList; &#125; public bool MoveNext() &#123; _position++; return _position &lt; _items.Count; &#125; public object Current =&gt; _items[_position]; public void Reset() &#123; _position = -1; &#125;&#125;public class DinnerIterator : IEnumerator &#123; private MenuItem[] items; private int _position = -1; public DinnerIterator(MenuItem[] items) &#123; this.items = items; &#125; public bool MoveNext() &#123; _position++; return _position &lt; items.Length &amp;&amp; items[_position] != null; &#125; public object Current =&gt; items[_position]; public void Reset() &#123; _position = -1; &#125;&#125; 这里为了方便迭代,_position 的初始值设置成了-1,因为MoveNext 函数同时完成了移动指针和判断是否能继续移动的功能 聚集数据: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class BreakfastMenu : IEnumerable &#123; private readonly ArrayList _menuItems; public BreakfastMenu() &#123; _menuItems = new ArrayList(); AddItem("牛奶", "牛奶description", false, 3.0); AddItem("油条", "油条description", false, 1.0); AddItem("馒头", "馒头description", true, 1.0); AddItem("豆浆", "豆浆description", true, 1.5); &#125; public void AddItem(string name, string description, bool vegetarian, double price) &#123; var menuItem = new MenuItem(name, description, vegetarian, price); _menuItems.Add(menuItem); &#125; public IEnumerator GetEnumerator() &#123; return new BreakfastIterator(menuItems); &#125;&#125;public class DinnerMenu : IEnumerable &#123; private const int MaxItems = 6; private int _numberOfItems = 0; private readonly MenuItem[] _menuItems; public DinnerMenu() &#123; _menuItems = new MenuItem[MaxItems]; AddItem("香菇豆腐饭", "香菇豆腐", false, 10.5); AddItem("蛋炒饭", "哈哈", false, 8.5); AddItem("鱼香肉丝", "你猜", true, 15.5); &#125; public void AddItem(string name, string description, bool vegetarian, double price) &#123; var menuItem = new MenuItem(name, description, vegetarian, price); if (_numberOfItems &gt; MaxItems) &#123; Console.WriteLine("菜单已满"); &#125; else &#123; _menuItems[_numberOfItems++] = menuItem; &#125; &#125; public IEnumerator GetEnumerator() &#123; return new DinnerIterator(menuItems); &#125;&#125; 客户端调用 12345678910111213141516171819202122232425262728class Program &#123; static void Main(string[] args) &#123; //使用.NET自带接口 var breakfastMenu = new BreakfastMenu(); var dinnerMenu = new DinnerMenu(); Console.WriteLine("早餐:"); foreach (MenuItem menuItem in breakfastMenu) &#123; Print(menuItem); &#125; Console.WriteLine("\n晚餐:"); foreach (MenuItem menuItem in dinnerMenu) &#123; Print(menuItem); &#125; Console.ReadKey(); &#125; static void Print(MenuItem menuItem) &#123; Console.WriteLine($"&#123;menuItem.Name&#125; &#123;menuItem.Price&#125; &#123;menuItem.Description&#125;"); &#125;&#125; 可以看出上面使用迭代器模式实现时使用的迭代器类没有了,while (iterator.HasNext()) {...} 也没有了,foreach 究竟使用了什么魔法呢? 看一下这段对应的IL代码 IL代码 比较难看懂,重点关注红线的部分,所以大概等价于这么回事(只分析breakfastMenu 部分) 1234567BreakfastMenu breakfastMenu = new BreakfastMenu ();IEnumerator e = breakfastMenu.GetEnumerator ();while (e.MoveNext ()) &#123; Print ((MenuItem) e.Current);&#125; 看出来了么?.NET 帮我们在底层实现了调用接口方法并迭代,foreach只是语法糖 不过,尽管我们少写了很多代码,这里的实现和不用特性的代码相比,在代码结构上仍然高度相似,并且我们把大部分的精力都花在了实现迭代器和可迭代的类上面.到了NET2.0 ,由于有了yield return 关键字,实现起来将更加的简单优雅. .NET2.0有了yield return可以这么实现聚集数据: 注意GetEnumerator方法中使用了yield return 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class BreakfastMenu : IEnumerable &#123; ArrayList _menuItems; public BreakfastMenu() &#123; _menuItems = new ArrayList(); AddItem("牛奶", "牛奶description", false, 3.0); AddItem("油条", "油条description", false, 1.0); AddItem("馒头", "馒头description", true, 1.0); AddItem("豆浆", "豆浆description", true, 1.5); &#125; private void AddItem(string name, string description, bool vegetarian, double price) &#123; var menuItem = new MenuItem(name, description, vegetarian, price); _menuItems.Add(menuItem); &#125; public IEnumerator GetEnumerator() &#123; for (int i = 0; i &lt; _menuItems.Length; i++) &#123; yield return _menuItems[i]; &#125; &#125;&#125;public class DinnerMenu : IEnumerable &#123; static readonly int MaxItems = 6; private int _numberOfItems = 0; private readonly MenuItem[] _menuItems; public DinnerMenu() &#123; _menuItems = new MenuItem[MaxItems]; AddItem("香菇豆腐饭", "香菇豆腐", false, 10.5); AddItem("蛋炒饭", "哈哈", false, 8.5); AddItem("鱼香肉丝", "你猜", true, 15.5); &#125; private void AddItem(string name, string description, bool vegetarian, double price) &#123; var menuItem = new MenuItem(name, description, vegetarian, price); if (_numberOfItems &gt; MaxItems) &#123; Console.WriteLine("菜单已满"); &#125; else &#123; _menuItems[_numberOfItems++] = menuItem; &#125; &#125; public IEnumerator GetEnumerator() &#123; for (int i = 0; i &lt; _menuItems.Length; i++) &#123; yield return _menuItems[i]; &#125; &#125;&#125; 客户端调用不变,注意因为是固定数组所以要判一下空 1234567891011121314151617181920212223242526272829303132class Program &#123; static void Main(string[] args) &#123; //使用yield return var breakfastMenu = new BreakfastMenu(); var dinnerMenu = new DinnerMenu(); Console.WriteLine("早餐:"); foreach (MenuItem menuItem in breakfastMenu) &#123; Print(menuItem); &#125; Console.WriteLine("\n晚餐:"); foreach (MenuItem menuItem in dinnerMenu) &#123; if (menuItem == null) &#123; continue; &#125; Print(menuItem); &#125; Console.ReadKey(); &#125; static void Print(MenuItem menuItem) &#123; Console.WriteLine($"&#123;menuItem.Name&#125; &#123;menuItem.Price&#125; &#123;menuItem.Description&#125;"); &#125;&#125; 这次我们连迭代器接口都省略了! 那yield又是个啥? 只看这一段 1234567public IEnumerator GetEnumerator() &#123; for (int i = 0; i &lt; _menuItems.Length; i++) &#123; yield return _menuItems[i]; &#125;&#125; 对应的IL代码 有点看不懂了,不过还是发现了一点蛛丝马迹,红线的部分,状态机! 是的编译器在幕后构建了一个状态机,隐藏了复杂性.原理就是记录下程序状态,下次迭代从状态处开始,这样来实现MoveNext 作为理解原理,到这里已经足够了.如果要知道yield 的具体实现,必须要去研究CLI 的内容 详细看一下这一段状态机是怎么实现的 this.&lt;&gt;1__state 代表内部的状态,下面记做state this.&lt;&gt;4__this 代表当前类的实例 this.&lt;i&gt;E5__1 代表当前迭代的下标,下面记做index this.&lt;&gt;2__current 代表当前的迭代结果item state的初始状态为0,state为0时给index初始值0 index&gt;=menuItems.Count时返回false 其余情况给state设为1,返回true state的初始状态为1时++index 还是很好理解的 可见 C#一直在努力改进语法糖,让我们能更方便的写代码.但是其实,内部实现了迭代器模式 实现自己的迭代器(js和ruby实现)需求:判断两个已经排序的数组里的元素是否完全相同 1.实现一个each 函数each函数接受两个参数,第一个为被循环的数组,第二个为循环中每一步后将触发的回调函数123456789101112var each = function(arr, callback) &#123; for (var i = 0; i &lt; arr.length; i++) &#123; //把下标和元素当作参数传给callback函数 callback.call(arr[i], i, arr[i]); &#125;&#125;;//调用each([1, 2, 3], function(i, n) &#123; console.log([i, n]);&#125;); 2.内部迭代器和外部迭代器内部迭代器上面的each函数属于内部迭代器,外界不需关心迭代器内部实现 然而,我们需要稍微改动each函数才能实现我们的需求 12345678var each = function(arr, callback) &#123; for (var i = 0; i &lt; arr.length; i++) &#123; if (callback(arr[i], i) === false) &#123; return false; &#125; &#125;&#125;; 1234567891011121314151617181920212223var isSameArray = function(arr1, arr2) &#123; if (arr1.length !== arr2.length) &#123; return false; &#125; var isSame = each(arr1, function(item, index) &#123; if (item !== arr2[index]) &#123; return false; &#125; &#125;); if (isSame === false) &#123; return false; &#125; return true;&#125;;//调用console.log(isSameArray([1,2,3], [1,2,3]));//trueconsole.log(isSameArray([1,2,3], [1,2,4]));//false 上面的isSameArray函数实现的相当难看,能够实现功能得益于在js 中可以把函数当作参数传递的特性,但是在其他语言中未必可行 在没有闭包的语言中,内部迭代器本身的实现相当复杂,比如C语言实现内部迭代器就就需要用到函数指针,循环处理所需要的数据都要以参数的形式明确地从外面传递进去 外部迭代器外部迭代器必须显式地请求迭代下一个元素。外部迭代器增加了一些调用的复杂度，但相对也增强了迭代器的灵活性，我们可以手工控制迭代的过程或者顺序。 下面这个外部迭代器的实现来自《松本行弘的程序世界》第4 章，原例用Ruby 写成 12345678910111213141516171819202122class ArrayIterator def initialize(array) @array = array @current = 0 end def first() @current = 0 end def next() @current += 1 end def is_done() return @current &gt;= @array.size() end def current_item return @array.get(@current) endend 改成js 并且实现的上面的需求 ES5版本: 123456789101112131415161718192021222324252627282930313233343536373839404142434445var Iterator = function(arr) &#123; var index = 0; var next = function() &#123; index++; &#125;; var hasNext = function() &#123; return index &lt; arr.length; &#125;; var getCurrentItem = function() &#123; return arr[index]; &#125;; return &#123; next: next, hasNext: hasNext, getCurrentItem: getCurrentItem &#125;;&#125;;var isSameArray = function(iterator1, iterator2) &#123; while (iterator1.hasNext() &amp;&amp; iterator2.hasNext()) &#123; if (iterator1.getCurrentItem() !== iterator2.getCurrentItem()) &#123; return false; &#125; iterator1.next(); iterator2.next(); &#125; return true;&#125;;//调用var iterator1 = Iterator([1, 2, 3]);var iterator2 = Iterator([1, 2, 3]);var iterator3 = Iterator([1, 4, 3]);var iterator4 = Iterator([1, 5, 3]);console.log(isSameArray(iterator1, iterator2));//trueconsole.log(isSameArray(iterator3, iterator4));//false ES6版本,其实这里Iterator最好用class实现 1234567891011121314151617181920212223242526272829303132333435363738394041const Iterator = arr =&gt; &#123; let index = 0; const next = () =&gt; &#123; index++; &#125;; const hasNext = () =&gt; index &lt; arr.length; const getCurrentItem = () =&gt; arr[index]; return &#123; next, hasNext, getCurrentItem &#125;;&#125;;const isSameArray = (iterator1, iterator2) =&gt; &#123; while (iterator1.hasNext() &amp;&amp; iterator2.hasNext()) &#123; if (iterator1.getCurrentItem() !== iterator2.getCurrentItem()) &#123; return false; &#125; iterator1.next(); iterator2.next(); &#125; return true;&#125;;//调用const iterator1 = Iterator([1, 2, 3]);const iterator2 = Iterator([1, 2, 3]);const iterator3 = Iterator([1, 4, 3]);const iterator4 = Iterator([1, 5, 3]);console.log(isSameArray(iterator1, iterator2));//trueconsole.log(isSameArray(iterator3, iterator4));//false 浏览器上传组件(js实现)重构某个项目中文件上传模块的代码时，发现了下面这段代码，它的目的是根据不同的浏览器获取相应的上传组件对象： 12345678910111213var getUploadObj = function () &#123; try &#123; return new ActiveXObject("TXFTNActiveX.FTNUpload"); // IE 上传控件 &#125; catch (e) &#123; if (supportFlash()) &#123; // supportFlash 函数未提供 var str = '&lt;object type="application/x-shockwave-flash"&gt;&lt;/object&gt;'; return $(str).appendTo($('body')); &#125; else &#123; var str = '&lt;input name="file" type="file"/&gt;'; // 表单上传 return $(str).appendTo($('body')); &#125; &#125;&#125; 为了得到一个upload 对象，这个getUploadObj 函数里面充斥了try，catch以及if 条件分支。缺点是显而易见的: 第一是很难阅读 第二是严重违反开闭原则 后来我们还增加了一些另外的上传方式，比如HTML5 上传，此时唯一的办法是继续往getUploadObj 函数里增加条件分支。 现在来梳理一下问题，目前一共有3 种可能的上传方式，我们不知道目前正在使用的浏览器支持哪几种。于是从第一种方式开始，迭代所有方式进行尝试，直到找到了正确的方式为止。做法如下 把每种获取upload 对象 的方法都封装在各自的函数里 使用一个迭代器，迭代获取这些upload 对象 ，直到获取到一个可用的为止： 1234567891011121314151617181920var getActiveUploadObj = function () &#123; try &#123; return new ActiveXObject("TXFTNActiveX.FTNUpload"); // IE 上传控件 &#125; catch (e) &#123; return false; &#125;&#125;;var getFlashUploadObj = function () &#123; if (supportFlash()) &#123; // supportFlash 函数未提供 var str = '&lt;object type="application/x-shockwave-flash"&gt;&lt;/object&gt;'; return $(str).appendTo($('body')); &#125; return false;&#125;;var getFormUpladObj = function () &#123; var str = '&lt;input name="file" type="file" class="ui-file"/&gt;'; // 表单上传 return $(str).appendTo($('body'));&#125;; 在getActiveUploadObj、getFlashUploadObj、getFormUpladObj 这3 个函数中都有同一个约定：如果该函数里面的upload 对象是可用的，则让函数返回该对象，反之返回false，提示迭代器继续往后面进行迭代。 所以我们的迭代器只需进行下面这几步工作 提供一个可以被迭代的方法，使得getActiveUploadObj，getFlashUploadObj 以及getFlashUploadObj依照优先级被循环迭代 如果正在被迭代的函数返回一个对象，表示找到了正确的upload 对象，反之如果该函数返回false，则让迭代器继续工作。 123456789101112var iteratorUploadObj = function () &#123; for (var i = 0, fn; fn = arguments[i++];) &#123; var uploadObj = fn(); if (uploadObj !== false) &#123; return uploadObj; &#125; &#125;&#125;;var uploadObj = iteratorUpload(getActiveUploadObj, getFlashUploadObj, getFormUpladObj); 重构代码之后，获取不同上传对象的方法被隔离在各自的函数里互不干扰，try、catch 和if 分支不再纠缠在一起。 并且我们可以很方便地的维护和扩展代码。比如，后来我们又给上传项目增加了Webkit 控件上传和HTML5 上传，我们要做的仅仅是下面一些工作。 增加分别获取Webkit 控件上传对象和HTML5 上传对象的函数： 123456var getWebkitUploadObj = function()&#123;// 具体代码略&#125;;var getHtml5UploadObj = function()&#123;// 具体代码略&#125;; 依照优先级把它们添加进迭代器： 123456var uploadObj = iteratorUploadObj( getActiveUploadObj, getWebkitUploadObj, getFlashUploadObj, getHtml5UploadObj, getFormUpladObj ); 计数器(python实现)123456789101112131415161718192021222324252627282930"""http://ginstrom.com/scribbles/2007/10/08/design-patterns-python-style/""""""Implementation of the iterator pattern with a generator"""def count_to(count): """Counts by word numbers, up to a maximum of five""" numbers = ["one", "two", "three", "four", "five"] # enumerate() returns a tuple containing a count (from start which # defaults to 0) and the values obtained from iterating over sequence for pos, number in zip(range(count), numbers): yield number# Test the generatorcount_to_two = lambda: count_to(2)count_to_five = lambda: count_to(5)print()print('Counting to two...')for number in count_to_two(): print(number, end=' ')print()print()print('Counting to five...')for number in count_to_five(): print(number, end=' ')print()]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[观察观察者模式]]></title>
    <url>%2F2018%2F11%2F25%2F%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[[TOC] 示例股票变化(C#实现)监控某一个公司(Microsoft)的股票价格变化，可以有多种方式，通知的对象可以是投资者，或者是发送到移动设备，还有电子邮件等 一开始我们先不考虑Observer模式，通过一步步地重构，最终重构为Observer模式 现在有这样两个类：Microsoft和Investor，如下图所示： 123456789101112131415161718192021222324252627282930313233public class Microsoft&#123; public void Update() &#123; Investor.SendData(this); &#125; public Investor Investor &#123; get; set; &#125; public String Symbol &#123; get; set; &#125; public double Price &#123; get; set; &#125;&#125;public class Investor&#123; private string _name; public Investor(string name) &#123; this._name = name; &#125; public void SendData(Microsoft ms) &#123; Console.WriteLine("Notified &#123;0&#125; of &#123;1&#125;'s " + "change to &#123;2:C&#125;", _name, ms.Symbol, ms.Price); &#125;&#125; 客户端: 123456789101112131415161718192021class Program&#123; static void Main(string[] args) &#123; Investor investor = new Investor("Jom"); Microsoft ms = new Microsoft &#123; Investor = investor, Symbol = "Microsoft", Price = 120.00 &#125;; ms.Update(); Console.ReadLine(); &#125;&#125; 运行后结果如下： 1Notified Jom of Microsoft&apos;s change to ￥120 可以看到，这段代码运行并没有问题，也确实实现了我们最初的设想的功能，把Microsoft的股票价格变化通知到了Jom投资者那儿。 但是这里面出现了如下几个问题： Microsoft和Investor之间形成了一种双向的依赖关系，即Microsoft调用了Investor的方法，而Investor调用了Microsoft类的属性。如果有其中一个类变化，有可能会引起另一个的变化。 当出现一种的通知对象，比如说是移动设备Mobile： 123456789101112131415161718public class Mobile &#123; private string _no; public Mobile(string no) &#123; this._no = no; &#125; public void SendData(Microsoft ms) &#123; Console.WriteLine("Notified &#123;0&#125; of &#123;1&#125;'s " + "change to &#123;2:C&#125;", _no, ms.Symbol, ms.Price); &#125; &#125; 这时候对应的Microsoft的类就应该改变为如下代码，在Microsoft类中增加Mobile，同时修改Update()方法使其可以通知到移动设备： 123456789101112131415public class Microsoft &#123; public void Update() &#123; Investor.SendData(this); Mobile.SendData(this); &#125; public Mobile Mobile &#123; get; set; &#125; public Investor Investor &#123; get; set; &#125; public String Symbol &#123; get; set; &#125; public double Price &#123; get; set; &#125; &#125; 显然这样的设计极大的违背了“开放-封闭”原则，这不是我们所想要的。 对此做进一步的抽象，既然出现了多个通知对象，我们就为这些对象之间抽象出一个接口，用它来取消Microsoft和具体的通知对象之间依赖。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public interface IObserver &#123; void SendData(Microsoft ms); &#125; public class Microsoft &#123; public void Update() &#123; Investor.SendData(this); //Mobile.SendData(this); &#125; //public Mobile Mobile &#123; get; set; &#125; public IObserver Investor &#123; get; set; &#125; public String Symbol &#123; get; set; &#125; public double Price &#123; get; set; &#125; &#125; public class Investor: IObserver &#123; private string _name; public Investor(string name) &#123; this._name = name; &#125; public void SendData(Microsoft ms) &#123; Console.WriteLine("Notified &#123;0&#125; of &#123;1&#125;'s " + "change to &#123;2:C&#125;", _name, ms.Symbol, ms.Price); &#125; &#125; public class Mobile &#123; private string _no; public Mobile(string no) &#123; this._no = no; &#125; public void SendData(Microsoft ms) &#123; Console.WriteLine("Notified &#123;0&#125; of &#123;1&#125;'s " + "change to &#123;2:C&#125;", _no, ms.Symbol, ms.Price); &#125; &#125; 可以看到，我们在降低两者的依赖性上已经迈进了一小步，正在朝着弱依赖性这个方向变化。在Microsoft类中已经不再依赖于具体的Investor，而是依赖于接口IObserver。 但同时我们看到，再新出现一个移动设备这样的通知对象，Microsoft类仍然需要改变，对此我们再做如下重构，在Microsoft中维护一个IObserver列表，同时提供相应的维护方法。 此时的实现 1234567891011121314151617181920212223242526272829public class Microsoft &#123; public void Update() &#123; foreach (IObserver observer in observers) &#123; observer.SendData(this); &#125; &#125; public void AddObserver(IObserver observer) &#123; observers.Add(observer); &#125; public void RemoveObserver(IObserver observer) &#123; observers.Remove(observer); &#125; private List&lt;IObserver&gt; observers = new List&lt;IObserver&gt;(); public IObserver Investor &#123; get; set; &#125; public String Symbol &#123; get; set; &#125; public double Price &#123; get; set; &#125; &#125; 客户端 123456789101112131415161718192021222324class Program &#123; static void Main(string[] args) &#123; IObserver investor1 = new Investor("Tom"); IObserver investor2 = new Investor("Jerry"); Microsoft ms = new Microsoft &#123; Symbol = "Microsoft", Price = 120.00 &#125;; ms.AddObserver(investor1); ms.AddObserver(investor2); ms.Update(); Console.ReadLine(); &#125; &#125; 走到这一步，已经有了Observer模式的影子了，Microsoft类不再依赖于具体的Investor，而是依赖于抽象的IOberver。 存在着的一个问题是Investor仍然依赖于具体的公司Microsoft，况且公司还会有很多IBM，Google等，解决这样的问题很简单，只需要再对Microsoft类做一次抽象。如下图所示： 此时的实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public interface IObserver &#123; void SendData(Stock stock); &#125; public abstract class Stock &#123; private List&lt;IObserver&gt; observers = new List&lt;IObserver&gt;(); public Stock(String symbol, double price) &#123; this.Symbol = symbol; this.Price = price; &#125; public void Update() &#123; foreach (IObserver ob in observers) &#123; ob.SendData(this); &#125; &#125; public void AddObserver(IObserver observer) &#123; observers.Add(observer); &#125; public void RemoveObserver(IObserver observer) &#123; observers.Remove(observer); &#125; public String Symbol &#123; get; &#125; public double Price &#123; get; &#125; &#125; public class Microsoft : Stock &#123; public Microsoft(String symbol, double price) : base(symbol, price) &#123; &#125; &#125; public class Investor : IObserver &#123; private string _name; public Investor(string name) &#123; this._name = name; &#125; public void SendData(Stock stock) &#123; Console.WriteLine("Notified &#123;0&#125; of &#123;1&#125;'s " + "change to &#123;2:C&#125;", _name, stock.Symbol, stock.Price); &#125; &#125; 客户端: 123456789101112131415class Program &#123; static void Main(string[] args) &#123; Stock ms = new Microsoft("Microsoft", 120.00); ms.AddObserver(new Investor("Tom")); ms.AddObserver(new Investor("Jerry")); ms.Update(); Console.ReadLine(); &#125; &#125; 到这里我们可以看到，通过不断的重构，不断地抽象，我们由一开始的很糟糕的设计，逐渐重构为使用Observer模式的这样一个方案。 在这个例子里面，IOberser充当了观察者的角色，而Stock则扮演了主题对象角色，在任何时候，只要调用了Stock的Update()方法，它就会通知它的所有观察者对象。 同时可以看到，通过Observer模式，取消了直接依赖，变为间接依赖，这样大大提供了系统的可维护性和可扩展性。 .NET中的观察者模式 利用事件和委托来实现Observer模式我认为更加的简单和优雅，也是一种更好的解决方案。 因为在上面的示例中我们可以看到，虽然取消了直接耦合，但是又引入了不必要的约束（暂且这么说吧）。即那些子类必须都继承于主题父类，还有观察者接口等。 上面的例子简单的用事件和委托实现如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Program &#123; static void Main (string[] args) &#123; Stock stock = new Stock ("Microsoft", 120.00); Investor investor = new Investor ("Tom"); stock.NotifyEvent += new NotifyEventHandler (investor.SendData); stock.Update (); Console.ReadLine (); &#125;&#125;public delegate void NotifyEventHandler (object sender);public class Stock &#123; public NotifyEventHandler NotifyEvent; private String _symbol; private double _price; public Stock (String symbol, double price) &#123; this._symbol = symbol; this._price = price; &#125; public void Update () &#123; OnNotifyChange (); &#125; public void OnNotifyChange () &#123; if (NotifyEvent != null) &#123; NotifyEvent (this); &#125; &#125; public String Symbol &#123; get &#123; return _symbol; &#125; &#125; public double Price &#123; get &#123; return _price; &#125; &#125;&#125;public class Investor &#123; private string _name; public Investor (string name) &#123; this._name = name; &#125; public void SendData (object obj) &#123; if (obj is Stock) &#123; Stock stock = (Stock) obj; Console.WriteLine ("Notified &#123;0&#125; of &#123;1&#125;'s " + "change to &#123;2:C&#125;", _name, stock.Symbol, stock.Price); &#125; &#125;&#125; 把无返回值委托换成action 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Program &#123; static void Main(string[] args) &#123; Stock1 stock = new Stock1("Microsoft", 120.00); Investor1 investor = new Investor1("Tom"); stock.NotifyEvent += investor.SendData; stock.Update(); Console.ReadLine(); &#125; &#125; // public delegate void NotifyEventHandler(object sender); //public Action&lt;object&gt; NotifyEventHandler; public class Stock1 &#123; public event Action&lt;object&gt; NotifyEvent; public Stock1(string symbol, double price) &#123; this.Symbol = symbol; this.Price = price; &#125; public void Update() &#123; OnNotifyChange(); &#125; public void OnNotifyChange() &#123; NotifyEvent?.Invoke(this); &#125; public string Symbol &#123; get; &#125; public double Price &#123; get; &#125; &#125; public class Investor1 &#123; private readonly string _name; public Investor1(string name) &#123; this._name = name; &#125; public void SendData(object obj) &#123; if (obj is Stock1 stock) &#123; Console.WriteLine("Notified &#123;0&#125; of &#123;1&#125;'s " + "change to &#123;2:C&#125;", _name, stock.Symbol, stock.Price); &#125; &#125; &#125; 钓鱼(C#实现)鱼竿是被观察者，铃铛是通知工具，垂钓者是观察者。 鱼儿咬钩，鱼竿通过铃铛通知垂钓者收钩。 先来定义鱼的品类枚举： 123456789public enum FishType&#123; 鲫鱼, 鲤鱼, 黑鱼, 青鱼, 草鱼, 鲈鱼&#125; 接下来申明一个钓鱼工具的抽象类，维护订阅者列表，并负责循环通知订阅者。 1234567891011121314151617181920212223242526272829303132/// &lt;summary&gt; /// 钓鱼工具抽象类 /// 用来维护订阅者列表，并通知订阅者 /// &lt;/summary&gt; public abstract class FishingTool &#123; private readonly List&lt;ISubscriber&gt; _subscribers; protected FishingTool() &#123; _subscribers = new List&lt;ISubscriber&gt;(); &#125; public void AddSubscriber(ISubscriber subscriber) &#123; if (!_subscribers.Contains(subscriber)) _subscribers.Add(subscriber); &#125; public void RemoveSubscriber(ISubscriber subscriber) &#123; if (_subscribers.Contains(subscriber)) _subscribers.Remove(subscriber); &#125; public void Notify(FishType type) &#123; foreach (var subscriber in _subscribers) subscriber.Update(type); &#125; &#125; 鱼竿的实现，这里用随机数模拟鱼儿咬钩： 123456789101112131415161718/// &lt;summary&gt;/// 鱼竿/// &lt;/summary&gt;public class FishingRod : FishingTool&#123; public void Fishing() &#123; Console.WriteLine("开始下钩！"); //用随机数模拟鱼咬钩，若随机数为偶数，则为鱼咬钩 if (new Random().Next() % 2 == 0) &#123; var type = (FishType)new Random().Next(0, 5); Console.WriteLine("铃铛：叮叮叮，鱼儿咬钩了"); Notify(type); &#125; &#125;&#125; 定义简单的观察者接口： 12345678/// &lt;summary&gt; /// 订阅者（观察者）接口 /// 由具体的订阅者实现Update()方法 /// &lt;/summary&gt; public interface ISubscriber &#123; void Update(FishType type); &#125; 垂钓者实现观察者接口，并定义了Name，FishCount 属性： 12345678910111213141516171819/// &lt;summary&gt; /// 垂钓者实现观察者接口 /// &lt;/summary&gt; public class FishingMan : ISubscriber &#123; public FishingMan(string name) &#123; Name = name; &#125; public string Name &#123; get; set; &#125; public int FishCount &#123; get; set; &#125; public void Update(FishType type) &#123; FishCount++; Console.WriteLine("&#123;0&#125;：钓到一条[&#123;2&#125;]，已经钓到&#123;1&#125;条鱼了！", Name, FishCount, type); &#125; &#125; 测试: 123456789101112131415161718192021222324252627class Program &#123; static void Main(string[] args) &#123; Console.WriteLine("简单实现的观察者模式："); Console.WriteLine("======================="); //1、初始化鱼竿 var fishingRod = new FishingRod(); //2、声明垂钓者 var fisher1 = new FishingMan("fisher1"); var fisher2 = new FishingMan("fisher2"); //3、将垂钓者观察鱼竿 fishingRod.AddSubscriber(fisher1); fishingRod.AddSubscriber(fisher2); //4、循环钓鱼 while (fisher1.FishCount &lt; 5 || fisher2.FishCount &lt; 5) &#123; fishingRod.Fishing(); Console.WriteLine("-------------------"); //睡眠0.5s Thread.Sleep(500); &#125; &#125; &#125; 用委托实现有了委托，我们就不再需要定义专门的抽象被观察者对象了，直接实现鱼竿： 12345678910111213141516171819202122/// &lt;summary&gt; /// 鱼竿 /// &lt;/summary&gt; public class FishingRod &#123; public delegate void FishingHandler(FishType type); //声明委托 public event FishingHandler FishingEvent; //声明事件 public void Fishing() &#123; Console.WriteLine("开始下钩！"); //用随机数模拟鱼咬钩，若随机数为偶数，则为鱼咬钩 if (new Random().Next() % 2 == 0) &#123; var a = new Random(10).Next(); var type = (FishType)new Random().Next(0, 5); Console.WriteLine("铃铛：叮叮叮，鱼儿咬钩了"); FishingEvent?.Invoke(type); &#125; &#125; &#125; 因为被观察者定义了委托，我们也没必要定义专门的观察者接口，只需要在具体的观察者中实现对应的委托即可。 12345678910111213141516171819/// &lt;summary&gt;/// 垂钓者(观察者)/// &lt;/summary&gt;public class FishingMan&#123; public FishingMan(string name) &#123; Name = name; &#125; public string Name &#123; get; set; &#125; public int FishCount &#123; get; set; &#125; public void Update(FishType type) &#123; FishCount++; Console.WriteLine("&#123;0&#125;：钓到一条[&#123;2&#125;]，已经钓到&#123;1&#125;条鱼了！", Name, FishCount, type); &#125;&#125; 客户端: 1234567891011121314151617181920212223242526class Program &#123; static void Main(string[] args) &#123; Console.WriteLine("委托实现的观察者模式："); Console.WriteLine("======================="); //1、初始化鱼竿 var fishingRod = new FishingRod(); //2、声明垂钓者 var fisher = new FishingMan("fisher1"); //3、注册观察者 fishingRod.FishingEvent += fisher.Update; //4、循环钓鱼 while (fisher.FishCount &lt; 5) &#123; fishingRod.Fishing(); Console.WriteLine("-------------------"); //睡眠0.5s Thread.Sleep(500); &#125; &#125; &#125; 进制转换器(python实现)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788"""http://code.activestate.com/recipes/131499-observer-pattern/"""class Subject(object): def __init__(self): self._observers = [] def attach(self, observer): if not observer in self._observers: self._observers.append(observer) def detach(self, observer): try: self._observers.remove(observer) except ValueError: pass def notify(self, modifier=None): for observer in self._observers: if modifier != observer: observer.update(self)# Example usageclass Data(Subject): def __init__(self, name=''): Subject.__init__(self) self.name = name self._data = 0 @property def data(self): return self._data @data.setter def data(self, value): self._data = value self.notify()class HexViewer: def update(self, subject): print('HexViewer: Subject %s has data 0x%x' % (subject.name, subject.data))class DecimalViewer: def update(self, subject): print('DecimalViewer: Subject %s has data %d' % (subject.name, subject.data))# Example usage...def main(): data1 = Data('Data 1') data2 = Data('Data 2') view1 = DecimalViewer() view2 = HexViewer() data1.attach(view1) data1.attach(view2) data2.attach(view2) data2.attach(view1) print("\nDetachSetting Data 1 = 10") data1.data = 10 print("\nSetting Data 2 = 15") data2.data = 15 print("\nSetting Data 1 = 3") data1.data = 3 print("\nSetting Data 2 = 5") data2.data = 5 print("\nDetach HexViewer from data1 and data2.") data1.detach(view2) data2.detach(view2) print("\nSetting Data 1 = 10") data1.data = 10 print("\nSetting Data 2 = 15") data2.data = 15 if __name__ == '__main__': main() 登录网站模块(JavaScript实现)JavaScript中的订阅-发布模式和别的语言(比如Java)中的实现还是有区别的. 在Java中,通常会把订阅者对象自身当成引用传入发布者对象中,同时订阅者对象还需提供一个名为诸如update 的方法,供发布者对象在合适的时候调用 在JavaScript中,用注册回调函数的形式来代替传统的发布-订阅模式,显得更加优雅和简单 注:在这里的回调其实就相当于例1中的事件委托stock.NotifyEvent += new NotifyEventHandler (investor.SendData);.NET使用事件封装了发布订阅模式,语法糖更易用,但是不可能达到解释型语言的灵活性 假如我们正在开发一个商城网站,网站里有header头部,nav 导航,消息列表,购物车等模块.这几个模块渲染有一个共同的前提条件,就是必须先用ajax 异步请求获取用户的登录信息 1234567login.succ(function(data) &#123; header.setAvatar(data.avatar); //设置header模块的头像 nav.setAvatar(data.avatar);//设置导航模块的头像 message.refresh()//刷新消息列表 cart.refresh()//刷新购物车列表 &#125;); header,nav等各个模块和用户信息产生了强耦合,方法名也不能随意再改变 等到有一天,项目中增加了一个收货地址管理的模块,则必须在原来的逻辑中增加 1address.refresh() 用发布订阅模式重写之后,登录模块不用关心业务模块究竟做什么,也不想了解内部细节 12345678910111213141516171819202122$.ajax('http://xxx.com?login', function(data) &#123; login.trigger('loginSucc', data);&#125;);var header = (function() &#123; // header模块 login.listen('loginSucc', function() &#123; header.setAvatar(data.avatar); &#125;) return setAvatar:function(avatar)&#123; console.log('设置header模块的头像') &#125;&#125;)();var nav = (function() &#123; // header模块 login.listen('loginSucc', function() &#123; nav.setAvatar(data.avatar); &#125;) return setAvatar:function(avatar)&#123; console.log('设置nav模块的头像') &#125;&#125;)(); 我们可以随时更改setAvatar 方法名 如果某天需求增加了刷新收货列表的行为,只需要增加监听消息的方法即可 12345678var address = (function() &#123; //address模块 login.listen('loginSucc', function() &#123; address.refresh(); &#125;) return refresh: function() &#123; console.log('刷新收货地址列表') &#125;&#125;)(); 事件管理器(JavaScript实现)使用发布订阅模式写一个事件管理器，可以实现如下方式调用 12345EventManager.on('text:change', function(val)&#123; console.log('text:change... now val is ' + val); &#125;);EventManager.trigger('text:change', '饥人谷');EventManager.off('text:change'); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879//ES5var EventManager = (function() &#123; var events = &#123;&#125;; function on(evt, handler) &#123; if (typeof handler !== 'function') &#123; throw new TypeError('handler is not a function!'); return; &#125; events[evt] = events[evt] || []; events[evt].push(handler); &#125; function trigger(evt, args) &#123; if (!events[evt]) &#123; return; &#125; events[evt].forEach(function(handler) &#123; handler(args); &#125;) &#125; function off(evt) &#123; delete events[evt]; &#125; return &#123; on: on, trigger: trigger, off: off &#125;;&#125;)();//ES6const EventManager = ((() =&gt; &#123; const events = &#123;&#125;; function on(evt, handler) &#123; if (typeof handler !== 'function') &#123; throw new TypeError('handler is not a function!'); return; &#125; events[evt] = events[evt] || []; events[evt].push(handler); &#125; function trigger(evt, args) &#123; if (!events[evt]) &#123; return; &#125; events[evt].forEach(handler =&gt; &#123; handler(args); &#125;) &#125; function off(evt) &#123; delete events[evt]; &#125; return &#123; on, trigger, off &#125;;&#125;))();//调用EventManager.on('text:change', function(val)&#123; console.log('text:change... now val is ' + val); &#125;);EventManager.trigger('text:change', '饥人谷');EventManager.off('text:change');]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java OOP]]></title>
    <url>%2F2018%2F11%2F17%2FJava-OOP%2F</url>
    <content type="text"><![CDATA[封装(Encapsulation )隐藏内部细节 1234567891011121314151617181920212223242526//save as Student.javapackage com.javatpoint;public class Student &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125;//save as Test.javapackage com.javatpoint;class Test &#123; public static void main(String[] args) &#123; Student s = new Student(); s.setName("vijay"); System.out.println(s.getName()); &#125;&#125; 1234Compile By: javac -d . Test.javaRun By: java com.javatpoint.TestOutput: vijay 继承(Inheritance )123456789101112131415161718192021222324252627282930class Calculation &#123; int z; public void addition(int x, int y) &#123; z = x + y; System.out.println("The sum of the given numbers:" + z); &#125; public void Subtraction(int x, int y) &#123; z = x - y; System.out.println("The difference between the given numbers:" + z); &#125;&#125;public class My_Calculation extends Calculation &#123; public void multiplication(int x, int y) &#123; z = x * y; System.out.println("The product of the given numbers:" + z); &#125; public static void main(String[] args) &#123; int a = 20; int b = 10; My_Calculation demo = new My_Calculation(); demo.addition(a, b); demo.Subtraction(a, b); demo.multiplication(a, b); &#125;&#125; My_Calculation使用了继承自Calculation的方法addition和Subtraction以及变量z,只有multiplication是My_Calculation 自身定义的方法 避免了重复 多态(Polymorphism )静态编译时多态例1方法重载(overload)在编译时确定 123456789101112131415class Adder &#123; static int add(int a, int b) &#123; return a + b; &#125; static double add(double a, double b) &#123; return a + b; &#125;&#125;public static void main(String args[])&#123; System.out.println(Adder.add(11,11)); System.out.println(Adder.add(12.3,12.6));&#125; 例2方法的参数在编译阶段常被静态地绑定 在whichFoo方法中，形式参数arg2的类型是Base, 因此不管arg2实际引用的是什么类型，arg1.foo(arg2)匹配的foo都将是foo(Base) 123456789101112131415161718192021222324252627282930313233343536public class Main &#123; public static void main(String[] args) &#123; Base b = new Base(); Derived d = new Derived(); whichFoo(b, b); whichFoo(b, d); whichFoo(d, b); whichFoo(d, d); &#125; public static void whichFoo(Base arg1, Base arg2) &#123; arg1.foo(arg2); &#125;&#125;class Base &#123; public void foo(Base x) &#123; System.out.println("Base.Base"); &#125; public void foo(Derived x) &#123; System.out.println("Base.Derived"); &#125;&#125;class Derived extends Base &#123; public void foo(Base x) &#123; System.out.println("Derived.Base"); &#125; public void foo(Derived x) &#123; System.out.println("Derived.Derived"); &#125;&#125; 动态运行时多态例1 方法重写(override)在运行时确定 12345678910111213141516public Class BowlerClass&#123; void bowlingMethod() &#123; System.out.println(" bowler "); &#125; public Class FastPacer&#123; void bowlingMethod() &#123; System.out.println(" fast bowler "); &#125; Public static void main(String[] args) &#123; FastPacer obj = new FastPacer(); obj.bowlingMethod(); // fast bowler &#125; &#125; 例212345678910111213141516171819202122232425262728293031323334public class Main &#123; public static void main(String[] args) &#123; Father father = new Son(); System.out.println(father.age); father.name(); father.age(); &#125;&#125;class Father &#123; public int age = 60; public static void name() &#123; System.out.println("father name"); &#125; public void age() &#123; System.out.println("father age:" + age); &#125;&#125;class Son extends Father &#123; public int age = 25; public static void name() &#123; System.out.println("son name"); &#125; public void age() &#123; System.out.println("Son age:" + age); &#125;&#125; 1234output:60father nameSon age:25 当执行 Father father = new Son();发生了向上转型，在编译期间 father就是个Father对象，系统不知道实际上它是一个 Son对象，这得在运行期间由JVM判断 在我们调用father.age的时候实际上，在处理java类中的成员变量时，并不是采用运行时绑定，而是一般意义上的静态绑定，即调用的是Father类的age成员变量 在调用father.name()的时候，注意这是个static方法，java当中的方法final，static，private和构造方法是前期绑定，因此调用的是Father类中的name方法 在调用father.age()的时候，需要采用动态绑定，此时father会被解析成它实际的对象，即Son对象，因此实际调用的是Son.age() 参考https://stackify.com/oops-concepts-in-java/ https://www.edureka.co/blog/object-oriented-programming/ https://blog.csdn.net/zlp1992/article/details/52557238]]></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java基础语法CheatSheet]]></title>
    <url>%2F2018%2F11%2F05%2FJava%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95CheatSheet%2F</url>
    <content type="text"><![CDATA[Java Programming Cheatsheetfrom https://introcs.cs.princeton.edu/java/11cheatsheet/This appendix summarizes the most commonly used Java language features and APIs in the textbook. Hello, World. Editing, compiling, and executing. Built-in data types. Declaration and assignment statements. Integers. ! Floating-point numbers. Booleans. Comparison operators. Printing. Parsing command-line arguments. Math library. Java library calls. Type conversion. Anatomy of an if statement. If and if-else statements. Nested if-else statement. Anatomy of a while loop. Anatomy of a for loop. Loops. Break statement. Do-while loop. Switch statement. Arrays. Inline array initialization. Typical array-processing code. Two-dimensional arrays. Inline initialization. Command line. Redirection and piping. Functions. Libraries of functions. Using an object. Instance variables. Constructors. Instance methods. Classes. Object-oriented libraries. Java’s String data type. The full java.lang.String API Java’s Color data type. The full java.awt.Color API Iterable.]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java IO]]></title>
    <url>%2F2018%2F11%2F03%2FJava%20IO%2F</url>
    <content type="text"></content>
      <categories>
        <category>Java</category>
        <category>Java基础</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[[置顶]我的博文目录]]></title>
    <url>%2F2018%2F11%2F03%2F%5B%E7%BD%AE%E9%A1%B6%5D%E6%88%91%E7%9A%84%E5%8D%9A%E6%96%87%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Java Java开发环境配置 Java OOP Java基础语法CheateSheet Java IO 硬核C Sharp 转载-IL汇编语言介绍（译） 设计模式 六个创建型模式 简单工厂模式-Simple Factory Pattern【学习难度：★★☆☆☆，使用频率：★★★☆☆】 工厂方法模式-Factory Method Pattern【学习难度：★★☆☆☆，使用频率：★★★★★】 抽象工厂模式-Abstract Factory Pattern【学习难度：★★★★☆，使用频率：★★★★★】 单例模式-Singleton Pattern【学习难度：★☆☆☆☆，使用频率：★★★★☆】 原型模式-Prototype Pattern【学习难度：★★★☆☆，使用频率：★★★☆☆】 建造者模式-Builder Pattern【学习难度：★★★★☆，使用频率：★★☆☆☆】 七个结构型模式 适配器模式-Adapter Pattern【学习难度：★★☆☆☆，使用频率：★★★★☆】 桥接模式-Bridge Pattern【学习难度：★★★☆☆，使用频率：★★★☆☆】 组合模式-Composite Pattern【学习难度：★★★☆☆，使用频率：★★★★☆】 装饰模式-Decorator Pattern【学习难度：★★★☆☆，使用频率：★★★☆☆】 外观模式-Facade Pattern【学习难度：★☆☆☆☆，使用频率：★★★★★】 享元模式-Flyweight Pattern【学习难度：★★★★☆，使用频率：★☆☆☆☆】 代理模式-Proxy Pattern【学习难度：★★★☆☆，使用频率：★★★★☆】 十一个行为型模式 职责链模式-Chain of Responsibility Pattern【学习难度：★★★☆☆，使用频率：★★☆☆☆】 命令模式)-Command Pattern【学习难度：★★★☆☆，使用频率：★★★★☆】 解释器模式-Interpreter Pattern【学习难度：★★★★★，使用频率：★☆☆☆☆】 迭代器模式-Iterator Pattern【学习难度：★★★☆☆，使用频率：★★★★★】 中介者模式)-Mediator Pattern【学习难度：★★★☆☆，使用频率：★★☆☆☆】 备忘录模式-Memento Pattern【学习难度：★★☆☆☆，使用频率：★★☆☆☆】 观察者模式-Observer Pattern【学习难度：★★★☆☆，使用频率：★★★★★】 状态模式-State Pattern【学习难度：★★★☆☆，使用频率：★★★☆☆】 策略模式-Strategy Pattern【学习难度：★☆☆☆☆，使用频率：★★★★☆】 模板方法模式-Template Method Pattern【学习难度：★★☆☆☆，使用频率：★★★☆☆】 访问者模式-Visitor Pattern【学习难度：★★★★☆，使用频率：★☆☆☆☆】 微服务理论文章阅读学习 [译]可伸缩的网站架构和分布式系统 Distributed systems for fun and profit 0引言&amp;1基本概念 2抽象描述系统的特征 3时间和顺序 4复制 工具建设 Hexo搭建个人博客 使用BaGet搭建私有nuget服务器 win10安装wsl2和docker]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java开发环境配置]]></title>
    <url>%2F2018%2F11%2F03%2FJava%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[[TOC] JDK安装和配置Windows环境 从Java官网下载Windows版本的JDK JDK8 JDK11 JDK12 点击安装包进行安装，记住安装路径 设置Windows环境变量Java_Home： 1C:\Program Files\Java\jdk-12 设置Windows环境变量Path，在原有内容之后添加: 1;%Java_Home%\bin 设置Windows环境变量CLASSPATH，在原有内容之后添加: 1;%Java_Home%\lib\dt.jar;%Java_Home%\lib\tools.jar 如果环境变量CLASSPATH不存在，则创建，设置其值为: 1%Java_Home%\lib\dt.jar;%Java_Home%\lib\tools.jar 运行命令java -version和javac -version，测试安装是否成功 MacOS环境 从Java官方网站下载macOS版本的JDK (Java SE Development Kit 8u161)：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 一直点击下一步安装 Java默认安装路径: /Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/JDK Home目录：/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home (配置IDE需要) 配置环境变量，在.zshrc (根据使用shell对应更改): 123export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=JAVA_HOME/libh/tools.jar:$JAVA_HOME/lib/dt.jar 运行命令 source .zshrc 确认安装成功，运行命令 java -version, 正确的输出为: Maven安装和配置Windows 安装下载和安装 从Maven官方 http://www-us.apache.org/dist/maven/maven-3/3.6.1/binaries/&gt;下载安装压缩包 解压文件夹 apache-maven-3.6.1 到任意文件夹, 例如我解压到 设置环境变量 设置环境变量 M2_HOME 为maven安装的路径, 例如: 添加%M2_HOME%\bin到系统环境变量path中, 例如: 运行命令测试安装运行命令: mvn -version: macOS 安装使用brew安装运行命令: brew update; brew install maven 运行命令测试安装运行命令: mvn -version IDE配置VSCode配置一.基础环境 在插件市场中搜索“Java Extension Pack”并安装。 在磁盘中创建一个临时空目录，并创建Main.java。之后，使用VSCode打开该目录。 在Main.java文件中输入或粘贴要测试的代码 打开调试选项卡（或按下“Ctrl+Alt+D”），并选择添加配置。 打开“launch.json”，并修改配置项“mainClass”为真实的主类名（本例中为“Main”）。 在启动调试前，还需要设置“JAVA_HOME”的路径。打开“文件”–“首选项”–“设置”（或按下“Ctrl+,”），并修改“java.home”配置项为真实的“JAVA_HOME”路径。 至此，环境配置完毕，可直接按下“F5”键启动调试，并可正常下断点和监视变量。 二.Maven环境 首先配置可以正常使用的Maven环境。 新建一个临时空目录，在其中创建必须的几个文件和文件夹，包括pom.xml、src/main/java。 修改VSCode中与Maven相关的配置，将Maven可执行文件（windows平台为mvn.cmd）的绝对路径填写在maven.executable.path配置项中，并将Maven配置文件settings.xml的绝对路径填写在java.configuration.maven.userSettings配置项中。 保存配置并重启VSCode，之后在“pom.xml”文件上右击，并选择“Update project configration”。 之后，VSCode的Java插件将会在工作区中生成相关的文件（“.classpath”、“target”等）。 在工作区下方，即可选择Maven工程的常用命令。 配置完成后，代码中可直接提示出“pom.xml”包含的包中的类。 IntelliJ IDEA 安装配置一. IntelliJ IDEA安装 在Intellij官方网站下载Community版 Windows 下载地址: https://www.jetbrains.com/idea/download/#section=windows Mac 下载地址: https://www.jetbrains.com/idea/download/#section=mac Linux 下载地址: https://www.jetbrains.com/idea/download/#section=linux 直接双击安装包，开始安装 选择安装JRE 选择导入配置信息或者不导入配置信息 二. IntelliJ IDEA 配置如果你没有导入配置文件，第一次运行IDEA会看到一个设置导航 选择喜欢的UI配色 选择安装推荐插件，这里保持默认就好，或者根据你的需要安装插件 3.配置build任务，保持默认就好 三. 配置项目默认JDK 在IDEA启动窗口，选择 Configure -&gt; Project Default -&gt; Project Structure, 打开 Default Project Structure 窗口 在 Project SDK 标签下，选择一个 Java SDK或者点击 New -&gt; + JDK 选择新的Java SDK 点击右下角 确认 四. 创建第一个Java项目 点击Java启动窗口的 Create New Project 左边标签栏选择 Java, 在Project设置窗口中还可以选择 Project SDK，不需要选择其他的库和框架，点击下一步 在 New Project 窗口下，选择 Create Project from template (从模板创建项目)，然后勾选 Command Line App (命令行程序)，点击下一步 设置 Project Name，选择 Project location，此处可以保持默认 package，课程中会再介绍 package 概念，然后点击下一步，完成项目创建。 五. IntelliJ IDEA 插件推荐 Material Theme UI： 一个很好看的 UI 主题 IdeaVim: 在IDEA里使用VIM编辑代码， Vim已死，Vim永存~ WakaTime: 记录每天写代码的时间 参考 使用VSCode搭建简单Java开发环境（一、基础环境） 使用VSCode搭建简单Java开发环境（二、Maven环境）]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo搭建个人博客]]></title>
    <url>%2F2018%2F10%2F21%2FHexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[[TOC] 本文介绍如何使用Hexo搭建个人博客并托管到GithubPages的过程,使用了Next(6.x版本)的Gemini主题,持续集成工具为travis ci 需要提前安装好git和node.js,并熟悉github的基本操作,访问部分工具需要科学上网 为方便说明 假设你的github名为jay,实际操作时请替换成自己的用户名 新建Github仓库仓库名为jay.github.io 新建完成后 123git clone git@github.com:jay/jay.github.io.gitcd jay.github.iogit checkout -b blog 这里把仓库clone到了本地,新建并切换到了blog分支 下面的操作都将在blog分支根目录进行(以下简称为根目录),blog分支用于保存源文件,master分支用于发布.不要手动操作master分支 也可以使用两个repo,分别作为源码版本管理的repo和发布的repo,不过一个已经够用了 安装Hexohexo详细教程请参考hexo官网,此处列出一些简要步骤 全局安装hexo脚手架 1npm install hexo-cli -g 在根目录生成hexo网站模板到temp文件夹 1hexo init temp 复制 temp文件夹中的内容到根目录,并\themes文件夹下的默认landscape主题,然后删除temp文件夹 约定 根目录下的_config.yml文件为站点配置文件 安装Next主题1git clone https://github.com/theme-next/hexo-theme-next themes/next 将next主题拷贝至 themes/next文件夹下,删除next文件夹下多余的文件,除了下图红框中包含的文件和文件夹之外全部删除,尤其注意要删除next下隐藏的.git文件夹 约定 next文件夹下的_config.yml文件为主题配置文件 在站点配置文件中启用next主题 1theme: next 在主题配置文件中启用Gemini分主题,去掉#注释 12345# Schemes# scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini 此时可以在本地调试看一下效果 1hexo s hexo s是 hexo server的缩写,启动后可以复制地址到浏览器中查看,默认为http://localhost:4000 调整Next样式基本配置修改语言,标题等信息修改站点配置文件 12345678910# Site# 语言，原来是zh-Hanslanguage: zh-CN# 统计插件# hexo-symbols-count-timesymbols_count_time: symbols: true time: true total_symbols: false total_time: false 顺便在# Site处修改站点标题,作者等信息 修改头像修改主题配置文件 123456789101112131415Avataravatar: # in theme directory(source/images): /images/avatar.gif # in site directory(source/uploads): /uploads/avatar.gif # You can also use other linking images. #/images/avatar.gif # 你的头像地址 url: # If true, the avatar would be dispalyed in circle. rounded: true # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: true url 头像图片网址,也可以使用/images目录下的文件 rounded 是否为圆形 rotated 鼠标hover时是否有旋转效果 修改社交信息修改主题配置文件 1234567891011social: GitHub: https://github.com/jay || github # E-Mail: #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook #VK Group: https://vk.com/yourname || vk #StackOverflow: https://stackoverflow.com/yourname || stack-overflow #YouTube: https://youtube.com/yourname || youtube #Instagram: https://instagram.com/yourname || instagram #Skype: skype:yourname?call|chat || skype hexo采用fontawesome字体图标,每个||之前的内容代表文件路径,||之后的内容代表字体图标class,如github的字体图标渲染时class为fa-github 主要样式\themes\next\source\css\_custom\custom.styl中加入以下代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401body &#123; font-family: 'Microsoft YaHei', sans-serif;&#125;/* 上方留白 */.header-inner, .content &#123; margin-top: 20px;&#125;/* 背景设置 */[class='container sidebar-position-left '], .main, [class$='page-home'] #footer, [class$='page-post-detail'] #footer, .page-archive &#123; background: rgba(255, 255, 255, 0.83);&#125;[class='container sidebar-position-left '] .main, [class$='sidebar-position-left'] #footer, .page-archive .main &#123; background: transparent;&#125;.pagination, .post-block, .comments, .container .header-inner, .sidebar-inner &#123; background: rgba(255, 255, 255, 0.45);&#125;// 归档#sidebar &#123; background: rgba(255, 255, 255, 0);&#125;.menu .menu-item a:hover &#123; border-bottom-color: transparent; background: #2222223b;&#125;.menu-item-active a &#123; border-bottom-color: transparent; background: #fbcb3fbd;&#125;.tabs ul.nav-tabs li.tab.active, .tabs .tab-content &#123; background-color: rgba(255, 255, 255, 0.62) !important;&#125;.gitment-editor-body textarea &#123; background-color: #ffffff9e !important;&#125;.gitment-editor-preview-field &#123; background-color: #ffffff40 !important;&#125;.gitment-comment-header &#123; background-color: transparent !important;&#125;#gitment-container a &#123; border-bottom: none;&#125;/* 侧边栏 */.post-toc ol a &#123; border-bottom-color: transparent; &amp;:hover &#123; border-bottom-color: #666; &#125;&#125;// active状态下的超链接样式.post-toc .nav li[class$='active']&gt;.nav-link &#123; border-bottom-color: transparent; &amp;:hover &#123; border-bottom-color: #fc6423; &#125;&#125;// 当前正在访问的超链接样式.post-toc .nav li[class$='active-current']&gt;.nav-link &#123; display: block; padding: 5px 7px; border-bottom-color: transparent; border-radius: 3.6px; background: #337ab7; color: white; &amp;:hover &#123; background: orange; color: black; &#125;&#125;.site-title &#123; font-weight: bold;&#125;.site-state-item &#123; border-left: 1px solid #b1b1b1;&#125;// 各级目录的显示宽度 - 主要是为了不让边框超出侧边栏.post-toc .nav .active-current.nav-level-2 a &#123; width: 303px;&#125;.post-toc .nav .active-current.nav-level-3 a &#123; width: 296px;&#125;.post-toc .nav .active-current.nav-level-4 a &#123; width: 286px;&#125;.post-toc .nav .active-current.nav-level-5 a &#123; width: 276px;&#125;.post-toc .nav .active-current.nav-level-6 a &#123; width: 266px;&#125;.post-toc &#123; width: calc(100% + 17px);&#125;/* 标题 */.posts-expand article:only-of-type h1 &#123; font-weight: bold; font-size: 37px;&#125;// 一级标题(正常显示器分辨率)// 其实要在左下角和右下角加上一个折角、翻折的效果（一个三角形）.post-body h1 &#123; position: relative; margin: 20px -42.3px 15px -48px; padding-left: 1.7%; height: 60px; border: 1px solid #ddd; border-radius: 2px; background-color: #eaf1f7; box-shadow: 0 2px 2px 0 rgba(0, 0, 0, 0.12), 0 3px 1px -2px rgba(0, 0, 0, 0.06), 0 1px 5px 0 rgba(0, 0, 0, 0.12); font-weight: bold; line-height: 60px;&#125;// 二级标题(移动终端分辨率)@media (min-width: 768px) and (max-width: 991px) &#123; .main .post-body h2 &#123; margin: 20px -23px 15px -28px; &#125;&#125;@media (max-width: 767px) &#123; .main .post-body h2 &#123; margin: 20px -15px 15px -18px; &#125;&#125;// 三级标题.post-body h3 &#123; padding-bottom: 0.3em; border-bottom: 1px solid #cfd8dc !important;&#125;/* 文章页面内的各种元素 */.post-body .note &#123; border-color: #ddd; background-color: #f9f9f980;&#125;// 文章内的普通超链接.post-body a &#123; border-bottom: none; color: #0593d3; &amp;:hover &#123; color: #FF4500; &#125;&#125;// 代码块内的超链接.post-body a code &#123; border-bottom: none; color: #0593d3; &amp;:hover &#123; color: #FF4500; &#125;&#125;// 分页导航（上/下一篇文章的超链接）.post-nav-item a &#123; border-bottom: none; color: #0593d3; &amp;:hover &#123; color: #FF4500; &#125;&#125;// 阅读全文按钮的超链接.post-button .btn &#123; border-bottom: 2px solid #555;&#125;// 无序列表.posts-expand .post-body ul li &#123; list-style: disc;&#125;// 有序列表.posts-expand .post-body ol &#123; counter-reset: counter;&#125;.posts-expand .post-body ol&gt;li &#123; position: relative; list-style-type: none;&#125;.posts-expand .post-body ol&gt;li:before &#123; position: absolute; top: 5px; left: -27px; padding: 3px; width: 13px; height: 13px; border-radius: 50%; background: #49b1f5; // #929ca2; color: #fff; content: counter(counter); counter-increment: counter; text-align: center; font-weight: 500; font-size: 12px; line-height: 1;&#125;// fancybox图片.posts-expand .post-body .fancybox img &#123; display: block !important; margin-left: 0; cursor: pointer; cursor: zoom-in; cursor: -webkit-zoom-in;&#125;// 图片底下的描述性文字.post-body .image-caption, .post-body .figure .caption &#123; text-align: left;&#125;// label块.post-body .label &#123; margin: 0 3px; padding: 3px 4px; border-radius: 0.25em; font-weight: bold;&#125;.post-body .label.primary &#123; background-color: #ecd6ff;&#125;.post-body .label.info &#123; background-color: #c3e8f9;&#125;.post-body .label.success &#123; background-color: #b2eabb;&#125;.post-body .label.default &#123; background-color: #dcdcdc;&#125;.post-body .label.warning &#123; background-color: #ffe89a;&#125;.post-body .label.danger &#123; background-color: #f9c1ca;&#125;// 代码区.highlight .code pre &#123; background-color: transparent;&#125;.highlight &#123; border: 1px solid #ddd; border-radius: 2px;&#125;// 原样输出的代码块pre &#123; border: 1px solid #ddd; border-radius: 2px; background: #f9f9f980;&#125;pre code &#123; border: transparent;&#125;pre, .highlight &#123; // background-color: #1d1f21; // color: #c5c8c6; // font-size: 13px;&#125;.highlight .gutter pre &#123; // background-color: #000; // color: #888f96;&#125;// 块引用blockquote &#123; border-left-color: #6b8afb; background-color: #f9f9f980;&#125;// 表格奇数行.highlight table&gt;tbody&gt;tr &#123; &amp;:nth-of-type(odd) &#123; background-color: transparent; &#125;&#125;// 表格每一行鼠标经过时的颜色table&gt;tbody&gt;tr &#123; &amp;:hover &#123; background-color: #FFFFF0; &#125;&#125;// 表格的单元格table td, table th &#123; border-right: 1px solid #dcdcdc;&#125;// 侧边栏头部.site-meta &#123; background: #337ab7;&#125;// 文章底部的标签.posts-expand .post-tags a &#123; padding: 3px 4px; border-bottom: none; border-radius: 3px; background: #e6e6e6; color: #2ca6cb; &amp;:hover &#123; background: #2ca6cb; color: white; &#125;&#125;// 分割线hr &#123; margin: 0; height: 1px; border: none; background-image: none;&#125;// code代码块code &#123; margin: 0 2px; background: #dcdcdc;&#125;// 选项卡被选中的那一项.tabs ul.nav-tabs li.tab.active a &#123; font-size: 19px;&#125;// 浏览器滚动条::-webkit-scrollbar &#123; width: 8px; height: 8px;&#125;::-webkit-scrollbar-thumb &#123; background: #49b1f5;&#125;::-webkit-scrollbar-track &#123; background-color: transparent;&#125;.page-archive #footer &#123; position: inherit; padding-bottom: 1.9%; height: 1px;&#125;.feed-link &#123; display: none;&#125; 显示宽度在\themes\next\source\css\_variables\custom.styl增加以下配置信息 123$content-desktop = 75%;$content-desktop-large = 75%;$content-desktop-largest = 75%; 增加其他功能增加标签和分类 根目录下输入命令,会在根目录的source文件夹下生成categories文件夹和tags文件夹 12hexo new page categories hexo new page tags 编辑新建的页面source/categories/index.md 12345---title: 分类date: 2018-10-21 20:34:08type: "categories"--- 编辑新建的页面source/tags/index.md 12345---title: 标签date: 2018-10-21 20:34:08type: "tags"--- 编辑文章模板scaffolds\post.md 123456---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories:tags:--- 修改 主题配置文件,增加菜单栏 123456menu: home: / || home #about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive 可以使用hexo new &quot;文章名&quot;生成新的文章查看效果,文章的标签和分类实例如下 123456789---title: Hexo搭建个人博客date: 2018-10-21 17:54:09categories: - 工具建设tags: - hexo- 教程--- 一篇文章只能有一个分类,可以有一个或多个标签 增加站内搜索 安装站内搜索插件 1npm install hexo-generator-searchdb --save 修改 站点配置 文件 12345search: path: search.xml field: post format: html limit: 10000 修改 主题配置 文件 12local_search: enable: true 本地网站重启后可以查看效果 增加RSS订阅 安装rss插件 1npm install hexo-generator-feed 修改站点配置文件,在底部增加 12345678# Extensionsplugins: hexo-generator-feed#Feed Atomfeed: type: atom path: atom.xml limit: 20 修改themes\next\languages\zh-CN.yml,新增rss: RSS订阅 12345678menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 rss: RSS订阅 about: 关于 search: 搜索 修改主题配置文件,在menu中增加rss菜单 1234567menu: home: / || home #about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive rss: /atom.xml || rss 使用hexo g生成atom.xml后重启网站查看效果,hexo g是hexo generate的缩写 增加评论区hexo支持多款评论插件,经过对比之后使用gitalk 在GitHub上注册新应用,注册地址 参数说明 Application name： # 应用名称，随意 Homepage URL： # 网站URL，如https://jay.github.io Application description # 描述，随意 Authorization callback URL：# 网站URL，https://jay.github.io 点击注册后，页面跳转如下，其中Client ID和Client Secret在后面的配置中需要用到，到时复制粘贴即可 根目录下安装gitalk 1npm i --save gitalk 修改主题配置文件,增加 12345678910# Support for LiveRe comments system.# You can get your uid from https://livere.com/insight/myCode (General web site)#livere_uid: your uidenable: truegithubID: jay repo: 你的github上任意一个公开的repoClientID: 步骤1中的ClientIDClientSecret: 步骤1中的ClientSecretadminUser: jaydistractionFreeMode: true 实际的评论会生成为repo中的issue 新建themes\next\layout\_third-party\comments\gitalk.swig文件，并添加内容： 1234567891011121314151617&#123;% if page.comments &amp;&amp; theme.gitalk.enable %&#125; &lt;link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"&gt; &lt;script src="https://unpkg.com/gitalk/dist/gitalk.min.js"&gt;&lt;/script&gt; &lt;script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; var gitalk = new Gitalk(&#123; clientID: '&#123;&#123; theme.gitalk.ClientID &#125;&#125;', clientSecret: '&#123;&#123; theme.gitalk.ClientSecret &#125;&#125;', repo: '&#123;&#123; theme.gitalk.repo &#125;&#125;', owner: '&#123;&#123; theme.gitalk.githubID &#125;&#125;', admin: ['&#123;&#123; theme.gitalk.adminUser &#125;&#125;'], id: md5(window.location.pathname), distractionFreeMode: '&#123;&#123; theme.gitalk.distractionFreeMode &#125;&#125;' &#125;) gitalk.render('gitalk-container') &lt;/script&gt;&#123;% endif %&#125; 这里用了一个md5，同时采用了 windows.location.pathname来区分不同的文章,md5算法来自https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js 解决了那个issue label长度不能超过50的问题 引入gitalk.swig 修改themes\next\layout\_third-party\comments\index.swig,添加内容 1&#123;% include 'gitalk.swig' %&#125; 修改themes\next\layout\_partials\comments.swig,添加内容(注释的为需要添加的内容,注意取消注释) 1234567 &#123;% elseif theme.valine.enable and theme.valine.appid and theme.valine.appkey %&#125; &lt;div class="comments" id="comments"&gt; &lt;/div&gt; //&#123;% elseif theme.gitalk.enable %&#125; //&lt;div id="gitalk-container"&gt;&lt;/div&gt; &#123;% endif %&#125;&#123;% endif %&#125; 新建themes\next\source\css\_common\components\third-party\gitalk.styl 123456.gt-header a, .gt-comments a, .gt-popup a &#123; border-bottom: none;&#125;.gt-container .gt-popup .gt-action.is--active:before &#123; top: 0.7em;&#125; 修改themes\next\source\css\_common\components\third-party\third-party.styl,添加内容 1@import 'gitalk' if (hexo-config('gitalk.enable')); 评论区的正式效果需要发布后才能查看 文章置顶+置顶标签 安装插件 12$ npm uninstall hexo-generator-index --save$ npm install hexo-generator-index-pin-top --save 文章模板scaffolds\post.md 1234567---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories:tags:top: --- 在实际的文章中top:true就可以置顶 设置置顶标志 打开：/blog/themes/next/layout/_macro 目录下的post.swig文件，定位到&lt;div class=&quot;post-meta&quot;&gt;标签下，插入如下代码： 12345&#123;% if post.top %&#125; &lt;i class="fa fa-thumb-tack"&gt;&lt;/i&gt; &lt;font color=7D26CD&gt;置顶&lt;/font&gt; &lt;span class="post-meta-divider"&gt;|&lt;/span&gt; &#123;% endif %&#125; 代码块添加复制功能 下载 clipboard.js,并保存到.\themes\next\source\js\src 在1的目录下创建clipboard-use.js，文件内容如下： 1234567891011121314151617/*页面载入完成后，创建复制按钮*/!function (e, t, a) &#123; /* code */ var initCopyCode = function()&#123; var copyHtml = ''; copyHtml += '&lt;button class="btn-copy" data-clipboard-snippet=""&gt;'; copyHtml += ' &lt;i class="fa fa-globe"&gt;&lt;/i&gt;&lt;span&gt;copy&lt;/span&gt;'; copyHtml += '&lt;/button&gt;'; $(".highlight .code pre").before(copyHtml); new ClipboardJS('.btn-copy', &#123; target: function(trigger) &#123; return trigger.nextElementSibling; &#125; &#125;); &#125; initCopyCode();&#125;(window, document); 在.\themes\next\source\css\_custom\custom.styl样式文件中添加下面代码： 123456789101112131415161718192021222324252627282930313233343536//代码块复制按钮.highlight&#123; //方便copy代码按钮（btn-copy）的定位 position: relative;&#125;.btn-copy &#123; display: inline-block; cursor: pointer; background-color: #eee; background-image: linear-gradient(#fcfcfc,#eee); border: 1px solid #d5d5d5; border-radius: 3px; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; -webkit-appearance: none; font-size: 13px; font-weight: 700; line-height: 20px; color: #333; -webkit-transition: opacity .3s ease-in-out; -o-transition: opacity .3s ease-in-out; transition: opacity .3s ease-in-out; padding: 2px 6px; position: absolute; right: 5px; top: 5px; opacity: 0;&#125;.btn-copy span &#123; margin-left: 5px;&#125;.highlight:hover .btn-copy&#123; opacity: 1;&#125; 添加引用 在.\themes\next\layout\_layout.swig文件中，添加引用（注：在 swig 末尾或 body 结束标签（&lt;/body&gt;）之前添加）： 123&lt;!-- 代码块复制功能 --&gt;&lt;script type="text/javascript" src="/js/src/clipboard.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript" src="/js/src/clipboard-use.js"&gt;&lt;/script&gt; 发布 安装 hexo-deployer-git。 1npm i hexo-deployer-git --save 修改站点配置文件,添加内容 1234deploy: type: git repo: git@github.com:jay/jay.github.io.git branch: master 将发布到master分支 使用hexo generate --deploy发布,可以简写成hexo g -d 至此,我们完成了博客的搭建,但是每次有新的文章都要发布,我们可以使用持续集自动化这个过程 持续集成使用Travis CI进行持续集成 生成github token 在github点击头像 - setting - Developer settings- Personal access tokens点击底部的生成,token只会出现一次,保存生成的token备用 使用Github账号登录Travis CI官网 https://travis-ci.org/account/repositories下选择博客所在的仓库 进入项目，在More options中点击setting 添加步骤1中生成的token,key值为GH_TOKEN 在根目录下新建.travis.yml文件,添加内容: 1234567891011121314151617181920212223language: node_jsnode_js: stable# S: Build Lifecycleinstall: - npm install#before_script: # - npm install -g gulpscript: - hexo gafter_script: - cd ./public - git init - git config user.name "这里填你的github用户名,这里假定为jay" - git config user.email "这里填你的github邮箱" - git add . -f - git commit -m "Update docs" - git push --force --quiet "https://$&#123;GH_TOKEN&#125;@$&#123;GH_REF&#125;" master:masterbranches: only: - blogenv: global: - GH_REF: github.com/jay/jay.github.io.git 注意GH_REF填写实际的项目地址,是https://后面的内容,不是ssh的地址 现在我们对blog分支推送的任何更新都会由Travis CI将构建后的内容推送到master分支,可以在Travis CI查看构建的过程 可以删除原来手动发布所需的相关依赖和配置,当然保留也没关系 参考参考]]></content>
      <categories>
        <category>工具建设</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
